"use client";

import React, { useState, useEffect, useMemo } from "react";
import {
  Bar,
  BarChart,
  CartesianGrid,
  Legend,
  ResponsiveContainer,
  Tooltip,
  XAxis,
  YAxis,
} from "recharts";

// --- TIPOS E INTERFACES ---
export type Autonomy = "A" | "B" | "C";
export type StudyType = "Systematic Review/Mapping" | "Experimental" | "Case Study" | "Other";
export type AnalysisType = "Quantitative" | "Qualitative" | "Mixed";
export type InteractionType = "Free Prompts" | "Fixed Prompts" | "Hybrid";
export type UseByStudents = "Yes" | "No" | "Partially" | "Not reported";

export interface Article {
  id: string;
  title: string;
  authors: string;
  year: number;
  source: string;
  studyType: StudyType;
  doiOrLink: string;
  motivation: string;
  educationalObjectives: string;
  genAITool: string;
  usedByStudents: UseByStudents;
  usedHow: string;
  autonomy: Autonomy;
  contentSkill: string;
  teachingStrategy: string;
  audience: string;
  analysis: AnalysisType;
  resources: string;
  metrics: string[];
  results: string;
  benefitsChallenges: string;
  swebokAreas: string[];
  interactionType: InteractionType;
}

export interface FullArticleDetails {
  id: string;
  level: string;
  year: string;
  source: string;
  authors: string;
  educationalObjectives: string;
  aiEmployment: string;
  llm: string;
  metrics: string;
  metricsUsage: string;
  resources: string;
  benefitsAndChallenges: string;
  swebokAreas: string;
  swebokSubAreas: string;
  doiOrLink: string;
  interaction: string;
  analysis: string;
  usedByStudents: string;
}

// --- CONSTANTES ---
const SWEBOK_AREAS: string[] = [
  "Software Requirements", "Software Architecture", "Software Design", "Software Construction", "Software Testing",
  "Software Engineering Operations", "Software Maintenance", "Software Configuration Management", "Software Engineering Management",
  "Software Engineering Process", "Models and Methods", "Software Quality", "Software Security",
  "Professional Practice", "Engineering Economics", "Computing Foundations", "Mathematical Foundations", "Engineering Foundations",
];

const SUBAREAS: Record<string, string[]> = {
  "Software Requirements": ["Software Requirements Fundamentals", "Requirements Elicitation", "Requirements Analysis", "Requirements Specification", "Requirements Validation", "Requirements Management Activities", "Practical Considerations", "Software Requirements Tools", "Persona"],
  "Software Design": ["Recording Software Designs", "Software Design Quality Analysis and Evaluation"],
  "Software Construction": ["Software Construction Fundamentals", "Managing Construction", "Practical Considerations", "Construction Technologies", "Software Construction Tools"],
  "Software Testing": ["Software Testing Fundamentals", "Test Levels", "Testing Techniques", "Test-Related Measures", "Test Process", "Software Testing in the Development Processes and the Application Domains", "Testing of and Testing Through Emerging Technologies", "Software Testing Tools"],
  "Software Maintenance": ["Software Maintenance Techniques"],
  "Software Configuration Management": [],
  "Software Engineering Management": ["Initiation and Scope  Definition", "Software  Project Planning", "Software  Project Enactment", "Review and Evaluation, Closure", "Software Engineering  Measurement", "Software Engineering  Management Tools"],
  "Software Engineering Process": ["Software Engineering Process Fundamentals", "Life Cycles", "Software Process Assessment and Improvement"],
  "Software Quality": ["Software Quality Fundamentals", "Software Quality Management Process", "Software Quality Assurance Process", "Software Quality Tools"],
  "Software Architecture": ["Software Architecture Description", "Software Architecture Evaluation", "Software Architecture Fundamentals", "Software Architecture Process"],
  "Models and Methods": ["Modeling", "Types of Models", "Software Engineering Methods", "Analysis of Models"],
  "Software Security": [],
  "Professional Practice": ["Professionalism", "Group Dynamics and Psychology", "Communication Skills"],
  "Engineering Economics": [],
  "Computing Foundations": ["Basic Concept of  a System  or Solution", "Computer Architecture  and  Organization", "Data Structures and  Algorithms", "Programming Fundamentals and Languages", "Operating Systems", "Database Management", "Computer Networks and   Communications", "User and Developer  Human Factors", "Artificial Intelligence  and Machine  fLearning",],
  "Mathematical Foundations": [],
  "Engineering Foundations": ["The  Engineering Process", "Engineering Design", "Abstraction and  Encapsulation", "Empirical Methods  and Experimental  Techniques", "Statistical Analysis", "Modeling,  Simulation and  Prototyping", "Measurement", "Standards", "Root Cause Analysis", " Industry 4.0 and Software Engineering"],
  "Software Engineering Operations": ["Software Engineering Operations Planning", "Software Engineering Operations Delivery", "Software Engineering Operations Control", "Practical Considerations", "Software Engineering Operations Tools"],
};


const AUTONOMY_COLORS: Record<Autonomy, string> = { A: "#10b981", B: "#f59e0b", C: "#ef4444" };

// üëá COLE ESTE NOVO BLOCO AQUI
const AUTONOMY_LABELS: Record<Autonomy, string> = {
  A: "Free Prompts",
  B: "Fixed or Hybrid Prompts",
  C: "Used by Professors",
};
// --- DADOS COMPLETOS PARA O MODAL (GERADO DO CSV) ---
const FULL_ARTICLE_DETAILS: Record<string, FullArticleDetails> = {
    "Students‚Äô Use of GitHub Copilot for Working with Large Code Bases": { "id": "ID01", "level": "A", "year": "2025", "source": " Proceedings of the 56th ACM Technical Symposium on Computer Science Education ", "authors": "Anshul Shah, Anya Chernova, Elena Tomson, Leo Porter, William G. Griswold, Adalbert Gerald Soosai Raj", "educationalObjectives": "This study was conducted to understand how undergraduate students utilize and experience GitHub Copilot when adding a feature to a large codebase, with an analysis of how they used the GitHub Copilot chat feature. The ultimate goal is for the computer science curriculum to adequately prepare students for a career in software engineering in the current landscape with LLMs (Large Language Models). The paper presents three research questions (RQs), which are: RQ1: \"How often do students use the diverse features of GitHub Copilot when adding a small feature to a large codebase?\" RQ2: \"How do students typically interact with the GitHub Copilot chat?\" RQ3: \"How much do students trust the output of GitHub Copilot for code generation and code comprehension features?\" The highlighted objectives are to prepare students for the effective use of LLMs in software engineering, developing skills such as navigating and modifying large codebases, best practices in using these tools, verification and debugging of LLM-generated outputs, in addition to fundamental competencies like decomposition, testing, and debugging. The paper seeks to teach how students can utilize GitHub Copilot as a tool to assist with complex software engineering tasks, such as working with extensive codebases, emphasizing the need for critical skills in verification, debugging, and decomposition, in addition to understanding the code itself.", "aiEmployment": "Initially, the instructor introduced Copilot's features in two lectures, with live demonstrations (such as the /explain, /fix, and /tests commands, etc.). Subsequently, students completed a small in-class activity using the tool. The main practical application occurred during an assignment that required adding a feature to a large, real-world codebase, within the context of the course (Working with Large Codebases). During this task, Copilot usage was observed through process diaries and chat transcripts, with the aim of understanding its application in real-world scenarios and reinforcing skills such as decomposition, verification, and debugging of AI-generated code. The primary objective was to observe how these students utilized the tool when tackling a complex programming task in a setting closer to a professional environment. The prompts and interactions with the AI were created by the students themselves while completing a designated programming task. The AI was used by the students as an assistant to aid in various stages of the software engineering task, from understanding the existing codebase and searching for relevant parts, through code generation and debugging, up to the creation of documentation and tests.", "llm": "GitHub Copilot", "metrics": "Questionnaires, User Satisfaction, Number of Interactions with Copilot Chat, and Rate of Use of Copilot Features", "metricsUsage": "A qualitative analysis of the Copilot Chat interactions was performed through open coding of the transcripts, categorizing the types of prompts and behaviors, such as one-shot prompting. Quantitatively, the most used features were also measured, including the Copilot Chat (97.9%) and the /explain command (66.7%), in addition to the frequency of strategies like one-shot prompting (63%).", "resources": "The study utilized in-class demonstrations, practical activities, the idlelib codebase, a programming assignment, usage diaries, and Copilot Chat transcripts.", "benefitsAndChallenges": "The study identified several benefits from using GitHub Copilot, such as the accelerated code comprehension via the /explain command, utilized by 66.7% of students, and the support from Copilot Chat (used by 97.9%) in the decomposition of complex problems. Rapid code generation through comments was also used by 37.5%, albeit with caveats. Features that promote transparency, such as clickable references, increased student trust. Conversely, challenges included the reliance on one-shot prompting (63% of students), which generated debugging cycles and time loss due to incorrect code. Trust in code generation was low, attributed to the lack of explanations and the occurrence of errors, such as hallucinations and ineffective commands.", "swebokAreas": "Software Construction", "swebokSubAreas": "Practical Considerations", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3641554.3701800", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Exploring LLMs Impact on Student-Created User Stories and Acceptance Testing in Software Development": { "id": "ID02", "level": "B", "year": "2025", "source": "", "authors": "Warren Liang, Wahib Mahana, Wanming Hu", "educationalObjectives": "This study specifically investigates the influence of Large Language Models (LLMs) on the quality, consistency, and efficiency with which students create user stories and acceptance tests. It also addresses the need to determine whether LLMs help students overcome common challenges in writing user stories and creating acceptance tests that are correctly aligned with requirements. The overarching goal is to enhance the students' learning experience and improve the quality of the software projects they create. This can be achieved by exploring the impact of LLMs on user story creation within the educational context. Potential advantages include reducing time spent on repetitive manual tasks and improving the clarity and structure of user stories.", "aiEmployment": "The article analyzes the interaction between students and LLMs (Large Language Models) during the creation of user stories. Among its main applications, the LLM assisted in the initial drafting of user stories, based on brief descriptions provided by the users or the team. Furthermore, the AI contributes to the refinement of these user stories, suggesting improvements that enhanced clarity. The role of the AI primarily appears to be that of an assistant in the creation process.", "llm": "ChatGPT-3 and ChatGPT-4", "metrics": "Comparison of Time, Quality, Consistency, and Efficiency of User Stories", "metricsUsage": "The time and quality of user story creation (with and without AI) were compared, along with the precision in defining the acceptance criteria.", "resources": "The only supplementary material evidenced in the text was the use of LLMs.", "benefitsAndChallenges": "The study identified that the use of LLMs (Large Language Models) brought benefits such as improved clarity, structure, and quality of user stories and acceptance criteria, in addition to increasing student productivity and facilitating the adoption of Agile practices. Conversely, challenges were observed related to the risk of excessive dependence on AI, which could compromise the development of students' critical thinking. It also became evident that there is a need to provide adequate training so that students are capable of evaluating and refining the content generated by the AI, thus preserving their creativity and ensuring that the artifacts remain aligned with the requirements.", "swebokAreas": "Software Requirements", "swebokSubAreas": "Software Requirements Fundamentals", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3641555.3705183?casa_token=VJycYFTNtkMAAAAA:zA5br9yM4eYbs1VpvtIvd7Srl7lACuldWKOSH9bqIBU40Eaoupb1GYa00Ufl548yVM7BQ9fNC8-9Jg", "interaction": "Hybrid", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Evaluating GenAI's Effectiveness for Students with Varied Programming Backgrounds in a Software Development Course": { "id": "ID03", "level": "A", "year": "2025", "source": "SIGCSE", "authors": "Warren Liang, Wahib Mahana e Wanming Hu", "educationalObjectives": "The article specifically focuses on practical software construction education, evaluating how tools like ChatGPT and Codex impact programming learning. It investigates the use of these tools for code generation, debugging, problem-solving, and the comprehension of algorithms and data structures, in addition to measuring the quality and efficiency of the code produced. Developing coding and debugging skills, including the generation, optimization, and correction of code, using Generative AI (GenAI) as a support to identify errors and explore alternatives. The tools were utilized to generate code, debug, reinforce concepts, explore new technologies, and optimize solutions. Beginner students used them as support to learn and gain confidence, while advanced students employed them to increase productivity.", "aiEmployment": "Intermediate students receive suggestions for improvements and advanced features; and advanced students utilize AI as a productivity tool. The study assessed the effectiveness of Generative AI in software development courses using a mixed-methods approach, combining quantitative metrics‚Äîsuch as code quality, completion time, problem-solving, engagement, and knowledge retention‚Äîwith qualitative student feedback.", "llm": "ChatGPT and Codex", "metrics": "Questionnaires, Time comparison, Accuracy of generated code, User satisfaction", "metricsUsage": "The study collected feedback from students with varying levels of knowledge to understand their perception of GenAI tools. Completion time was one of the metrics evaluated, along with code accuracy. The study sought to understand students' perception of GenAI tools.", "resources": "Traditional materials, such as textbooks, lectures, and in-person mentoring (or face-to-face mentoring), were used in the teaching process.", "benefitsAndChallenges": "Among the benefits are real-time personalized support, code generation, debugging assistance, improved comprehension of complex concepts, greater accessibility, and increased productivity for advanced students. The main challenges include the risk of over-reliance on AI, gaps in deep conceptual understanding, and the need for constant instructor guidance.", "swebokAreas": "Software Construction, Computing Foundations", "swebokSubAreas": "Practical Considerations, Programming Fundamentals and Languages", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3641555.3705175?casa_token=SbS4b3JEAc4AAAAA:0MgmDy-EMI1MA23_n5OjnxoiyRQK5Dzps2EGvFrn_BKYz2f8Fe700oY1ceSdoUcwruZ6_Sm7gXtl2w", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Learning without Limits: Analysing the Usage of Generative AI in a Summative Assessment": { "id": "ID04", "level": "A", "year": "2025", "source": "Computing Education Practice (CEP 2025)", "authors": "Lee Clift e Olga Petrovska", "educationalObjectives": "The paper reports that students directly utilized LLMs during an assessment in a mobile application development course. The study aimed to teach students how to use Artificial Intelligence in a critical and practical manner in software development. It sought to prepare them for the job market by integrating current technologies into the curriculum.", "aiEmployment": "Students developed mobile applications in two stages: a short design phase (2 weeks) and a longer development phase (7 weeks), with freedom to use any tool, including AI. The focus of the assessment was on the final product and project comprehension, not the code. Final-year students developed apps in two stages, with the freedom to use Generative AI as they wished. After the submission, they completed a self-reflection form regarding their use of AI. Quantitative analysis showed that 100% of the students used AI, especially ChatGPT, and the majority considered the experience positive. Qualitative analysis revealed common uses such as code generation and bug fixing, alongside comments on utility, frustrations, and perceptions.", "llm": "ChatGPT", "metrics": "Questionnaires, Likert Scale, User satisfaction", "metricsUsage": "A post-assessment self-reflection form was administered to the students upon project submission. Students were requested to rate specific aspects on a 5-point Likert scale, focusing on their satisfaction and perceptions regarding the experience.", "resources": "Students utilized Dart, Flutter, and a standard IDE for app development, supported by resources such as online forums and weekly practical laboratory sessions.", "benefitsAndChallenges": "GenAI, as a tool, brought numerous benefits, including a positive learning experience, more ambitious and higher-quality projects, and increased speed in programming by automating simple tasks. Among the challenges, the majority of students found the process of citing AI-generated code frustrating, while concerns regarding privacy were minimal.", "swebokAreas": "Computing Foundations", "swebokSubAreas": "Programming Fundamentals and Languages", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3702212.3702214", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Effect of Large Language Model Use on Programming Project Groups": { "id": "ID05", "level": "A", "year": "2025", "source": "Proceedings of the Doctoral Consortium of the 19th European Conference on Technology Enhanced Learning", "authors": "Laura Graf", "educationalObjectives": "AI has provided an environment that can enhance students' self-efficacy and motivation. However, there are concerns that frequent use may reduce autonomy by fostering dependency and hindering the development of proprietary problem-solving strategies. Programming skills (such as coding, debugging, code comprehension), teamwork/team collaboration in software engineering.", "aiEmployment": "The study employed collaborative programming projects in university courses as the main teaching activity. These projects simulate real-world software development scenarios where student groups work together. Within the scope of this project, students individually utilized Large Language Models (LLMs) to assist in programming problem-solving. Each student interacted with the model according to their own needs, which reflected an autonomous and contextualized use of the technology. The approach focuses on observing how the integration of LLMs affects both individual learning and group dynamics. The study adopts a mixed-methods approach. The quantitative analysis is based on data automatically extracted from the collaborative development environments. The qualitative analysis, on the other hand, uses interviews, self-assessments, and group assessments to capture perceptions regarding learning, contributions, AI usage, motivation, confidence, and the development of professional identity.", "llm": "An exact model such as ChatGPT was not specified.", "metrics": "Self-reports, Student interviews, and Collaborative platform log data", "metricsUsage": "The metrics were used to analyze students' perceptions of learning, self-efficacy, professional identity, and group collaboration through self-reports and interviews. Furthermore, log data were employed to identify objective patterns of contribution and coordination among group members.", "resources": "Students participated in practical, group programming projects conducted within collaborative version control environments, which allowed for detailed tracking of contributions and interactions among team members. Upon completion of the practical activity, self-assessment materials and interviews were administered to record individual and collective perceptions regarding learning, group collaboration, and the use of AI in the educational process.", "benefitsAndChallenges": "Some evidenced benefits include support in code comprehension, rapid generation of solutions, and increased confidence and motivation for less experienced students. As for the challenges, the risks presented are dependency, difficulties in judging colleagues' competence, problems with contribution transparency, imbalance in group effort, and potential negative effects on the construction of professional identity.", "swebokAreas": "Professional Practice", "swebokSubAreas": "Group Dynamics and Psychology", "doiOrLink": "https://ceur-ws.org/Vol-3927/paper16.pdf", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Exploring the Integration of Generative AI Tools in Software Testing Education: A Case Study on ChatGPT and Copilot for Preparatory Testing Artifacts in Postgraduate Learning": { "id": "ID06", "level": "B", "year": "2025", "source": "IEEE EDUCATION SOCIETY SECTION", "authors": "Susmita Haldar, Mary Pierce, e Luiz Fernando Capretz", "educationalObjectives": "Utilizing LLMs as a pedagogical support, the study promotes the creation of test artifacts, the practical understanding of functional and non-functional testing, and the application of critical and systematic thinking. Students learn prompt engineering, comparative analysis between manual and AI-generated results, and the identification of tool limitations, such as hallucinations and lack of context. RQ1: Can a single teaching approach be effective for all types of testing, or do different contexts necessitate specialized methods? RQ2: What should be the role of AI-based testing within the curriculum, and how should it be implemented? RQ3: Does the incorporation of LLMs into testing education help students develop skills aligned with industry needs?", "aiEmployment": "The 15-week course began with students manually creating test artifacts in groups (use cases, Requirements Traceability Matrices ‚Äì RTMs, test cases, and test scripts) to consolidate foundational knowledge. Subsequently, they conducted an individual activity utilizing AI tools (ChatGPT 3.5 or Copilot) to generate the same artifacts, enabling a comparison between the manual and the AI-generated outcomes. The study employed a mixed-methods approach to analyze student responses regarding the use of generative AI in education. The quantitative analysis was based on the counting of keywords in the responses (such as ‚Äúuse cases‚Äù and ‚ÄúRTM‚Äù), allowing for the assessment of the generated artifacts' completeness and the identification of patterns, with results visualized in comparative bar charts between ChatGPT and Copilot. The qualitative analysis was performed using the VADER tool, which classified the responses as positive, neutral, or negative, providing insights into the emotional tone of the students' perceptions.", "llm": "ChatGPT 3.5 and Copilot.", "metrics": "Questionnaires, Likert Scale, Time Comparison, Accuracy of Generated Code, User Satisfaction", "metricsUsage": "Questionnaires comprising 11 analytical items were administered, including Likert scales to measure students' perception of the efficiency of both ChatGPT and Copilot. The assessment evaluated the speed of generation, the accuracy of the artifacts, in addition to the error rate‚Äîspecifically AI hallucinations‚Äîwhile sentiment analysis was used to gauge student satisfaction", "resources": "The study utilized real-world software applications, such as a travel application and MoneyManagerEx, in addition to the generative AI tools ChatGPT 3.5 and Copilot for teaching software testing.", "benefitsAndChallenges": "Copilot was more highly valued for its speed, whereas ChatGPT performed better regarding ease of use and test case coverage. Both tools aided in complementing manual work by offering support for brainstorming and providing models for comparison. However, students also identified challenges, such as the necessity for human supervision to validate the accuracy and relevance of the outcomes, in addition to the occurrence of hallucinations.", "swebokAreas": "Software Testing", "swebokSubAreas": "Testing of and Testing Through Emerging Technologies, Testing Techniques", "doiOrLink": "https://ieeexplore.ieee.org/abstract/document/10904141", "interaction": "Hybrid", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Exploring the Use of Large Language Models in Requirements Engineering Education: An Experience Report with ChatGPT 3.5": { "id": "ID07", "level": "B", "year": "2024", "source": "(SBQS 2024)", "authors": "S√°vio Sampaio, M√°rcia Lima, Eriky Rodrigues, Maria Alcimar Meireles, Marcela Pessoa, e Tayana Conte.", "educationalObjectives": "This article seeks to promote the integration of technologies such as Large Language Models (LLMs) into teaching-learning processes, with a special focus on the requirements elicitation phase, which is a crucial step for the success of software projects. The central objective of the study is to investigate how the use of LLMs affects learning outcomes and socio-affective aspects in collaborative programming projects, especially among novice programmers.", "aiEmployment": "The teaching strategy is based on collaborative programming projects, and the research seeks to understand how students' integration of LLMs within this strategy affects their learning outcomes and socio-affective results. The study uses a mixed-methods approach, combining quantitative and qualitative analyses to investigate the effects of using LLMs in collaborative programming projects. The first focus will examine how the use of LLMs affects individual perceptions and group socio-affective attitudes, with data collected through self-assessments and interviews. The second focus will analyze process data obtained from code logs, AI interactions, and planning documents, seeking relationships between contribution patterns, perceived effort, confidence, mutual support, and identification with the field.", "llm": "ChatGPT 3.5", "metrics": "Questionnaires, Time Comparison, User Satisfaction", "metricsUsage": "The qualitative analysis focused on students' perceptions regarding the use of ChatGPT 3.5, specifically the advantages and disadvantages they observed. The teams/pairs utilized a spreadsheet for the assessment of the Functional Requirements (FRs) and Non-Functional Requirements (NFRs) generated by ChatGPT 3.5. In this spreadsheet, they responded to specific questions for each requirement generated by the AI, evaluating them under three main criteria: equivalence, innovation, and pertinence.", "resources": "The experience with ChatGPT 3.5 was structured with a detailed script (protocol), training, reports, assessment spreadsheets, and ethical consent.", "benefitsAndChallenges": "The study results reveal that students perceived ChatGPT 3.5 as a useful tool in Requirements Engineering, highlighting benefits such as the generation of equivalent or innovative requirements and the acceleration of the elicitation process with skills approaching those of requirements engineers. On the other hand, challenges were also identified, including the need for prior knowledge, inaccuracies in responses, confusion between requirement types, generation of generic or unnecessary requirements, and dependence on well-structured prompts.", "swebokAreas": "Software Requirements", "swebokSubAreas": "Requirements Elicitation, Requirements Analysis, Requirements Validation", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3701625.3701687?casa_token=6VuWOhinK_oAAAAA:FyS_qF-DyuUSKeHZyti5VtZOsFDpsmaQyErIe7XVM0ohP2R3S3gT1C7eakSJ8YWoM4mAhV7sY5AwRw", "interaction": "Hybrid", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Integrating AI Tutors in a Programming Course": { "id": "ID08", "level": "A", "year": "2024", "source": "SIGCSE Virtual 2024", "authors": "Iris Ma, Alberto Krone Martins, e Cristina Videira Lopes", "educationalObjectives": "The paper seeks to teach, through generative AI, primarily the comprehension of programming concepts, the development of problem-solving skills, and the mastery of challenging material in an introductory course. Furthermore, it promotes engagement, offering support for both task-specific queries and general programming questions.", "aiEmployment": "The teaching strategy focused on providing individualized and interactive support through generative AI, which acts as a guide, directing students toward reasoning and the discovery of solutions, rather than simply providing them. RAGMan was deployed as an optional resource for the students. The AI tutors were available 24/7, offering personalized, on-demand assistance. The AI primarily functioned as a tutor, specifically in the role of a Teaching Assistant. The prompts are neither fixed nor solely created by the students; they are automatically constructed by the system based on the students' free-form questions, but always within a pre-defined template set by the instructors, for example, to avoid giving direct answers about the code.", "llm": "The generative AI tool utilized was OpenAI, integrated into the RAGMan tutoring system.", "metrics": "Questionnaires/Surveys, Error Rate, User Satisfaction, AI Tool Usage Rate", "metricsUsage": "The survey/questionnaire sought student feedback regarding their overall experience with the AI tutors, including their perceived usefulness, advantages, and disadvantages. A random sample of 248 conversations (37% of the total) was analyzed to determine the quality of the responses. Students appreciated the AI tutors' ability to clearly explain assignments, provide step-by-step guidance, and, notably, create a safe, judgment-free learning environment. Finally, the study also quantified the general usage of AI tools, with 79% of students using at least one AI tool in the course.", "resources": "18 laboratory exercises, 5 major assignments, 5 Workout Projects (WPs) with optional 24/7 support from RAGMan AI tutors, and a final exam as part of the assessment.", "benefitsAndChallenges": "The perceived benefits of using LLMs included: effective help with projects, clear explanations, step-by-step guidance, a judgment-free environment, high-quality responses (94%), conversational ability, and potential support for struggling students. Conversely, the reported challenges were: tutor slowness, contradictory responses in some cases, lack of detailed explanations, and the risks associated with the use of unrestricted AIs, which can compromise learning by providing direct answers without stimulating reasoning.", "swebokAreas": "Software Construction, Computing Foundations", "swebokSubAreas": "Practical Considerations, Programming Fundamentals and Languages, User and Developer Human Factors, Artificial Intelligence and Machine Learning", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3649165.3690094", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Experiences from Integrating Large Language Model Chatbots into the Classroom": { "id": "ID09", "level": "A", "year": "2024", "source": "SIGCSE Virtual 2024", "authors": " Arto Hellas, Juho Leinonen e Leo Lepp√§nen", "educationalObjectives": "The educational objectives include introducing the working principles and use of LLMs, applying these tools across the various phases of the Software Development Life Cycle (SDLC), and carrying out practical tasks such as documentation, programming, and testing with AI support. The focus is on skill updating and the integration of LLMs into professional software engineering practices. The article reports that students directly utilized LLMs in the educational context. They had access to a GPT-4-based chatbot, integrated into the course's learning platform, allowing for authentic and unfiltered interactions. The use was permitted to support learning, such as obtaining explanations, identifying code errors, or requesting assistance with tasks, but not for generating complete solutions. Most students (98%) in the course focused on Software Engineering with LLMs used the chatbot, indicating high adherence and perceived utility in this context. The Software Engineering with LLMs course aimed to teach students the working principles and use of Large Language Models (LLMs), with a focus on their practical application in software development life cycle tasks, such as documentation, programming, and testing. Furthermore, the course explored how LLMs can support code generation and the construction of complete applications, including problem decomposition and graphical interface development.", "aiEmployment": "The core strategy involved the direct integration of an LLM-based chatbot into the courses' online learning platform. Students were given unrestricted and free access to the chatbot, allowing them to use it similarly to ChatGPT. The chatbot was utilized as an assistive technology to support learning, for example, by explaining code, identifying errors, and offering assistance with programming tasks, but strictly without permitting the direct generation of solutions for assignments. In the Software Engineering with LLMs course, the chatbot was used in a central, curriculum-integrated manner, with practical activities involving documentation, programming, and testing with the assistance of the LLM, in addition to showcasing examples of code generation and application development. In the other courses, the usage was more limited, serving only as supplementary support. This approach also enabled the observation of student autonomy in their use of AI.", "llm": "ChatGPT 4.0", "metrics": "Questionnaires, User Satisfaction,", "metricsUsage": "A voluntary background survey was administered to the students. Furthermore, at the conclusion of each dialogue with the chatbot, students were asked to rate the chatbot's utility on a 1-to-5 star scale.", "resources": "The course utilized interactive e-books with embedded tasks hosted on an online learning platform, an AI chatbot (GPT‚àí4) integrated into the course materials, practical projects, and programming exercises.", "benefitsAndChallenges": "Among the benefits, the high utility of the chatbot in the LLM-focused course was highlighted, evidenced by positive ratings and intensive use for solving programming problems and exploring code generation capabilities. Generally, students who used the chatbot considered it useful, especially for explaining code, identifying bugs (albeit with limitations), and offering assistance when they were stuck. Conversely, usage was low in the courses not focused on LLMs, concentrated among a small number of users ('superusers'). The limitations included the chatbot‚Äôs poor performance in debugging larger codebases and in supporting very recent technologies, such as Svelte 5 alpha and Deno KV, due to the lag in its training data. Furthermore, the AI tool did not replace human support, with students continuing to seek assistance in labs and on the platform.", "swebokAreas": "Computing Foundations", "swebokSubAreas": "Programming Fundamentals and Languages", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3649165.3690101", "interaction": "Free Prompts", "analysis": "Quantitative", "usedByStudents": "Yes" },
    "Unrestricted Use of LLMs in a Software Project Course: Student Perceptions on Learning and Impact on Course Performance": { "id": "ID10", "level": "A", "year": "2024", "source": "Koli Calling ‚Äô24", "authors": "Kai Korpimies, Antti Laaksonen, Matti Luukkainen", "educationalObjectives": "The educational objectives included learning how to conceptualize, design, implement, test, and document an application, thereby developing competencies in development methodologies, version control, UML modeling, automated testing, and software design principles. The educational objectives for the software design course focused on aspects such as software development, design, testing, documentation, and quality. Generative AI was integrated as a support tool, not as the primary focus", "aiEmployment": "Course participants were freely permitted to utilize Large Language Models (LLMs) to develop a small application; the tool was employed as a programming assistance tool for students in a software design course. The prompts provided to the Generative AI were created by the students themselves. The closed-ended questions analyzed the frequency of LLM usage in tasks such as code generation and code improvement, as well as documentation. Meanwhile, the open-ended questions explored perceptions regarding the benefits and challenges of using these tools. The analysis considered student performance, the time invested in the project, and usage patterns, leading to the identification of two distinct groups.", "llm": "ChatGPT, Google Gemini, Microsoft Copilot and GitHub Copilot.", "metrics": "Questionnaires, Time Comparison, User Satisfaction", "metricsUsage": "he core methodology involved conducting surveys through an online form. The study compared the time invested by each student in the project (time-on-task). Part 2 of the survey consisted of open-ended questions where participants were able to elaborate on how beneficial they found the different uses of LLMs for the learning process.", "resources": "Online discussion environment, an initial in-person class, optional lab guidance, practical exercises on Git, testing, and UML, an online form about the use of LLMs, and support from teaching assistants.", "benefitsAndChallenges": "The study shows that students view LLMs as useful tools, highlighting benefits such as 24/7 personalized assistance, time savings, support in debugging, explanations, brainstorming, and motivation. However, concerns also arise regarding dependency, inconsistent answer quality, difficulty with prompt engineering, lack of context, and the risk of superficial learning.", "swebokAreas": "Software Construction, Computing Foundations", "swebokSubAreas": " Practical Considerations, Software Construction Fundamentals, Programming Fundamentals and Languages", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3699538.3699541", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Enhancing educational efficiency: Generative AI chatbots and DevOps in Education 4.0": { "id": "ID11", "level": "A", "year": "2023", "source": "Computer Applications in Engineering Education", "authors": "Edis Mekiƒá, Mihailo Jovanoviƒá, Kristijan Kuk, Bojan Prlinƒçeviƒá e Ana Saviƒá", "educationalObjectives": "The article addresses Software Engineering education, focusing on the practical and modern training of students by integrating agile methodologies, DevOps, and generative artificial intelligence into the teaching process, with an emphasis on code generation. The article teaches Content Management System development with a practical focus on object-oriented PHP, WordPress theme creation, and plugin development.", "aiEmployment": "ChatGPT is used as a support tool for debugging, code generation, reverse engineering, and solution creation, promoting a more active, autonomous, and efficient learning process. Efficiency metrics were used to compare groups of students who employed the traditional methodology with those who utilized AI and DevOps, demonstrating improvement in productivity and learning quality. ChatGPT acted as an auxiliary tool for debugging, generation, and refinement of solutions, while DevOps tools (Git, GitHub, Trello) facilitated version control and task management.", "llm": "ChatGPT 3.5", "metrics": "Production Rate", "metricsUsage": "The main form of assessment was through efficiency formulas based on artifact production rates.", "resources": "Slides and theoretical materials, practical programming exercises in PHP, templates and source code for developing WordPress themes and plugins, virtual environments for development and testing, in addition to digital tools such as ChatGPT for support in debugging and code generation, Git and GitHub for version control and collaboration, and Trello for task management.", "benefitsAndChallenges": "The study demonstrates that the use of LLMs in Software Engineering education yields significant benefits, such as accelerated learning, support for code generation and debugging, complex problem solving, and assisted reverse engineering, thereby increasing student efficiency and engagement. However, challenges were also identified, including the need for constant supervision and debugging by teaching staff, limitations in model comprehension, and risks of excessive dependency.", "swebokAreas": "Software Construction", "swebokSubAreas": "Practical Considerations", "doiOrLink": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/cae.22804?casa_token=gUc2znEhjaYAAAAA%3AAZN58VQHYsX2SF1iLfFAL3deVH7LhJN5xKk4AVmqh4FUt_zVonhlL86azL8G9MRgeQIsgILwv9_5QAE", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "DevCoach: Supporting Students in Learning the Software Development Life Cycle at Scale with Generative Agents": { "id": "ID12", "level": "A", "year": "2024", "source": " Proceedings of the Eleventh ACM Conference on Learning", "authors": " Tianjia Wang, Ramaraja Ramanujan, Yi Lu, Chenyu Mao, Yan Chen e Chris Brown", "educationalObjectives": "The article about DevCoach highlights the primary educational objective of enabling students to practically understand and apply the Software Development Life Cycle (SDLC), with a focus on the planning, design, and development phases. The focus is on providing students with an in-depth understanding of the Software Development Life Cycle (SDLC) and the dynamics of teamwork.", "aiEmployment": "The DevCoach tool uses generative AI to simulate interactions with typical roles in a development team. The process included a pre-questionnaire to measure initial knowledge, a tutorial, the execution of tasks in the \"\"Plan and Design\"\" and \"\"Develop\"\" phases, a post-questionnaire, and a survey on usability and experience.", "llm": " GPT-4 Turbo", "metrics": "Questionnaires, Likert Scale, Time Comparison, User Satisfaction", "metricsUsage": "A pre-questionnaire and a post-questionnaire were administered to assess participants' understanding of the Software Development Life Cycle (SDLC). A Likert scale was utilized in a post-use survey (post-survey) to collect participants' opinions about the system. The time spent completing the tasks in each phase was monitored and recorded. Finally, user satisfaction was captured by the high average ratings given by the participants in the post-use survey.", "resources": "The system offers practical tasks with real-time feedback, a code editor, a terminal, and chat-based interactions. Tutorials, questionnaires, and a textbook are also utilized to support SDLC learning.", "benefitsAndChallenges": "DevCoach utilized AI with Large Language Models (LLMs) to simulate a development team and teach the Software Development Life Cycle (SDLC) in a practical, interactive, and personalized manner. The approach was highly rated, proven effective for learning, and faced no reported technical challenges.", "swebokAreas": "Software Engineering Process, Software Requirements", "swebokSubAreas": "Software Engineering Process Fundamentals, Life Cycles,Requirements Elicitation, Persona", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3657604.3664663", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "An Exploratory Study on Upper-Level Computing Students' Use of Large Language Models as Tools in a Semester-Long Project": { "id": "ID13", "level": "B", "year": "2024", "source": "ASEE 2024", "authors": "Ben Arie Tanay, Lexy Arinze, Siddhant S. Joshi, Kirsten A. Davis e James C. Davis", "educationalObjectives": "The study investigates how software engineering students are taught to use Large Language Models (LLMs) in core software lifecycle activities, such as requirements elicitation, design, implementation, testing, and project management. The course teaches, through project-based learning, all stages of the software engineering lifecycle, including technical aspects (such as requirements, design, implementation, and testing) and social aspects (such as ethics and teamwork).", "aiEmployment": "The course adopted a practical, project-based approach, requiring the mandatory use of Large Language Models (LLMs) throughout a semester-long team project. To support this integration, a specific module on LLMs was developed, including a lecture, hands-on instructions, and an applied assignment. Students were also required to describe the responsible use of these tools in their project plans. The study utilized a qualitative methodology, focusing on understanding how computer engineering students used LLMs in a project. Interviews were conducted with nine students at two points during the semester, and the data was analyzed using thematic analysis. This approach allowed for the identification of students' perceptions, experiences, and uses of LLMs in the educational context.", "llm": "ChatGPT and GitHub Copilot", "metrics": "User Satisfaction", "metricsUsage": "Students discussed how Large Language Models (LLMs) increased their productivity, the ease of access to information and solutions, and concerns regarding knowledge retention and dependency.", "resources": "The course utilized materials such as a dedicated AI module, practical assignments (or hands-on tasks), a revised curriculum (or syllabus), and online resources to integrate Large Language Models (LLMs) into software engineering education.", "benefitsAndChallenges": "Students in the course perceived that Large Language Models (LLMs), such as ChatGPT and Copilot, increased their productivity, facilitated access to information and solutions, and fostered greater autonomy in learning. However, they reported challenges such as potential detriment to knowledge retention, the need for technical prerequisites for effective use, the risk of dependency, and dilemmas concerning authorship and ethical use.", "swebokAreas": "Software Engineering Process", "swebokSubAreas": "Software Process Assessment and Improvement,  Life Cycles", "doiOrLink": "https://arxiv.org/pdf/2403.18679", "interaction": "Hybrid", "analysis": "Qualitative", "usedByStudents": "Yes" },
    "An Empirical Study on How Large Language Models Impact Software Testing Learning": { "id": "ID14", "level": "B", "year": "2024", "source": " EASE 2024", "authors": "Simone Mezzaro, Alessio Gambi, e Gordon Fraser", "educationalObjectives": "The article analyzes the use of Large Language Models (LLMs) in teaching software testing through the game Code Defenders and the AI assistant AI Defenders. It focuses on Software Testing, exploring concepts such as mutation testing, mocking, and student performance metrics. The article investigates how generative AI can support the teaching of Software Testing in Software Engineering, focusing on content such as testing concepts, mutation testing, mocking, and the writing of effective tests.", "aiEmployment": "Utilizing the gamified environment Code Defenders and an AI assistant, the study aims to help students comprehend testing technologies, apply concepts in practice, resolve doubts, and develop reasoning and problem-solving skills. The quantitative analysis compared performance with and without the AI, using the Wilcoxon test to check for statistical differences. The qualitative analysis, on the other hand, examined questions asked by students, the AI's responses, and student feedback on utility and expectations. The students' ability to correct errors generated by the AI was also assessed.", "llm": "GPT-3.5", "metrics": "Questionnaires, Error Rate, User Satisfaction.", "metricsUsage": "An optional questionnaire was administered at the end of the session. Code Defenders collected metrics on Failing Tests. Finally, satisfaction was evaluated through the survey responses regarding the perceived utility of the assistant.", "resources": "The gamified platform Code Defenders, an AI assistant (AI Defenders), real Java classes, and a survey for feedback were used.", "benefitsAndChallenges": "The study showed that LLMs, such as GPT, can offer useful explanations and help with conceptual doubts in Software Engineering education, but their unrestricted use may hinder learning. While some students benefited, the majority performed worse. Thus, the use of LLMs must be guided and mediated to prevent unrealistic expectations and misuse.", "swebokAreas": "Software Testing", "swebokSubAreas": "Testing Techniques", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3661167.3661273", "interaction": "Hybrid", "analysis": "Mixed", "usedByStudents": "Yes" },
    "The Impact of Large Language Models on Programming Education and Student Learning Outcomes": { "id": "ID15", "level": "A", "year": "2024", "source": "MDPI", "authors": "Gregor Jo≈°t, Viktor Taneski e Sa≈°o Karakatiƒç", "educationalObjectives": "The article reports on the direct use of Large Language Models (LLMs) such as ChatGPT, Copilot, Bing Chat, and Gemini by students in a programming course. Over nine weeks, they were allowed to freely use these tools to generate code, debug bugs, and seek explanations, simulating a real-world programming environment. The article investigates how the informal and self-directed use of LLMs, such as ChatGPT and Copilot, by Software Engineering students impacts the learning of React with TypeScript and the development of essential skills (or core competencies) such as code generation, debugging, autonomous problem-solving, and seeking explanations.", "aiEmployment": "The students completed four projects over nine weeks, with the freedom to use LLMs for support, in addition to human assistance. Following this phase, they completed a final task without LLMs to assess independent learning. The experiment involved 32 participants and lasted 10 weeks: nine with free LLM use and one with restricted use. During the unrestricted phase, students carried out progressive tasks in React with TypeScript, with permission to use LLMs and support from tutors. In the controlled phase, they completed a task without LLMs, relying only on Google and the official documentation. After this phase, they responded to a questionnaire on their LLM usage habits, with responses collected on a Likert scale.", "llm": "ChatGPT, Google Gemini, Microsoft Copilot and GitHub Copilot.", "metrics": "Questionnaires, Time Comparison", "metricsUsage": "After the completion of the controlled phase task, participants filled out a questionnaire. This questionnaire was designed to gather feedback on their study habits and implementation strategies during the experiment. The study investigated the impact of using LLMs for code generation and debugging on student performance.", "resources": "The main resources utilized were practical programming tasks involving React and TypeScript, freedom to use LLMs, assistance from experienced tutors, and access to Google and official documentation during the AI-free phase.", "benefitsAndChallenges": "The study highlights that LLMs offer benefits such as support for additional explanations, quick access to information, aid in debugging, increased productivity, personalization of learning, and the stimulation of creativity, thus preparing students for an AI-infused future. However, challenges include the risk of excessive dependence that detrimental to critical and independent skills, the generation of incorrect information, and plagiarism.", "swebokAreas": "Computing Foundations", "swebokSubAreas": "Programming Fundamentals and Languages", "doiOrLink": "https://www.mdpi.com/2076-3417/14/10/4115", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project": { "id": "ID16", "level": "A", "year": "2024", "source": "(ICSE 2024)", "authors": "Sanka Rasnayaka, Guanlin Wang, Ridwan Shariffdeen, Ganesh Neelakanta Iyer", "educationalObjectives": "The research investigated the use of LLMs by 214 students working in teams, directly capturing and analyzing the AI-generated code, the prompts used to generate this code, and the levels of human intervention required to integrate the code into the project's codebase. The study investigated the students' autonomy in using LLMs in Software Engineering projects by encouraging them, without imposing, to integrate these tools as they deemed productive. The 214 students were free to choose between tools such as GitHub Copilot, ChatGPT-3.5, and 4, using them for different purposes.", "aiEmployment": "The study employed a project-based approach, simulating industry practices, where students worked in teams to develop a static analyzer in C++ over the course of the semester. The strategy included Agile methodologies, the use of tools such as JIRA and code review, in addition to the active and autonomous integration of LLMs into the workflow. Students also documented their AI usage, and a survey based on the UTAUT model was applied to analyze perceptions and prepare students for effective collaboration with AI.", "llm": "copilot, chatGPT-3.5 and chatGPT-4", "metrics": "Questionnaires, Likert Scale, Accuracy of generated code, User Satisfaction, Level of Human Intervention", "metricsUsage": "A voluntary online survey was conducted with 139 students to collect their perceptions, with many survey questions formulated on a 5-point Likert scale. Student satisfaction and perceptions were measured through the responses to the online survey and the two open-ended questions included. Finally, students were instructed to annotate the AI-generated code with the level of human intervention required to integrate it into the project.", "resources": "The materials used included a course website with information and requirements, code repositories for project development and submission, as well as industry tools like JIRA for Agile management. Practical exercises within the semester project and instructions for annotating AI-generated code were also utilized", "benefitsAndChallenges": "Student perceptions regarding the use of LLMs in the project highlighted benefits such as increased productivity, assistance in the initial phase, support with syntax and debugging, improvement in code quality, and simplification of repetitive tasks. However, concerns were also raised regarding the loss of coding skills, excessive dependence on AI, the need for human intervention to adjust the generated code, the impact on the job market, and the reduction in the tools' utility as project complexity increased.", "swebokAreas": "Software Construction, Software Engineering Process", "swebokSubAreas": "Life Cycles, Construction Technologies", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3643795.3648379", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "AI-Tutoring in Software Engineering Education Experiences with Large Language Models in Programming Assessments": { "id": "ID17", "level": "B", "year": "2024", "source": "International Conference on Software Engineering: Software Engineering Education and Training (ICSESEET)", "authors": "Eduard Frankford, Clemens Sauerwein, Patrick Bassner, Stephan Krusche, and Ruth Breu", "educationalObjectives": "The study specifically explores the experience and interaction of students with Large Language Models (LLMs), a powerful tool, in the context of an Introduction to Programming course. Another focus on the use of AI is its potential to personalize instruction, offer contextualized feedback, and support the development of fundamental skills, such as algorithmic thinking, debugging, and problem-solving. The objective also includes evaluating the efficacy of the AI as a support tool for the work of human tutors, enabling them to concentrate on more significant interactions with students. Students were tasked with solving a specific programming assignment: the implementation of Pascal's Triangle. The novelty of the study was the integration of an AI-Tutor (based on GPT-3.5-Turbo). Students had the option to request feedback from the AI on their current code at any time. The generative AI did not replace instruction but was integrated as a support tool to enhance the feedback experience, allowing students to receive additional and real-time guidance on their code during the problem-solving process.", "aiEmployment": "The study utilized a mixed-methods approach for data collection (combining empirical interaction data and survey data), but the primary analysis was qualitative, focusing on understanding the interaction patterns and the students' experiences with the AI-Tutor. The study's methodology involved three distinct stages: Integration: The GPT-3.5-Turbo model was integrated as an AI-Tutor into the Artemis platform, allowing students to solicit feedback on their code at any time. Practical Application: A practical application was conducted with novice programming students, who were given one week to implement Pascal's Triangle in C. Students had the option to choose whether or not to use the AI-Tutor. Assessment: A questionnaire based on the Technology Acceptance Model (TAM) was administered to evaluate the students' perception of ease of use and usefulness, and to gather qualitative feedback regarding their experience.", "llm": " GPT-3.5", "metrics": "Questionnaires, Likert Scale, User Satisfaction, Quality of AI Feedback", "metricsUsage": "The questionnaire aimed to collect students' experiences and perceptions regarding the use of the AI-Tutor. Responses to these questions were gathered using a Likert scale, and the analysis presented the average sentiment of these responses.", "resources": "The main resources utilized in the educational process included the Artemis system (APAS) with its online code editor, where students implemented Pascal's Triangle in C. Students were provided with a task description, an an initial template, and unit tests for automatic evaluation. Submissions were stored via GitLab, and the data was logged in a database. Following the activity, a questionnaire collected students' perceptions regarding the use of the AI-Tutor.", "benefitsAndChallenges": "The study identified benefits and challenges in using the AI-Tutor. Among the benefits, notable ones include immediate and scalable feedback, support for human tutors, the ability to provide broader feedback than unit tests, ease of adaptation through prompt engineering, and perceived ease of use. On the other hand, the challenges included generic feedback, low interactivity, lack of concrete examples, variable reliability (with 26.6% of the feedback considered unhelpful), and the risk of hindering independent learning.", "swebokAreas": "Computing Foundations", "swebokSubAreas": "Programming Fundamentals and Languages", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3639474.3640061", "interaction": "Fixed Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Diverging assessments: What, Why, and Experiences": { "id": "ID18", "level": "B", "year": "2024", "source": "SIGCSE 2024", "authors": "Amin Sakzad, David Paul, Judithe Sheard, Ljiljana Brankovic, Matthew P. Skerritt, Nan Li, Sepehr Minagar, Simon e William Billingsly", "educationalObjectives": "The article primarily focuses on practical Software Engineering education, addressing the integrated teaching of construction, testing, security, configuration management, and computing fundamentals. The objectives include: teaching how to integrate AI into the problem-solving process, how to formulate effective prompts, and how to critically evaluate the responses obtained.", "aiEmployment": "Each student receives unique data (such as pcap files, VHDs, websites, or compiled programs), which prevents generic answers and encourages self-assessment, debugging, and peer discussion without compromising academic integrity. The strategy also allows and encourages the responsible use of generative AI tools. Independent surveys were conducted at three of the four participating universities, focusing on students' perceptions of the divergent assessments. The analysis yielded overall impressions, highlighting that students valued the assessments for reflecting real industry practices.", "llm": "ChatGPT", "metrics": "Questionnaires", "metricsUsage": "Independent surveys were conducted on the divergent assessments in courses at three of the four participating universities.", "resources": "The article describes the use of personalized assessments featuring unique data for each student, employing industry tools and environments to solve real-world problems. Preparatory materials and support for the use of generative AI were also provided.", "benefitsAndChallenges": "The study highlights that divergent assessments enable and incentivize the use of generative AI tools, promoting active learning, metacognition, and the development of strategic AI usage skills. Among the benefits are the personalization of the learning experience and the training of students for the conscious use of the technology. As a challenge, there are concerns regarding academic integrity, but the assessments maintain their validity by utilizing complex and specific data.", "swebokAreas": "Professional Practice", "swebokSubAreas": "Group Dynamics and Psychology", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3626252.3630832", "interaction": "Hybrid", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Computer Science Education in ChatGPT Era: Experiences from an Experiment in a Programming Course for Novice Programmers": { "id": "ID19", "level": "A", "year": "2024", "source": "MDPI", "authors": "Toma≈æ Kosar, Dragana Ostojiƒá, Yu David Liu e Marjan Mernik", "educationalObjectives": "This study specifically focuses on the use of Large Language Models (LLMs) in the context of Software Engineering education. The research centers on the practical teaching of programming and software construction with AI support, assessing its pedagogical impact, as well as the quality, ethics, and educational processes involved. In this paper, fundamental skills for software developers are highlighted, such as the understanding of basic object-oriented concepts (classes, inheritance, methods), practical coding in C++, code analysis and modification, problem-solving, and critical thinking. The course covered content such as object-oriented programming in C++, practical coding, code analysis, comprehension, and refactoring.", "aiEmployment": "ChatGPT was primarily used by students to optimize and compare code, rather than to generate complete solutions. The study employed a mixed-methods approach, with an emphasis on quantitative analysis and qualitative elements, through a controlled experiment involving 182 introductory programming students divided into two groups. One group was incentivized to use ChatGPT, and the other was not. The quantitative analysis compared academic performance, specifically tasks, exams, and final grades. The qualitative analysis examined student perceptions via feedback questionnaires.", "llm": "GPT-3.5", "metrics": "Questionnaires, User Satisfaction", "metricsUsage": "The study administered initial, weekly, and final questionnaires to collect student data, monitor the use of ChatGPT, and evaluate perceptions and comprehension of tasks. Likert scales were utilized in some of the questionnaires. Participant satisfaction was primarily assessed through the feedback questionnaires.", "resources": "The course utilized a combination of lectures, practical assignments, supporting materials, IDEs (Integrated Development Environments), and paper-based assessments.", "benefitsAndChallenges": "The study identified that ChatGPT offers benefits such as learning support, enhanced code comprehension, motivation, and use for optimization and debugging. However, it also presented challenges, including risks of inaccuracy, excessive dependency, loss of essential skills, plagiarism, and difficulties in use by beginners.", "swebokAreas": "Computing Foundations", "swebokSubAreas": "User and Developer  Human Factors, Programming Fundamentals and Languages", "doiOrLink": "https://www.mdpi.com/2227-7390/12/5/629", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Incorporating Generative AI into Software Development Education": { "id": "ID20", "level": "A", "year": "2024", "source": "Computing Education Practice (CEP 2024)", "authors": "Olga Petrovska, Lee Clift, Faron Moller, e Rebecca Pearsall", "educationalObjectives": "This article addresses Software Engineering education, focusing on the use of generative AI to support the teaching of programming, software construction, testing, and maintenance. Generative AI was employed in the study as a tool for direct integration into assessments. The prompts used in the study were created by the students themselves.", "aiEmployment": "Students generated and analyzed code and textual arguments, compared them with their own work, reflected on their experience with the tool, and discussed its limitations and implications. Data collection included online surveys with students and industry professionals, and experimental assessments involving practical tasks for generating and analyzing code and text. AI was employed both as a programming tool and as an object of critical analysis and ethical discussion.", "llm": "ChatGPT", "metrics": "Questionnaires, Likert Scale, Accuracy of generated code, User Satisfaction.", "metricsUsage": "Three anonymous online surveys were conducted to collect background data and opinions. First-year students evaluated the quality of pre-generated Java code by ChatGPT for a Stack class using a 5-point Likert scale. The summative assessment included the creation and use of a coding scheme to measure sentiment toward different tasks and the usability of GenAI.", "resources": "In the teaching process, the main resources used were ChatGPT for code and text generation and analysis, online surveys for data collection, pre-generated code and code developed by the students, in addition to written reflection activities and discussions on ethical and professional aspects.", "benefitsAndChallenges": "The study revealed that the use of LLMs like ChatGPT in Software Engineering education brings benefits such as increased efficiency, time savings, idea generation, and programming support, in addition to preparing students for the job market. However, challenges exist, including limitations in the quality and robustness of the generated code, difficulties with specific languages, and criticisms regarding the quality of the texts produced.", "swebokAreas": "Professional Practice", "swebokSubAreas": "Professionalism", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3633053.3633057", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Analyzing the uses and perceptions of computer science students towards generative AI tools": { "id": "ID21", "level": "A", "year": "2024", "source": "N√£o identifiquei", "authors": "Alisha HASAN, Bogdan SIMION, Florin POP", "educationalObjectives": "Reinforce knowledge, enhance programming and prompt engineering skills, develop effective learning habits, and teach the ethical and responsible use of AI. Students employ Generative AI (GenAI) as an auxiliary tool to learn concepts, generate code, and review text, treating it as a personalized tutor.", "aiEmployment": "The main focus of the article was to analyze how Computer Science students themselves are already utilizing and perceiving these tools in their learning process. The prompts are created and refined by the students themselves. The study employed a data collection approach using a questionnaire that included Likert scale questions (quantitative) and open-ended questions (qualitative). The quantitative analysis identified patterns of use and perceptions regarding GenAI, while the qualitative analysis extracted themes from the open-ended responses.", "llm": "ChatGPT and GitHub Copilot", "metrics": "Questionnaires, Likert Scale, User Satisfaction", "metricsUsage": "The data was collected using a questionnaire distributed to Computer Science students across three institutions. The questionnaire included Likert scale questions. Students' overall satisfaction was analyzed based on their positive perceptions of GenAI's usefulness and assistance in the learning process.", "resources": "The study does not describe specific pedagogical materials or resources utilized by the institutions.", "benefitsAndChallenges": "The study highlights benefits such as time saving, increased productivity, learning support, and ease of use. Simultaneously, students pointed out challenges like the risk of over-reliance (or over-dependence), inaccuracy of responses, and concerns regarding privacy, bias, and skill degradation (or loss of skills).", "swebokAreas": "Computing Foundations", "swebokSubAreas": "User and Developer  Human Factors, Programming Fundamentals and Languages", "doiOrLink": "https://ibn.idsi.md/sites/default/files/imag_file/ICVL_2024.pdf#page=363", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "A Conceptual Framework to Transform Coding Education in Times of Generative AI": { "id": "ID22", "level": "B", "year": "2024", "source": "Lecture Notes in Informatics (LNI)", "authors": "Stefan Bente, Natasha Randall, Dennis W√§ckerle", "educationalObjectives": "The study addresses the necessity of adapting Software Engineering curricula, specifically focusing on coding education, due to the use of generative AI tools like ChatGPT. Initially, the goal is for students to learn the basics of coding, applying syntax rules to small-scale tasks.", "aiEmployment": "The objective is to teach students how to strategically interact with and utilize generative AI tools within the context of coding education, in addition to ensuring the development of fundamental programming skills. Instead of using AI merely as a tool to generate code for simple exercises, the study seeks the critical evaluation of AI's capabilities and limitations in coding, and the strategic and autonomous integration of AI in solving high-complexity Software Engineering problems. The article adopts an educational strategy based on a framework that guides coding instruction in Software Engineering. This approach organizes the students' learning journey into three stages based on Bloom‚Äôs Revised Taxonomy and the complexity of the coding tasks. Stage 1: The focus is on coding fundamentals, with the prohibition of using AI tools such as ChatGPT and GitHub Copilot. The strategy is centered on hands-on experience, allowing students to develop technical knowledge without resorting to shortcuts. Stage 2: Students advance to what is called \"\"unassisted coding\"\", where the use of AI is permitted and explicitly integrated into the pedagogical activities. In this phase, activities are proposed such as analyzing AI-generated code, comparing it with self-developed solutions, refactoring, debugging, and reflecting on the use of the tools. The main goal is to develop AI literacy among participants, empowering students to identify the strengths and limitations of these technologies and to use them critically and consciously. Stage 3: Students are given broader-scoped tasks involving the unrestricted and implicit use of AI tools.", "llm": "ChatGPT 4.0, ChatGPT 3.0 and GitHub Copilot", "metrics": "Time Comparison, Ability to Handle Scope and Complexity", "metricsUsage": "The study explicitly analyzed how the AI's capability varied with the scope (code size) and the cognitive complexity (Bloom's level) of the tasks.", "resources": "The main materials and resources utilized in the teaching process include coding exercises and tasks with varying levels of complexity and modules, digital examinations conducted in the laboratory, generative AI tools, task descriptions, as well as development environments and computer labs.", "benefitsAndChallenges": "Among the benefits, the study highlights the ability to quickly and accurately generate low-scope code, assist in error detection, support the learning of new technologies, and perform effective refactoring. Conversely, the challenges involve the risks of misuse (such as replacing learning with copying), cognitive overload for beginners, difficulties in tasks with complex logic or architectural principles, and limitations in assessment, requiring new exam formats and grading criteria.", "swebokAreas": "Professional Practice, Software Construction", "swebokSubAreas": "Professionalism, Practical Considerations", "doiOrLink": "https://dl.gi.de/server/api/core/bitstreams/64e215b9-70d1-427d-ba07-550a21a1f8f6/content", "interaction": "Fixed Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Twisted Games: A First Experience of Inclusion of AI tools in First Year Programming Classes": { "id": "ID23", "level": "A", "year": "2024", "source": "Latin American Conference on Computational Intelligence (LA-CCI)", "authors": "Pedro Wightman", "educationalObjectives": "This mapping specifically addresses the integration of Large Language Models (LLMs) within the context of Software Engineering education. It aims to develop students' capacity to define and adapt high-level requirements, craft effective prompts, test and debug AI-generated code, and integrate these components into existing systems. Students were responsible for defining and adapting requirements, utilizing LLMs in an iterative development process, creating and refining prompts, debugging code, and making project decisions such as the programming language and the specific AI tool used. The research indicated that the majority felt capable of defining requirements, although many found it challenging to express them in effective prompts.", "aiEmployment": "The study adopted a Project-Based Learning (PBL) strategy, integrating collaboration with generative AI tools in an introductory programming course. Students developed modified versions of classic games with direct support from Large Language Models (LLMs) of their choice, such as ChatGPT and Copilot, following an iterative process of prompt creation, evaluation, debugging, and refinement. The research employed a mixed-methods approach to evaluate the students' experience using Artificial Intelligence (AI) tools in programming classes, combining qualitative and quantitative analyses. The qualitative analysis was based on the final reports prepared by the student groups. These reports included records of development milestones, the prompts used, screenshots, and reflections on the process and the final outcome. The quantitative analysis, in turn, was conducted using a questionnaire administered to all students, which collected data on the use of the AI tools and their perception of the experience. The results showed that the majority of students used ChatGPT, followed by other tools like Claude, Copilot, and Gemini, primarily for research and text generation.", "llm": "ChatGPT was the most used generative AI tool and considered most effective by students, followed by Claude, Copilot, and Gemini.", "metrics": "Questionnaires, Time Comparison, User Satisfaction", "metricsUsage": "A survey was administered to all students to assess their perception and the impact of the experience on their programming competencies (or skills). Students evaluated the perceived time savings. Ninety-two percent (92%) of students agreed that the AI tool saved them time on coding compared to starting from scratch. The overall satisfaction and student perception ultimately indicated a positive student experience.", "resources": "The study utilized modified and challenging game projects as its practical basis. The process involved iterative interaction with the AIs, documentation in final reports, and a perception survey.", "benefitsAndChallenges": "The study's findings indicate that students had a positive perception regarding the use of LLMs in programming classes, highlighting benefits such as time savings, assistance with coding, useful feedback, and improvements in professional skills (or competencies). The majority intend to continue using these tools in the future. However, they faced challenges with prompt creation, debugging, and the learning process for the tools themselves.", "swebokAreas": "Software Construction, Computing Foundations", "swebokSubAreas": "Practical Considerations, Programming Fundamentals and Languages, User and Developer Human Factors", "doiOrLink": "https://ieeexplore.ieee.org/document/10814750?denied=", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Investigating Developers' Preferences for Learning and Issue Resolution Resources in the ChatGPT Era": { "id": "ID24", "level": "A", "year": "2024", "source": "International Conference on Software Maintenance and Evolution (ICSME 2024)", "authors": "Ahmad Tayeb, Mohammad Alahmadi, Elham Tajik e Sonia Haiduc", "educationalObjectives": "The paper focuses on Education, specifically within the context of programming learning, coding problem-solving, and technology exploration, utilizing AI tools as complements or substitutes for traditional methods such as video and written tutorials. The study highlights the use of AI for concept comprehension, code development and refinement, personalized learning support, information synthesis, task automation, and, to a lesser extent, academic assistance and ethical considerations.", "aiEmployment": "The principal uses include queries for concept explanation, problem-solving and debugging, code generation and refinement, learning with personalized support, exploration of new technologies, information synthesis, and automation of practical tasks. The study employed a mixed-methods approach to analysis, combining quantitative and qualitative methods. The quantitative analysis was based on questionnaires with multiple-choice and ranking questions, allowing for the calculation of the average preference for different learning resources and the frequency of ChatGPT use. The qualitative analysis, on the other hand, was performed using 1,520 open-ended responses.", "llm": "ChatGPT", "metrics": "Questionnaires, User Satisfaction, Frequency of LLM Use", "metricsUsage": "The primary data collection tool was a questionnaire structured into four sections. The perceived advantages of the AI chatbots and frequent usage serve as indirect indicators of user satisfaction or acceptance regarding the features. Responses indicated usage levels such as weekly, daily, monthly, rarely, or never. ", "resources": "The materials utilized included video and written tutorials, API documentation, AI chatbots, question-and-answer forums (or Q&A forums), search engines, discussion platforms, and books.", "benefitsAndChallenges": "The main reported benefits include efficiency, accessibility, personalized responses, on-demand educational support, technical coding assistance, information summarization, and interactivity. Participants valued the speed of the responses and the AI's adaptability to their queries. On the other hand, the most frequently cited challenges were the limited reliability of the responses, the risk of over-reliance (or excessive dependency), technical limitations, difficulties in prompt formulation, barriers to deep understanding, and ethical concerns.", "swebokAreas": "Professional Practice", "swebokSubAreas": "Professionalism", "doiOrLink": "https://ieeexplore.ieee.org/document/10795042?denied=", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "How LLMs Aid in UML Modeling: An Exploratory Study with Novice Analysts": { "id": "ID25", "level": "A", "year": "2024", "source": "Software Services Engineering (SSE 2024)", "authors": "Beian Wang, Chong Wang, Peng Liang, Bing Li eCheng Zeng", "educationalObjectives": "The article investigates how Large Language Models (LLMs), such as ChatGPT, assist students in the creation of UML diagrams from textual descriptions. Furthermore, it discusses AI fundamentals, professional practices, and educational and industrial implications. The educational objectives in Software Engineering, within the context of requirements analysis and UML modeling, include: Teaching students how to create UML models (use case, class, and sequence diagrams). Equipping them to use LLMs as auxiliary tools with a critical eye and without excessive dependency. The study aims to teach and assess the skill of creating UML models (use case, class, and sequence diagrams) with the support of LLMs, developing students' proficiency in the critical use of these tools.", "aiEmployment": "The study utilized a Project-Based Learning (PBL) approach, in which undergraduate Software Engineering students applied their UML modeling knowledge with the aid of LLMs such as ChatGPT. The central activity involved the creation of models for a case study, with the submission of diagrams and the interaction history with the AI. The study employed a predominantly quantitative methodology, with qualitative elements in the interpretation of the results. Conducted with 45 Software Engineering students, the experiment involved the creation of three UML models (use case, class, and sequence diagrams) with the mandatory assistance of LLMs, such as ChatGPT. The output formats and the interaction patterns with the LLMs were also assessed, highlighting that the best results were obtained through hybrid approaches involving human intervention.", "llm": "ChatGPT 3.5 and ChatGPT 4.0", "metrics": "Correction Rate", "metricsUsage": "For each evaluation criterion defined for the UML models, the correction rate was calculated.", "resources": "In the experiment, the students used a case study, LLMs, and tools such as StarUML to create UML models.", "benefitsAndChallenges": "The study showed that LLMs, such as ChatGPT, can assist novice students in UML model creation, proving effective at identifying elements like classes, use cases, and objects. However, they face difficulties in correctly representing the relationships between these elements and may generate inconsistent output or information loss.", "swebokAreas": "Software Requirements, Software Design", "swebokSubAreas": "Requirements Specification, Recording Software Designs", "doiOrLink": "https://ieeexplore.ieee.org/document/10664407?denied=", "interaction": "Free Prompts", "analysis": "Quantitative", "usedByStudents": "Yes" },
    "Teaching UML using a RAG-based LLM": { "id": "ID26", "level": "A", "year": "2024", "source": "International Joint Conference on Neural Networks (IJCNN)", "authors": "Pasquale Ardimento, Mario Luca Bernardi e Marta Cimitile", "educationalObjectives": "The article's primary focus is the teaching of UML in Software Engineering courses. It aims to understand and improve student behavior and capabilities during modeling activities. The system seeks to support professors in student monitoring and assessment and reduce the teacher's effort. (Software Engineering Models and Methods) (Modeling, Methods, and Techniques). The paper proposes an approach utilizing a cloud-based tool that employs a Large Language Model (LLM) with Retrieval-Augmented Generation (RAG) to provide personalized feedback to students.", "aiEmployment": "The strategy involves the integration of a supervised and non-intrusive modeling environment capable of automatically capturing and analyzing the UML diagrams created by students. Using the UML Info plugin, the graphical diagrams are converted into text and stored in a vector repository. When a student or professor submits a prompt, the system retrieves relevant information and sends it, along with the context, to LLaMA, which then generates personalized feedback. In the quantitative analysis, the objective was to measure the precision, recall, and F1 score of the system's responses in identifying entities and relationships within the UML diagrams. A set of 422 diagrams from the ModelSet database were used, and the system's responses were compared against expert-provided annotations. The results showed that the configuration with RAG (Retrieval-Augmented Generation) consistently outperformed the non-RAG version across all metrics, proving to be more relevant than simply increasing the size of the LLaMA model. For the qualitative analysis, 12 participants (students, experts, and novices) interacted with the system and responded to a questionnaire containing open- and closed-ended questions. The analysis indicated a significantly positive perception regarding the usability, accuracy of suggestions, and utility of the feedback, based on statistical tests.", "llm": "LLaMA", "metrics": "Questionnaires, Likert Scale, T-tests", "metricsUsage": "T-tests were performed to test whether the users' perception of the system was significantly better than a neutral value. A comprehensive questionnaire was administered to 12 participants following their interaction with the system. The questionnaire contained both closed-ended and open-ended questions. Responses to the closed-ended questions were quantified using a Likert scale from 1 to 5.", "resources": "The study utilizes UML diagrams created by students, a cloud-based modeling environment, and LLaMA models for automated feedback generation as its primary resources.", "benefitsAndChallenges": "The main benefits include providing automated and immediate feedback, which assists students in correcting errors and improving their capabilities, as well as reducing teacher effort in monitoring and assessment. On the other hand, the study also identified challenges, such as the precision limitations of the LLMs, the influence of biases in the training data, the difficulty of replicating results due to the variability in responses, and the limitation in generalizing the findings, since the utilized data is specific to the aviation industry.", "swebokAreas": "Models and Methods", "swebokSubAreas": "Modeling, Analysis of Models", "doiOrLink": "https://ieeexplore.ieee.org/document/10651492?denied=", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Cultivating Software Quality Improvement in the Classroom: An Experience with ChatGPT": { "id": "ID27", "level": "B", "year": "2023", "source": "(CSEE&T) 2024", "authors": "Eman Abdullah AlOmar e Mohamed Wiem Mkaouer", "educationalObjectives": "The study addresses various educational objectives in Software Engineering, such as fostering a culture of bug fixing and software quality improvement through the use of tools like PMD, as well as developing students' code debugging skills and their ability to recognize poor programming practices. It also aims to promote the critical use of LLMs like ChatGPT, stimulating students' analytical reasoning, the validation of generated responses, and awareness of their limitations. Students utilized the tool to analyze problems identified by PMD, decide on their relevance, and suggest corrections. A total of 102 submissions were analyzed, with students being instructed to critically validate the model's responses. The study also included prior training and required students to report their use of ChatGPT. According to the survey, 85.9% considered the tool useful in the debugging process.", "aiEmployment": "The article proposes the use of ChatGPT as a support tool in Software Engineering and Computer Science education, focusing on software quality improvement and bug fixing. The contents and skills taught to students include: code debugging, identification and refactoring of poor practices, understanding of static code analysis, and development of analytical and critical thinking. The study adopted a mixed-methods approach, combining quantitative and qualitative analyses, with the goal of investigating the use of PMD and ChatGPT in teaching software quality assurance. Conducted within a course involving 102 undergraduate and graduate students in Software Engineering and Computer Science, the study involved the inspection of open-source Java projects using PMD to identify and discuss quality issues. Data were collected from 102 submissions, totaling 1,230 analyzed rules, and via an online survey with open- and closed-ended questions regarding the students' experience with ChatGPT. The quantitative analysis examined the types of problems selected, the correction time by category, and the degree of success of ChatGPT based on the feedback. The qualitative analysis involved the categorization of student comments and the evaluation of the produced artifacts.", "llm": "ChatGPT", "metrics": "Questionnaires, Time Comparison, Accuracy of Generated Code", "metricsUsage": "The study utilized an online survey (with 6 open-ended and 16 multiple-choice questions) to collect feedback from 102 students regarding their use of ChatGPT, addressing demographics, utility, and accuracy. The correction time by problem type and the quality of the generated solutions were also analyzed. Although 85.9% considered ChatGPT useful, 20.4% reported regressions, and 13.6% noted new violations generated by the corrections.", "resources": "The activity was integrated into a software quality course and included lectures, supporting materials, detailed instructions, and an online questionnaire to collect student feedback on the use of ChatGPT.", "benefitsAndChallenges": "Among the benefits, key points include debugging automation, improved code quality, detailed explanations, ease of use, and the stimulation of critical thinking. Students also perceived ChatGPT as a helpful \"\"coding companion,\"\" especially for novices. Conversely, the main challenges included its limitation in understanding complex contexts, the introduction of regressions, dependence on prompt quality, lack of originality, difficulty with propagated problems (e.g., those arising from cascading errors), and the necessity of human validation.", "swebokAreas": "Software Quality", "swebokSubAreas": "Software Quality Assurance Process", "doiOrLink": "https://ieeexplore.ieee.org/document/10663028?denied=", "interaction": "Hybrid", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Using Generative AI to Create User Stories in the Software Engineering Classroom": { "id": "ID28", "level": "B", "year": "2024", "source": "Conference on Software Engineering Education and Training (CSEE&T)", "authors": "Allan Brockenbrough e Dominic Salinas", "educationalObjectives": "The educational objectives in Software Engineering, with the introduction of Generative AI, include: emphasis on skills beyond coding, such as requirements, design, and testing; teaching the fundamentals of requirements engineering, including structured user stories, Definition of Done (DoD), and traceability; and evaluating artifact quality using frameworks like INVEST. The article aims to teach the fundamentals of requirements engineering, focusing on the structured creation of user stories, including elements such as Definition of Done and traceability.", "aiEmployment": "Students performed a practical task based on user feedback and were divided into two groups: one utilizing ChatGPT 3.5 and one without. The strategy promoted the creation and critical evaluation of requirements both with and without AI assistance, encouraging the conscious use of technology. The study conducted a quantitative analysis to evaluate the quality and creation time of user stories with and without the use of ChatGPT. The user stories were assessed by two professors based on the INVEST model. A total of 75 user stories were analyzed (38 with AI, 37 without). The task completion time was also measured. The results showed that the AI group produced higher-quality stories and was, on average, 38.1% faster.", "llm": "ChatGPT 3.5", "metrics": "Time Comparison, User Stories", "metricsUsage": "The study analyzed the quality of the user stories using an INVEST classification system. The study also investigated the impact of ChatGPT on the time students took to create the user stories.", "resources": "Materials used included a theoretical lecture on user stories and an explanation of the standard format. Following the lecture, students performed a practical task based on fictitious feedback.", "benefitsAndChallenges": "The use of ChatGPT in user story creation resulted in 17% higher quality, a greater proportion of high-quality stories (58%), and 38.1% faster completion. It excelled particularly in grammar and traceability, but showed limitations in grouping related feedback, creating detailed Definitions of Done, and maintaining consistency. Despite being useful, ChatGPT did not guarantee consistent quality and required critical review from the students.", "swebokAreas": "Software Requirements", "swebokSubAreas": "Requirements Elicitation, Requirements Specification", "doiOrLink": "https://ieeexplore.ieee.org/document/10662994?denied=", "interaction": "Fixed", "analysis": "Quantitative", "usedByStudents": "Yes" },
    "Automated Programming Exercise Generation in the Era of Large Language Models": { "id": "ID29", "level": "C", "year": "2024", "source": "CSEE&T 2024", "authors": "Niklas Mei√üner, Sandro Speth, Steffen Becker", "educationalObjectives": "Generative AI is used to assist in the creation of exercises aimed at developing practical programming skills and the comprehension of fundamental and advanced concepts within a beginner-to-intermediate-level programming course.", "aiEmployment": "The underlying teaching strategy is practice-based and problem-solving learning, where students acquire programming skills by executing exercises on specific topics. The study employed a mixed-methods analysis, combining qualitative and quantitative approaches. The qualitative component involved the manual assessment of the quality of the exercises generated by the LLM tools, based on criteria such as accuracy, relevance, and completeness, in addition to the researchers' usability analysis of the tools. The quantitative analysis, on the other hand, considered the number of prompts required to obtain satisfactory responses, as a measure of the simplicity of the generation process.", "llm": "ChatGPT, Bing AI Chat and Google Bard", "metrics": "Quality of Generated Exercises", "metricsUsage": "The quality analysis of the generated exercises was predominantly qualitative, with complementary categorization. It utilized the evaluation criteria to classify the exercises as \"\"poor,\"\" \"\"mediocre\"\" (or \"\"average\"\"), or \"\"good.\"\" The assessment was conducted manually by the researchers, considering aspects such as adherence to object-orientation, creativity, correctness, use of API descriptions, and the inclusion of intentional errors.", "resources": "The materials included Java code, API descriptions, and tasks covering flow control, advanced Object-Oriented Programming (OOP), and error identification. These resources were used to support practical programming instruction.", "benefitsAndChallenges": "Among the main benefits are the simplification and efficiency of the creation process, the generation of initial drafts and quality variants, and the standout performance of ChatGPT with GPT-4 as the most effective tool, especially for topics like OOP (Object-Oriented Programming), API usage, flow control, and error identification. On the other hand, the challenges include the need for manual refinement of the generated exercises, error patterns in complex concepts, and the limited performance of other tools like Bing AI Chat and Google Bard, which struggled to produce useful exercises.", "swebokAreas": "Computing Foundations", "swebokSubAreas": "Programming Fundamentals and Languages", "doiOrLink": "https://ieeexplore.ieee.org/document/10662984/?denied=", "interaction": "Fixed", "analysis": "Mixed", "usedByStudents": "No" },
    "Students' Perspectives on AI Code Completion: Benefits and Challenges": { "id": "ID30", "level": "A", "year": "2024", "source": "Annual Computers, Software, and Applications Conference (COMPSAC)", "authors": "Wannita Takerngsaksiri, Cleshan Warusavitarne, Christian Yaacoub, Matthew Hee Keng Hou, Chakkrit Tantithamthavorn", "educationalObjectives": "The article discusses the perspectives of computer science students on AI code assistance in the context of programming education. Through an analysis of student benefits, challenges, and expectations, it highlights the importance of students developing a deep understanding of programming concepts, strong problem-solving skills, creativity, the ability to critically evaluate code, and adherence to best practices. The issue of over-reliance on automation is also addressed.", "aiEmployment": "The students utilized the AutoAurora tool as a Visual Studio Code extension to complete programming tasks; specifically, the students used AutoAurora as an intelligent coding assistant within Visual Studio Code to aid in programming problem-solving. The participants used this tool to perform two competitive programming tasks in Python, involving string, file, and array manipulation. During the activities, the AI generated real-time code suggestions, which the students could accept or ignore. Furthermore, they were able to adjust the tool's settings, such as the number of lines and suggestions generated, to explore its functionality and potential. The analysis conducted in the study was qualitative. Ten undergraduate computer science students from Monash University, Australia, were interviewed. The participants were aged between 18 and 22 and had at least one year of programming experience. The participants used the AutoAurora tool to complete two competitive programming tasks in Python. The tasks included string and text file manipulation, and array manipulation. Following the completion of the tasks, an interview composed of open-ended questions was conducted to collect participant feedback on code completion tools in the educational context. The interviews were recorded and transcribed.", "llm": "AutoAurora, a tool developed by the researchers themselves to serve as a research instrument.", "metrics": "User Satisfaction and Open-Ended Question Interviews", "metricsUsage": "The students' perceptions regarding the benefits, challenges, and expectations of AI code completion were the central focus of the study. Interviews were the primary data collection methodology. The data were recorded and transcribed. Grounded Theory was used to analyze the interview transcripts.", "resources": "The AutoAurora extension for Visual Studio Code, specifically developed for the study, is based on the StarCoder AI model and offers code completion suggestions. Two practical programming tasks in Python were applied, focusing on string, text file, and array manipulation, such as reading and processing files and generating modified outputs.", "benefitsAndChallenges": "The study highlights that AI tools, such as coding assistants, can boost productivity, aid in syntax learning, and suggest best practices and alternative solutions, thus serving as a support in the coding process. Conversely, it warns of challenges, such as the risk of over-dependence, the need to critically evaluate the AI-generated code, and the importance of revising assessment methods, including documentation, oral interviews, and randomized evaluations. Furthermore, students expect the tools to offer code explanations, refactoring suggestions, and customization, which opens the door for future improvements in the use of AI in programming education.", "swebokAreas": "Computing Foundations", "swebokSubAreas": "Programming Fundamentals and Languages", "doiOrLink": "https://arxiv.org/pdf/2311.00177", "interaction": "Free Prompts", "analysis": "Qualitative", "usedByStudents": "Yes" },
    "Developing Critical Thinking Practices Interwoven with Generative AI usage in an Introductory Programming Course": { "id": "ID31", "level": "B", "year": "2024", "source": "EDUCON 2024", "authors": " Arne Styve, Outi T. Virkki, e Usman Naeem", "educationalObjectives": "The main motivation of the study was to investigate how to design learning activities that introduce generative AI tools (GitHub Copilot and ChatGPT) for programming, while promoting critical thinking practices among students in an introductory programming course. The key objectives include: fostering comprehensive mastery in programming, moving beyond simple language learning toward building robust and sustainable solutions; adopting good Software Engineering practices from the outset, with emphasis on code quality and readability, coding style, design principles such as SOLID, unit testing, and robustness. The course also seeks to teach the responsible use of generative AI tools, such as GitHub Copilot and ChatGPT, by encouraging critical thinking to evaluate the reliability and security of the generated code. The paper reports that students directly utilized tools based on Large Language Models (LLMs) in an introductory programming course. Specifically, they used GitHub Copilot as an aid for generating code and ChatGPT 3.5 as a \"\"peer reviewer\"\" to analyze that code. The study's goal was to integrate these tools into the learning process while simultaneously encouraging the development of students' critical thinking skills.", "aiEmployment": "The proposal fostered reflection on the use of AI, promoting individual responsibility for software quality. Pre- and post-intervention research assessed changes in critical thinking practices, and the assessment was conducted via a portfolio, rather than a written examination, favoring an approach more aligned with the practical use of AI. The study adopted a mixed-methods methodology, with an emphasis on quantitative data, characterizing it as a study. It was conducted in an introductory Object-Oriented Programming (Java) course aimed at first-semester students. The main intervention was the \"\"AI with CT\"\" task, which integrated the use of the GitHub Copilot and ChatGPT tools with the objective of promoting the responsible use of AI and the development of critical thinking. Data collection combined quantitative and qualitative instruments. Out of the 90 students in the course, 55 responded to the questionnaires, and 88 submitted the assignment with their reflections.", "llm": "ChatGPT 3.5 and GitHub Copilot", "metrics": "Questionnaires, Likert Scale, User Satisfaction, Behavioral Change Analysis", "metricsUsage": "Questionnaires were administered before and after the intervention to collect students' opinions and practices related to programming and studying. Students were asked to critically analyze the code suggested by GitHub Copilot concerning the criteria for quality and robustness taught in the course. The responses from the pre- and post-intervention questionnaires were compared to observe changes in students' opinions and behaviors.", "resources": "The study utilized generative AI tools (GitHub Copilot and ChatGPT 3.5) integrated into IDEs such as IntelliJ and VS Code. Questionnaires and open-ended reflections were used to assess the development of students' critical thinking.", "benefitsAndChallenges": "Among the benefits, highlights include support for productivity, the reinforcement of understanding regarding code quality and robustness, and the stimulation of critical reflection, preparing students for the responsible use of these tools in the job market. Conversely, the challenges include the generation of inaccurate or incomplete code, risks to quality and security, a potential reduction in students' critical thinking and sense of responsibility, as well as difficulties in educational assessment and in combating the uncritical use of AI tools.", "swebokAreas": "Software Quality, Professional Practice", "swebokSubAreas": "Professionalism, Software Quality Assurance Process", "doiOrLink": "https://ieeexplore.ieee.org/document/10578746?denied=", "interaction": "Fixed", "analysis": "Mixed", "usedByStudents": "Yes" },
    "How Far Are We? The Triumphs and Trials of Generative AI in Learning Software Engineering": { "id": "ID32", "level": "A", "year": "2024", "source": "ICSE 2024", "authors": "Rudrajit Choudhuri, Dylan Liu, Igor Steinmacher, Marco Gerosa, Anita Sarma", "educationalObjectives": "The study's educational objective was to equip students with the ability to use APIs, develop skills for identifying and correcting code errors, improve code quality through the identification and correction of code smells, and master tools such as Git and GitHub. These objectives were defined to develop practical and contextualized competencies, especially in tasks requiring decision-making, critical analysis, and the application of development best practices. The article sought to teach and assess Software Engineering skills, such as API usage (interaction with the NOAA_SDK library), code debugging (correcting functions with incorrect logic, errors in API usage, and issues in web scraping with Selenium), code quality improvement (removal of code smells such as global variables, unused imports, dead code, and magic numbers), and version management with Git and GitHub.", "aiEmployment": "The study employed a mixed-methods analysis, combining quantitative and qualitative methods to evaluate the effectiveness and limitations of generative AI. The research utilized a cross-over experimental design, dividing the students into two groups: one that exclusively used ChatGPT (GPT-4) and another that used traditional resources without generative AI. The approach centered on task-based learning in Software Engineering, featuring three main challenges: (1) debugging and API usage (such as NOAA_SDK and Selenium), (2) code quality improvement, focusing on the removal of code smells, and (3) version management with Git and GitHub, including the creation of branches, commits, and pull requests. The tasks were developed in collaboration with the instructor, aligned with the course objectives, and adjusted to the level of the students, who were novices in SE. For the experimental group, ChatGPT provided contextualized support via natural language, assisting them in overcoming the challenges. Furthermore, the study assessed the tool's impacts on productivity, self-efficacy, future usage intention, and the perceived limitations of AI in the learning process.", "llm": "ChatGPT 4.0", "metrics": "Questionnaires, Accuracy of Generated Code", "metricsUsage": "An adapted questionnaire was used to assess the students' perception of their ability to perform the tasks. It was administered both before and after the tasks. Additionally, the students' satisfaction regarding the use of ChatGPT was evaluated.", "resources": "The materials used included ChatGPT (GPT-4) as the exclusive tool for the experimental group and traditional online resources, excluding generative AI, for the control group. The tasks were developed in collaboration with the instructor, aligned with the course objectives, addressing API usage, debugging, code quality, and version management.", "benefitsAndChallenges": "The perceived benefits of using ChatGPT were the possibility of natural language dialogue, contextualized help, and support with general information, such as Git commands. There was also a slight improvement in the removal of \"\"code smells.\"\" However, the challenges stood out, including increased frustration, higher cognitive load, and a perception of lower performance. There were no significant gains in overall productivity or self-efficacy, and students even experienced a drop in confidence regarding their comprehension of Python code when using ChatGPT.", "swebokAreas": "Software Construction, Software Quality", "swebokSubAreas": "Debugging, Code Smells, Software Construction Tools, Practical Considerations, Software Quality Assurance Process", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3597503.3639201", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Analyzing the Learning Effectiveness of Generative AI for Software Development for Undergraduates in Sri Lanka": { "id": "ID33", "level": "A", "year": "2024", "source": "International Research Conference on Smart Computing and Systems Engineering (SCSE)", "authors": "Pramodya Samarakoon, Shantha Jayalal, Dinesh Asanka, Nirasha Jayalath", "educationalObjectives": "The core motivation of the research was to investigate the effectiveness of Generative AI in enhancing students' coding proficiency. The study seeks to evaluate the impact of Generative AI on learning outcomes, student engagement, and the overall educational experience. The educational objectives of the study focus on improving the coding proficiency of university students, fostering active learning, the acquisition of practical skills (or competencies), and the application of knowledge in real-world contexts.", "aiEmployment": "The study adopted an instructional strategy based on coding problem-solving, augmented by the use of Generative AI. This approach was divided into three phases: a pre-assessment to measure students' initial proficiency without AI, an intermediate assessment where participants utilized Generative AI tools to solve progressive Object-Oriented Programming (OOP) problems, and a post-assessment to verify retention and understanding after the intervention. Additionally, the study employed scenario-based questions derived from 18 objective criteria and utilized Google Classroom as the platform for activity application and data collection. The study employed a quantitative approach to evaluate the effectiveness of Generative AI in enhancing coding proficiency, without the use of qualitative methods. The research was conducted with 57 undergraduate students and involved three assessment phases, all focused on Object-Oriented Programming (OOP). The assessments, delivered via Google Classroom, consisted of scenario-based questions analyzed against 18 objective code quality criteria, including readability, functionality, and documentation. A demographic questionnaire collected information about the participants to investigate the influence of variables such as age, gender, and preferred programming language. The data were analyzed using mean scores, which indicated a learning gain of (0.33), revealing progress in learning with the use of Generative AI.", "llm": " ChatGPT", "metrics": "Questionnaires, Accuracy of Generated Code, Pre-test, Mid-test, and Post-test Scores", "metricsUsage": "The study administered three assessments to measure coding proficiency: a pre-assessment (without AI), an intermediate assessment (with the use of Generative AI), and a post-assessment (without AI). The intermediate assessment specifically focused on the quality and originality of the solutions produced. The mean scores for the assessments were: 11.54 (pre), 14.49 (intermediate), and 13.66 (post), out of a maximum of 18 points. The code submitted by the participants was subjected to manual review (or manual grading). Finally, a questionnaire was used to collect data on participant demographics, such as gender, age, academic level (or year of study), and field of study.", "resources": "The study utilized Generative AI tools to support coding problem-solving in scenario-based Object-Oriented Programming (OOP) assessments. The activities were administered via Google Classroom, with participants having the flexibility to choose their programming language.", "benefitsAndChallenges": "The study identified several benefits of using Generative AI in software development education, such as improved coding proficiency, real-time assistance, learning personalization, support for students with low experience, and aid in problem-solving. However, several challenges were also highlighted, including copyright concerns, risks of academic dishonesty (or fraud), limitations in AI accuracy, difficulties with non-textual descriptions, and variations in learning gains.", "swebokAreas": "Software Construction, Computing Foundations", "swebokSubAreas": "Programming Fundamentals and Languages, Practical Considerations", "doiOrLink": "https://ieeexplore.ieee.org/document/10550837?denied=", "interaction": "Free Prompts", "analysis": "Quantitative", "usedByStudents": "Yes" },
    "ChatGPT as a Software Development Bot: A Project-Based Study": { "id": "ID34", "level": "A", "year": "2024", "source": "ENASE 2024", "authors": " Muhammad Waseem, Teerath Das, Aakash Ahmad, Peng Liang, Mahdi Fahmideh, Tommi Mikkonen", "educationalObjectives": "The study aimed to evaluate the role of ChatGPT as a support tool throughout the various phases of the software development lifecycle, as performed by students. Furthermore, the paper addressed recurring challenges faced by students, such as difficulties in understanding software architecture, system integration, program design, tool usage, in addition to deficiencies in the deployment and software management stages. The paper has the following research questions: \"\"What are the impacts, advantages, and limitations of using ChatGPT as a development bot in each phase of the software lifecycle?\"\" \"\"How did the use of ChatGPT in projects influence students' learning curve and skill development?\"\" \"\"To what extent did the use of ChatGPT contribute to improving students' proficiency in software development concepts?\"\" \"\"What challenges did students face when utilizing ChatGPT as a development bot?\"\" The educational objectives in Software Engineering are to empower students with the knowledge, skills, and techniques for all phases of the software development lifecycle. This includes understanding architectural concepts, system integration, debugging, program design, error identification, and production of quality code.", "aiEmployment": "The seven undergraduate students were engaged in three publicly announced software projects over a three-month period. These projects were for real clients. ChatGPT (versions GPT-3.5 and GPT-4) was utilized to support the students across all phases of the software development lifecycle (SDLC). The projects included activities such as requirements gathering, CI/CD setup, development, testing, and deployment, in addition to collaboration with real clients. Learning occurred both through the use of ChatGPT and by overcoming the challenges associated with the tool. The methodology involved the use of real-world software projects with clients, public repositories for collaboration, CI/CD infrastructure, interaction with specialists, and ChatGPT as a support tool across all development phases.", "llm": "GPT-3.5 and GPT-4", "metrics": "Questionnaires, Likert Scale, Time Comparison, Accuracy of Generated Code, User Satisfaction", "metricsUsage": "Entry questionnaires (53 questions) and exit questionnaires (114 questions), both utilizing closed-ended questions on a 5-level Likert scale, were employed to collect data regarding the students' experience. The analysis focused on ChatGPT's effectiveness in reducing the time required for tasks such as requirements definition, as well as assessing the improvement in code quality, including error reduction and early defect detection. Student satisfaction regarding ChatGPT's contributions during the software development process was also investigated.", "resources": "Real-world software projects with clients, public repositories for collaboration, CI/CD infrastructure, interaction with specialists, and ChatGPT as a support tool across all development phases were utilized.", "benefitsAndChallenges": "The article evidenced that ChatGPT had a positive impact across all phases of software development, improving clarity, code quality, testing, and deployment, although it has limitations such as the risk of dependency, quality challenges, and ethical concerns. Students reported that using ChatGPT accelerated the learning curve, enhanced technical and problem-solving skills, and fostered innovative thinking. There was also a consensus regarding the enrichment of knowledge in software development. Despite the ease of use, the challenges faced were considered learning opportunities, including issues related to quality, integration, privacy, and the reliability of the generated code.", "swebokAreas": "Software Engineering Process", "swebokSubAreas": "Software Engineering Process Fundamentals, Life Cycles", "doiOrLink": "https://arxiv.org/pdf/2310.13648", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Examining the Utilization of Artificial Intelligence Tools by Students in Software Engineering Projects": { "id": "ID35", "level": "B", "year": "2024", "source": "(CSEDU 2024)", "authors": "Amir Dirin e Teemu Laine", "educationalObjectives": "The article aims to answer the following questions: \"\"How does the use of AI tools affect the functionalities implemented in software engineering students' projects?\"\" \"\"How does the use of AI tools affect the tasks completed per sprint in software engineering students' projects?\"\" \"\"How do software engineering students perceive the use of AI tools in academic projects?\"\" The article intends to evaluate the impacts of AI tools, specifically ChatGPT and GitHub Copilot, on the outcomes, student motivation, and perceived potential in software engineering student projects. They also sought to evaluate the quality of the resulting software, the number of implemented functionalities, and the impact of AI on the implementation of tasks and user stories within the Agile methodology.", "aiEmployment": "The study‚Äôs methodology was developed within the context of a software engineering project course, aimed at fourth-semester students who had already completed full-stack development courses. The course lasted eight weeks and was structured into four bi-weekly sprints, following the Agile Scrum approach. In this environment, students formed nine teams of four members, totaling 36 participants. These teams were organized into three groups, based on different levels of permission for the use of Artificial Intelligence (AI) tools: Group 1 (AITU - AI Tools Unlimited): Had total freedom to use ChatGPT and GitHub Copilot. Group 2 (PAIU - Prior Approval for AI Use): Could use AI tools subject to prior instructor approval. Group 3 (NAIA - No Access to AI): Had no access to any type of AI tool, developing their projects based solely on their own knowledge. During the course, the teams carried out typical software engineering project activities within the Scrum methodology, including the free selection of technologies, mandatory adoption of agile practices, use of Trello for task management, and rotation of the Scrum Master role. The study observes and analyzes how AI is applied by students in the learning and execution of traditional software engineering skills and content, such as project management, requirements definition (or requirements elicitation), implementation, and testing. The study reveals how AI impacts (both positively and negatively, such as concerning code quality) these areas.", "llm": "ChatGPT and GitHub Copilot", "metrics": "Questionnaires, Quality of the Resulting Software, and Implemented Features", "metricsUsage": "Questionnaires were utilized as part of the mixed-methods approach to address the research questions. These questionnaires were used to collect students' perceptions regarding the use of AI tools. The study also compared how the use of AI tools affected the number of functionalities implemented in the software projects.", "resources": "The study noted the use of Trello for project management and progress tracking. The AI tools themselves, ChatGPT and GitHub Copilot, were resources introduced into the process for two of the three groups.", "benefitsAndChallenges": "Students and researchers perceived that LLMs can boost productivity and creativity in Software Engineering tasks, effectively functioning as assistants. However, the use of these tools introduces challenges related to the initial learning curve (or initial learning time), a tendency to prioritize task quantity over quality, which resulted in a lower quality of the final software. This outcome highlights the need for greater emphasis on quality assurance when utilizing LLMs.", "swebokAreas": "Software Engineering Process", "swebokSubAreas": "Life Cycles, Software Process Assessment and Improvement", "doiOrLink": "https://www.scitepress.org/Papers/2024/127294/127294.pdf", "interaction": "Hybrid", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Toward an AI Knowledge Assistant for Context-Aware Learning Experiences in Software Capstone Project Development": { "id": "ID36", "level": "A", "year": "2024", "source": " Transactions on Learning Technologies (TLT)", "authors": "Andr√©s Neyem, Luis A. Gonz√°lez, Marcelo Mendoza, Juan Pablo Sandoval Alcocer, Leonardo Centellas, and Carlos Paredes", "educationalObjectives": "The study analyzed the use of an AI Knowledge Assistant by students, comparing three query strategies: direct queries to GPT-3.5, queries enriched with lessons learned, and queries enriched with data from Stack Overflow. Students preferred direct queries or those enriched with course content, highlighting their effectiveness. This approach proved to be as useful as more complex methods, suggesting that Large Language Models (LLMs) could replace traditional lessons learned repositories.", "aiEmployment": "Artificial Intelligence was employed in the study through a Knowledge Assistant, an educational tool designed to offer context-aware learning experiences in software development courses. For general interaction with the assistant, students could formulate their own questions through a chat interface. Its primary role is described as an assistant, learning facilitator, and problem-solving support. The assistant functions as a real-time chatbot, offering personalized support to students based on multiple knowledge sources: course lessons learned, LLMs (GPT-4, GPT-3.5, LLaMa 2), and Stack Overflow. Recommendation strategies were used to compare responses with and without contextual enrichment. The quantitative evaluation included feedback rubrics, readability metrics, and textual similarity analyses. The qualitative assessment was based on open-ended student feedback. Participants interacted with the AI Knowledge Assistant integrated into their Kanban board, utilizing strategies with different LLMs and knowledge sources.", "llm": "The study integrated the GPT-4, GPT-3.5 Turbo, and LLaMa 2 models into the AI Knowledge Assistant.", "metrics": "Questionnaires, Error Rate, User Satisfaction.", "metricsUsage": "The evaluation was based on a user-centered framework for recommender systems. Students mentioned the need for improved error handling during API calls.", "resources": "The environment included documentation tools, client meetings, and evaluation rubrics.", "benefitsAndChallenges": "Highlights include an improved understanding of the recommendations, greater relevance, novelty, and serendipity of the responses, in addition to overall satisfaction with the use of the system. However, significant challenges were identified, such as slow response times, inaccuracies or a lack of clarity in some answers, the need for user interface and error handling improvements, as well as low textual quality in some models or when using information from Stack Overflow.", "swebokAreas": "Professional Practice", "swebokSubAreas": "Communication Skills", "doiOrLink": "https://ieeexplore.ieee.org/document/10518103?denied=", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Practical Application of AI and Large Language Models in Software Engineering Education": { "id": "ID37", "level": "B", "year": "2024", "source": "IJACSA", "authors": "Vasil Kozov, Galina Ivanova, Desislava Atanasova", "educationalObjectives": "The article addresses how to teach and apply AI in all phases of the software development lifecycle, developing practical skills, critical thinking, and the strategic use of modern tools, preparing students for the real-world challenges of the job market in the Industry 4.0 era. Students learn to use Large Language Models (LLMs) like ChatGPT to generate project ideas, refine functional requirements, and design databases and interfaces.", "aiEmployment": "In the Requirements Analysis, Artificial Intelligence, and Introduction to Programming courses, students learn to use AI to generate and refine ideas, create and populate databases, suggest interface designs, and automate tasks. Workshops featuring ChatGPT were conducted in three courses, and positive motivational effects were observed. Data collection methods included student surveys, interviews during project presentations, and comparative observations with previous cohorts.", "llm": "ChatGPT and Bard", "metrics": "Questionnaires, User Satisfaction", "metricsUsage": "Students were surveyed, and their feedback was analyzed. A positive impact was observed both during and after the workshops.", "resources": "Os recursos inclu√≠ram guias de prompt engineering, v√≠deos de backup, bibliotecas de programa√ß√£o e conjuntos de dados.", "benefitsAndChallenges": "The study highlights that LLMs motivate students and facilitate complex tasks, enhancing skills, especially for lower-performing students. However, they present shortcomings such as providing inaccurate responses, requiring prior knowledge, and being dependent on well-crafted prompts.", "swebokAreas": "Computing Foundations", "swebokSubAreas": "Data Structures and Algorithms,  Programming Fundamentals and Languages", "doiOrLink": "https://pdfs.semanticscholar.org/8525/101b70d5d5073364f3f4c7bd02329aad4c13.pdf", "interaction": "Hybrid", "analysis": "Mixed", "usedByStudents": "Yes" },
    "A Systematic Evaluation of Code-generating Chatbots for Use in Undergraduate Computer Science Education": { "id": "ID38", "level": "B", "year": "2024", "source": "Frontiers in Education Conference", "authors": "Adam Torek, Elijah Sorensen, Natalie Hahle, Casey Kennington", "educationalObjectives": "The article focuses on the assessment of code-generating chatbots for use in the context of Computer Science Education. The study suggests a pedagogical progression, stating that novice programmers should first master important programming concepts without the aid of code assistants to demonstrate their comprehension and to learn how to evaluate the assistants' output. Once intermediate skills, such as Object-Oriented Programming (OOP) and data structures are mastered, the introduction of coding assistants is appropriate to aid with specific tasks. Among the topics covered are: basic concepts, object-oriented programming, data structures, algorithm analysis, and specific coding problem-solving. AI was also utilized to support novice programmers in understanding Python language conventions. Research Questions (RQs) RQ1: What are the capabilities and limitations of code-based chatbots, and how do they affect student responses to questions and problems from authentic Computer Science assignments? RQ2: How do these capabilities and limitations impact human-chatbot conversations and the chatbots' capacity to answer questions? RQ3: Is it possible to integrate code-based chatbots in a way that assists both novice and experienced programmers in performing programming tasks more efficiently?", "aiEmployment": "The study was divided into two experiments. In Experiment 1, the capability of four open-source chatbots to solve authentic university course assignments was assessed through a simulated interaction between an experienced student and the chatbots, following a trial-and-error process. This simulation replicated how students might use chatbots to debug or resolve programming problems. In Experiment 2, novice and experienced programmers utilized the Llama chatbot, integrated into VSCode, as a coding assistant to solve problems in Python. It was observed that the AI was particularly useful for novices in understanding language conventions and implementing basic functions.", "llm": "CodeLlama, Phind-CodeLlama, OctoCoder and WizardCoder", "metrics": "Questionnaires, Code Accuracy, Error Rate, User Satisfaction", "metricsUsage": "Upon completion of the tasks, participants filled out questionnaires with questions regarding their experiences and the perceived difficulty of the tasks. Open-ended feedback and testimonials were also collected from participants, providing qualitative insights into their perceptions of and experiences with the AI tool. For tasks that included test cases, the percentage of tests that the generated code successfully passed was measured, indicating the solution's completeness with respect to the requirements.", "resources": "Authentic programming exercises from the Computer Science (CS) curriculum (Java and Python) were utilized, along with task descriptions, test cases, reference solutions, and a runtime analyzer. The VSCode environment was employed, with the Llama chatbot integrated for Experiment 2.", "benefitsAndChallenges": "Overall, while AI tools demonstrate significant potential to assist in programming, especially for novices on specific tasks, their limitations in accuracy, conversational behavior, and the potential for misuse pose significant challenges to their integration into the educational setting.", "swebokAreas": "Software Construction", "swebokSubAreas": "Practical Considerations, Software Construction Tools", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893165&casa_token=hW_Z71RTB0EAAAAA:shXRW9bz85-BjBamVQdv88WJYhZisd5eRD3oSgD5ccf57el2xyk4_g-l8cBcRZg16p-Q4gYOoPU&tag=1", "interaction": "Fixed", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Integrating Conversational Large Language Models into Student Learning: A Case Study of ChatGPT in Software Engineering Education": { "id": "ID39", "level": "A", "year": "2023", "source": " Frontiers in Education Conference (FIE)", "authors": "Yi Liu", "educationalObjectives": "Students were assigned to a project in which they were required to use ChatGPT as a tool to assist in various stages of the software development process, including problem clarification, modeling assistance, system design feedback, and implementation support. They were encouraged to use ChatGPT 3.5 to enhance their understanding of the problem, refine their models, and receive assistance with debugging. The paper explores the use of ChatGPT as an educational support tool in a \"\"Formal Methods for Software Engineering\"\" course. The learning objectives include: (1) applying model-based formal methods, (2) modeling solutions with Finite State Machines (FSM), and (3) implementing models using concurrent programming in Java.", "aiEmployment": "ChatGPT was integrated as a support tool at various stages: problem clarification, modeling, system design, and implementation/debugging. Students were instructed not to request code directly and to apply critical thinking when evaluating the AI's responses. The assessment included the analysis of the students' modeling, implementation, and feedback, in addition to the logs of their interactions with ChatGPT. Generative AI was employed in the study as a tool to support and assist the students' learning process in a Software Engineering course. The prompts were primarily created by the students. Data was collected from three sources: student performance on the project, the logs of interactions with ChatGPT, and a questionnaire with 10 questions (7 Likert scale and 4 open-ended). The quantitative analysis involved evaluating student grades and aggregating the Likert scale responses. The qualitative analysis examined the textual feedback from the questionnaire and the conversation logs to identify patterns, limitations, and the effectiveness of using ChatGPT.", "llm": "ChatGPT 3.5", "metrics": "Questionnaires, Likert Scale, User Satisfaction", "metricsUsage": "A 10-item questionnaire was used, which was delivered along with the other project requirements. The questionnaire included 7 Likert-scale questions to quantify students' perceptions of the usefulness of and satisfaction with ChatGPT during different phases of the project, such as problem clarification, modeling assistance, system design feedback, and support.", "resources": "The primary materials used in the study included the project description, guidelines for the use of ChatGPT 3.5, the AI tool itself, languages such as FSP and Java, a questionnaire with open-ended and Likert-scale questions, and the logs of the students' conversations with ChatGPT.", "benefitsAndChallenges": "Among the benefits, support for problem clarification, design feedback, assistance with Java implementation and debugging, and a positive impact on learning were highlighted. The challenges, in turn, included limitations in modeling, a dependency on prompt quality, the need for refining questions, the risk of over-reliance and incorrect answers, as well as feedback that some students deemed basic or redundant.", "swebokAreas": "Models and Methods", "swebokSubAreas": "Analysis of Models, Software Engineering Methods", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893046&casa_token=XtR-VXhF8oEAAAAA:lISA2zg8HCCk899xwR0A1QabLfK0_4w2TbBQg-FExPGTRFGfi7Kib6u3AW3KGvCW-h3jF2XKduM", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Incorporating AI in the Teaching of Requirements Tracing Within Software Engineering": { "id": "ID40", "level": "B", "year": "2024", "source": "Frontiers in Education Conference (FIE)", "authors": "Juan Ortiz Coude, William C. Daniel A. Machado, Omar Ochoa", "educationalObjectives": "The primary motivation of the study was to address the challenge of adequately teaching requirements traceability in a traditional classroom environment. The study sought to incorporate the use of AI (Large Language Models - LLMs) into the teaching of requirements traceability. The paper aims to investigate whether LLMs can be used as a tool to teach requirements traceability to students and track their progress. The study proposes integrating the use of LLMs into requirements engineering education to bridge the gap between what is taught in the classroom and what is used in the industry. The paper seeks to teach and assist with the skill of Requirements Traceability, including the process of tracing requirements back to their originating artifact and handling the traceability of modified requirements.", "aiEmployment": "A division of groups was established to allow for a performance comparison between the LLMs and the students. The first group consisted of 20 master's students in Software Engineering, who performed the requirements traceability manually. They analyzed a client interview transcript, a requirements specification document, and visual prototypes to create a traditional traceability matrix. The second and third groups consisted of the GPT-3.5 and GPT-4 versions, respectively. Both were provided with the same artifacts used by the students. To ensure a fair evaluation, the models were tested with multiple prompts, adjusting the instructions to optimize their performance on the traceability task. The prompts were fixed and iterated upon by the researchers throughout the experiment to improve performance. In the experiment, the AI acted as an executor of the traceability task. Based on the results, the paper suggests its potential role as a tool for testing the completeness of the traceability process and as a self-assessment tool for students.", "llm": " GPT-3.5 and GPT-4", "metrics": "Comparison of Traced Requirements", "metricsUsage": "Metrics (counts of traced requirements) were used to compare the performance of the LLMs (GPT-3.5 and GPT-4) with the performance of the master's student group on the traceability task. The quantitative results were used to validate the capability of LLMs in assisting with or performing the traceability and to assess the performance difference between the GPT versions and in comparison to human performance.", "resources": "Transcript of an interview between clients and the engineering team. Requirements Specification Document (SRS) containing 888 elicited requirements. Visual prototypes used to validate requirements with clients.", "benefitsAndChallenges": "LLMs demonstrated benefits in teaching requirements traceability, acting as support tools to test the completeness of students' work and to assist in their self-assessment. They also traced some requirements that the students did not identify. Furthermore, they indicate a potential to reduce the gap between education and industry practices, especially with GPT-4's performance compared to GPT-3.5, owing to its better contextual understanding and larger input capacity. However, they still exhibit limitations, such as inferior performance to students in tracing modified requirements, difficulty in interpreting images, and a dependency on prompt formulation. Therefore, they do not replace human requirements engineers.", "swebokAreas": "Software Requirements", "swebokSubAreas": "Practical Considerations, Requirements Specification, Requirements Validation", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10892858&casa_token=BB-02Lml2CIAAAAA:cxieEYV0I8SH3__goM_94FtfLyMEJ7IshPxApr6tlTpa7VgcWFxM1ntsOcsvhFHov3cPp209ORU", "interaction": "Fixed Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Use of Generative Artificial Intelligence in the Education of Software Verification and Validation": { "id": "ID41", "level": "A", "year": "2024", "source": "Frontiers in Education Conference (FIE)", "authors": "Ayca Tuzmen, afiliada a Ira A. Fulton Schools of Engineering, Arizona State University, Tempe, Arizona", "educationalObjectives": "O artigo relata que alunos de p√≥s-gradua√ß√£o usaram diretamente LLMs, como ChatGPT 3.5 e Bard, em um curso de Verifica√ß√£o e Valida√ß√£o de Software. Eles aplicaram essas ferramentas para gerar casos de teste de unidade, explorando suas capacidades e limita√ß√µes. O artigo busca ensinar conte√∫dos e habilidades em Engenharia de Software com foco em Verifica√ß√£o e Valida√ß√£o e no uso de IA Generativa. Entre os principais objetivos est√£o, aplicar testes de unidade com frameworks adequados, desenvolver senso cr√≠tico para avaliar e complementar os testes gerados pela IA, entender os princ√≠pios de garantia de qualidade de software e explorar as limita√ß√µes e potencialidades da GenAI.", "aiEmployment": "A IA generativa foi integrada ao curr√≠culo por meio de um exerc√≠cio pr√°tico redesenhado, no qual os alunos implementaram um algoritmo, escolheram ferramentas e usaram a GenAI para gerar e avaliar casos de teste. O estudo utilizou uma an√°lise descritiva com abordagem mista, baseada em 290 relat√≥rios de 180 alunos de p√≥s-gradua√ß√£o. A parte quantitativa incluiu dados como frequ√™ncias de linguagens e frameworks utilizados e a aus√™ncia de correla√ß√µes significativas entre vari√°veis t√©cnicas. A parte qualitativa envolveu a avalia√ß√£o dos testes gerados pela GenAI, an√°lise de lacunas e feedback dos estudantes sobre efic√°cia e limita√ß√µes da IA.", "llm": "ChatGPT 3.5, with some students also using Bard-AI", "metrics": "Analise dos relatorios ", "metricsUsage": "A an√°lise dos resultados foi realizada com base em quatro aspectos principais: o tipo de framework de unidade utilizado, a cobertura dos testes de unidade, a efic√°cia dos casos de teste e as avalia√ß√µes dos alunos sobre a GenAI na cria√ß√£o de testes de n√≠vel de unidade.", "resources": "Os alunos usaram ferramentas como ChatGPT 3.5 e Bard-AI para criar e avaliar casos de teste com frameworks como JUnit e PyTest. Eles tinham liberdade para escolher linguagem, IDE e criar seus pr√≥prios prompts, com suporte dispon√≠vel em hor√°rios de atendimento.", "benefitsAndChallenges": "O estudo identificou benef√≠cios como a gera√ß√£o r√°pida de testes de alto n√≠vel, apoio na resolu√ß√£o criativa de problemas e co-cria√ß√£o de solu√ß√µes v√°lidas com a GenAI, considerada √∫til e eficaz pelos alunos. No entanto, tamb√©m foram observadas limita√ß√µes, como a cobertura insuficiente de casos de canto, falta de criatividade e expertise de dom√≠nio da GenAI, varia√ß√£o nas respostas e risco de alucina√ß√µes. ", "swebokAreas": "Software Testing, Software Quality", "swebokSubAreas": "Test Levels, Software Quality Assurance Process", "doiOrLink": "https://ieeexplore.ieee.org/document/10893333", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "AI2SQL.io: Empowering SQL Learning and SocialChange Through IT-Enabled Smart Tutoring in Computer Science and Engineering": { "id": "ID42", "level": "A", "year": "2024", "source": "2024 International Conference on Artificial Intelligence and Quantum Computation-Based Sensor Application (ICAIQSA)", "authors": "Dr. Rashmi Dixit e Nikhil Gajjam", "educationalObjectives": "The educational objectives in Software Engineering addressed by the article focus on enhancing the teaching and learning of SQL, an essential skill within the discipline. The study aims to develop students' fundamental competencies in the language, improve their theoretical understanding and practical application, and facilitate the transition from foundational knowledge to advanced use in real-world scenarios. It also supports the achievement of curriculum objectives related to SQL proficiency. Furthermore, this approach promotes learning with real-time feedback, adapts to varying skill levels, and seeks to increase student engagement through more dynamic and interactive methods. The article aims to teach and improve both practical and theoretical proficiency in SQL. By utilizing the AI tool AI2SQL.io, the study centers on developing database skills, specifically in the formulation, comprehension, and optimization of SQL queries. The tool also assists in translating natural language into SQL, allowing for daily practice and the application of theoretical concepts in practical settings, such as professional environments.", "aiEmployment": "The article compares two SQL teaching strategies: a traditional one and an innovative approach using the AI tool AI2SQL.io. The traditional approach involved lectures, textbook exercises, and manual feedback. In contrast, the AI-driven approach implemented an Intelligent Tutoring System that offered personalized learning, daily query-based practice, instant feedback, and natural language to SQL translation. The tool also encouraged collaborative learning and created a safe environment for experimentation. This strategy supplemented, rather than replaced, traditional instruction, promoting a more dynamic learning experience adapted to individual student needs. The analysis conducted in the study was of a mixed-methods nature, combining quantitative and qualitative approaches. The methodology involved pre-tests and post-tests with randomized groups, comprising a total of 60 students divided equally into a control group (traditional teaching) and a treatment group (using the AI2SQL.io tool). Quantitative data were collected through exams, engagement surveys, and tool usage metrics, while qualitative data were gathered from interviews with students and instructors.", "llm": "AI2SQL.io Although AI2SQL.io has the capability to convert natural language input into SQL queries and generate context-specific queries, the study presents it as a specialized AI tutor for teaching SQL, rather than as a general-purpose generative AI tool.", "metrics": "Questionnaires, Time Comparison, Code Accuracy, User Satisfaction", "metricsUsage": "Questionnaires were administered to measure student engagement levels. The treatment group, which benefited from AI2SQL, showed a significant decrease in task completion time compared to the control group. Furthermore, the treatment group demonstrated a substantial increase in the accuracy of their queries following the intervention with AI2SQL", "resources": "The traditional teaching method utilized textbooks, lectures, homework assignments, and instructor feedback. In contrast, the AI-assisted group used the AI2SQL.io tool, which provided personalized SQL queries, instant feedback, natural language to SQL translation, step-by-step visualizations, and customized exercises", "benefitsAndChallenges": "The study highlights that the use of AI2SQL.io yielded several benefits, such as simplifying the SQL learning process, enhancing in-depth comprehension, and accelerating the learning curve. It also provided accessibility for students at different skill levels and offered a safe environment for experimentation. Among the challenges, the study noted the AI's difficulty in understanding context and nuances, its dependence on the quality of the training data, and the fact that it cannot completely replace the role of a human instructor.", "swebokAreas": "Computing Foundations", "swebokSubAreas": "Database Management, Artificial Intelligence and Machine Learning", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10882332&casa_token=bfjwWiQuTNkAAAAA:YqwplATvMnKgXu3A7-zcvrwMQdrZfozuLOdrm1iAgFFmsuIn2DxW3r9ZZYt5KSH9KrGkA7XAT9c", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Roadmap for Software Engineering Education using ChatGPT": { "id": "ID43", "level": "B", "year": "2023", "source": "International Conference on Artificial Intelligence Science and Applications in Industry and Society (CAISAIS)", "authors": "Aly Maher Abdelfattah, Hany H Ammar, Nabila Ahmed Ali, e Mohamed Abd Elaziz", "educationalObjectives": "The proposal highlights the active interaction of students with AI for gathering user stories, creating diagrams (use case, class, and sequence), and refining the results. The proposal focuses on requirements modeling, guiding students in defining and extracting user stories, and in generating use case, class, and sequence diagrams. The study also promotes the practical application of knowledge in real-world scenarios, encouraging critical thinking, problem-solving, and communication skills.", "aiEmployment": "The approach adopts an agile methodology, promoting an immersive environment where students interact directly with the AI to create and refine artifacts such as user stories, use case, class, and sequence diagrams. The process is collaborative, featuring feedback loops between the student and ChatGPT, allowing for the enhancement of the generated artifacts. In the study, Artificial Intelligence was employed as a virtual assistant to support students in the Requirements Engineering process. The analysis conducted in the study was qualitative in nature, based on collecting student feedback to gain insights into the effectiveness of the proposed methodology. Although it does not use formal qualitative analysis methods, such as thematic analysis, the assessment was descriptive and observational, highlighting perceived experiences and benefits.", "llm": "ChatGPT-3 and ChatGPT-4 ", "metrics": "User Satisfaction", "metricsUsage": "The effectiveness of the interactive learning experience was evaluated by collecting feedback from the students. The data gathered from this feedback provided insights into the utility of ChatGPT as a learning tool and its impact on the students' understanding of requirements engineering.", "resources": "The primary materials used in the study included the project description, guidelines for using ChatGPT 3.5, the AI tool itself, languages such as FSP and Java, a questionnaire with open-ended and Likert-scale questions, and the logs of the students' conversations with ChatGPT.", "benefitsAndChallenges": "Among the benefits, the study highlighted support in clarifying problems, feedback on design, assistance with Java implementation and debugging, and a positive impact on learning. The challenges included limitations in modeling, dependency on prompt quality, the need for refining questions, the risk of over-reliance, and incorrect answers, in addition to feedback that was considered basic or redundant by some students.", "swebokAreas": "Software Requirements", "swebokSubAreas": "User Stories, Requirements Elicitation, Requirements Analysis, Requirements Specification", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10270477&casa_token=5R49R6e2t2oAAAAA:zlznaKYR2yMZpO9dYpB3xGZWHuoN_zgfzn3YtQL3i3huvSLCePV8qnAqt0a_sstWHq8KpaSoWo8", "interaction": "Hybrid", "analysis": "Qualitative", "usedByStudents": "Yes" },
    "A Data Science Course Utilizing GenAI": { "id": "ID44", "level": "A", "year": "2024", "source": "Frontiers in Education Conference (FIE)", "authors": "Jonathan W. Browning, John Bustard, Neil Anderson, e Leo Galway", "educationalObjectives": "The article describes the application of ChatGPT in a Data Science course for master's students in software development. Students investigated an industrial sector, collected and analyzed a relevant dataset, and ultimately produced two detailed reports. The use of ChatGPT was mandatory and integrated into all phases of this project. The study aims to ensure that students achieve a solid understanding of essential disciplines, such as requirements engineering, design, and testing, which constitute the foundation for developing high-quality systems. Furthermore, emphasis is placed on developing higher-order skills, which include critical analysis, the evaluation of solutions, and a comprehensive understanding of the project in its entirety. The article seeks to teach a set of skills that combine the fundamentals of data science and software engineering with the critical use of generative AI. Among the course content, practical data analysis, project planning, and data-driven reasoning are highlighted. Additionally, the course promotes the critical assessment of AI-generated responses and their synthesis with human knowledge.", "aiEmployment": "The central strategy was to maintain the project-based learning structure while deeply integrating the mandatory and assessed use of Generative Artificial Intelligence. This integration was guided by two objectives: to facilitate data analysis (especially for students with limited Python experience) and to ensure that students utilized GenAI to assist in their projects. The study adopts a qualitative methodology. The analysis was based on three main fronts: (1) student feedback collected in online sessions and the virtual learning environment, reporting experiences, uses of ChatGPT, and difficulties in planning analyses; (2) instructor observations, including insights into student limitations, such as accepting generic AI responses, difficulty in decomposing tasks, and an over-reliance on examples; and (3) analysis of student projects, based on the final reports and the prompt engineering report, which allowed for the assessment of both the quality of the data analysis and the proficiency in critical interaction with the AI. Students used ChatGPT to gain initial insights into an industry sector before interviewing a human expert and to create data analysis plans. The main and assessed task required students to create, test, and refine their own prompts to complete the project. The AI acted as a productivity-enhancing tool, for instance, by finding datasets in minutes.", "llm": "ChatGPT", "metrics": "User Satisfaction, Instructor Observation Analysis", "metricsUsage": "The researchers collected and analyzed feedback that students provided continuously throughout the course. This information was gathered during the weekly online sessions designed for Q&A. The assessment included an analysis of the reflections from the instructor who led the course. This encompassed observations on how students interacted with the AI and the difficulties they faced.", "resources": "The main component was a Practical Example, featuring model reports, sample prompts for ChatGPT, and demonstrations of structured outputs. This was created to guide the students but ultimately led to over-reliance. A guide was also developed, detailing criteria and providing structured feedback. Additionally, the course included weekly two-hour online classes, a virtual learning environment as the central platform, and weekly online Q&A sessions, which served both as direct support and as a means of collecting student feedback.", "benefitsAndChallenges": "Among the benefits were improved analytical and decision-making capabilities, the facilitation of technical analysis even for Python beginners, increased productivity and efficiency, an elevation in the quality of the work, a pedagogical shift toward focusing on the ‚Äúwhy‚Äù of the analysis, and a reduced need for technical support. The challenges, on the other hand, included difficulties with prompt engineering (generic responses, lack of refinement, and poor task decomposition), a deficiency in critical thinking and initiative, problems in synthesizing human and AI-generated content, and an over-reliance on provided examples.", "swebokAreas": "Professional Practice", "swebokSubAreas": "Professionalism, Communication Skills", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10893452&casa_token=18GYYKemxaEAAAAA:9SzorM71cDb2ScpfzFcRFNEuqGvVYaS9S9skDuF3aRN4fQlMxjeThgPTWt4XDm53cDUANDTtbHg", "interaction": "Free Prompts", "analysis": "Qualitative", "usedByStudents": "Yes" },
    "Analysis of Student-LLM Interaction in a Software Engineering Project": { "id": "ID45", "level": "A", "year": "2024", "source": "International Workshop on Large Language Models for Code (LLM4Code)", "authors": "Agrawal Naman, Ridwan Shariffdeen, Guanlin Wang, Sanka Rasnayaka, Ganesh Neelakanta Iyer", "educationalObjectives": "The primary application was in the generation of code for the project's functionalities, such as the Source Parser, the Program Knowledge Base (PKB), and the Query Processing Subsystem (QPS). The study proposes that the use of AI assistants should evolve beyond being merely a tool for code generation. Students should be encouraged to use AI more strategically throughout the entire software development lifecycle. Students not only used the generated code but frequently modified, refactored, or even removed it. The main motivation of the study was to understand how software engineering students interact with and utilize Large Language Models (LLMs) during the development of a complex, long-term project. Education in this area should extend beyond using AI as a mere code generation tool. The objective is for students to learn how to use AI assistants more broadly and strategically across the entire software development lifecycle. The primary objective was to analyze how software engineering students naturally used AI tools (ChatGPT and CoPilot) in a complex project to, from these observations, extract insights and recommendations for educators.", "aiEmployment": "The course's primary strategy was Project-Based Learning, in which students, organized into teams, developed a Static Program Analyzer over 13 weeks, divided into three delivery milestones. Software Engineering content was taught in a practical manner, requiring the construction of complex components. The main applications of AI included code generation, the creation of test cases, and iterative refinement through dialogue with the tool. Furthermore, students were required to evaluate and label the generated code, indicating the necessary level of human intervention, which promoted critical analysis rather than passive acceptance of the AI's outputs. The study adopted a mixed-methods analysis (quantitative and qualitative) to investigate students' interactions with LLMs. For the quantitative analysis, software metrics were applied to 730 code snippets, showing that CoPilot produced longer and more complex code, while ChatGPT required fewer modifications. The analysis also covered tool usage per task, the levels of human intervention, and a sentiment analysis of the prompts, revealing a pattern of initial optimism, intermediate frustration, and final resolution. For the qualitative analysis, 62 conversations with ChatGPT (318 messages) were examined, highlighting the refinement process, the evolution of prompts from simple to strategic throughout the semester, and the analysis of the final code in the repositories. AI was employed as a software development assistance tool. It played multiple roles, but its primary function can be defined as that of a code co-author and a refinement tool. The prompts were created entirely by the students.", "llm": "ChatGPT and CoPilot", "metrics": "Human Intervention, Code Complexity", "metricsUsage": "The study evaluated student interaction with ChatGPT and CoPilot using metrics for code complexity, human intervention, sentiment in prompts, tool usage, final code similarity, and conversation dynamics, which allowed for the analysis of the quality, engagement, and iterative refinement of the AI-generated code.", "resources": "The environment included code repositories and a labeling system, which enabled the recording of the tool used, the level of human intervention, and conversation history, serving for both self-assessment and data collection. No traditional materials, such as slides or videos, were used.", "benefitsAndChallenges": "The use of ChatGPT resulted in more concise, simple, and easy-to-understand code, with lower complexity and cognitive effort, allowing for iterative refinement through its conversational interface and alignment with student preferences. The LLMs increased productivity, particularly in the initial phases, and promoted active learning and the development of prompt engineering skills, in addition to generating modular and reusable code. Among the challenges were the unnecessary complexity of some solutions, the need for human intervention, the risk of over-reliance, and temporary frustration during interaction. The initial AI-generated code was not always definitive and was frequently refactored or removed in the final project.", "swebokAreas": "Software Construction, Computing Foundations", "swebokSubAreas": "Practical Considerations, User and Developer Human Factors, Programming Fundamentals and Languages", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11028221&casa_token=no213ApmlwwAAAAA:1Jxt0oGE_O99EKKc-370fRJsjaa5UQgLXJzs7bu9-utlh7OEFJyBeRK9Fk3R2i_giFJoKV-h0es", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Concepts for Teaching Software Development in the Age of AI-Tools": { "id": "ID46", "level": "A", "year": "2024", "source": "Global Engineering Education Conference (EDUCON)", "authors": "Axel B√∂ttcher, Veronika Thurner, e Benedikt Z√∂nnchen", "educationalObjectives": "The most direct purpose was the students' attempt to use ChatGPT and GitHub Copilot to solve programming exercises, especially when they encountered difficulties. Teaching programming promotes valuable skills such as computational thinking, problem-solving, and logic, which are crucial for any field. Furthermore, students first need to develop a solid foundation in programming to be able to critically integrate and evaluate the code suggestions generated by AI. The study focuses on three main areas of development for Software Engineering students. The first involves the fundamentals of programming, reinforcing essential concepts, computational thinking, and logic skills, ensuring that students have a solid foundation to work with code. The second area focuses on specific skills related to generative AI, including prompt engineering, understanding the workflow with AI assistants, and the practical application of AI in programming tasks. Finally, the third area covers cognitive skills, such as critical analysis and evaluation of AI-generated code, integration and validation of solutions, and the responsible use of these tools, preparing students to interact strategically and ethically with intelligent systems.", "aiEmployment": "First, the methodology is cyclical and iterative, involving the design of interventions, in-class application, qualitative evaluation, and continuous refinement. Second, three strategies for GenAI integration are discussed: banning AI, using it from the beginning, or combining conventional teaching with subsequent AI interventions, with the latter considered the most effective for building a solid foundation prior to AI interaction. Finally, the study details specific teaching activities, including the observation of spontaneous AI use and prompted usage with feedback. AI was employed in multiple ways in the educational environment, both through student initiative and by instructor direction. The prompts were created by the students themselves. The study utilized a qualitative analysis based on direct classroom observations and experiences, without quantitative data. The execution followed a cyclical and iterative process, beginning with initial observations of students using ChatGPT independently, followed by the design of interventions, in-class application, and qualitative evaluation. Data collection involved student reports, GitHub Copilot demonstrations with plenary and small-group discussions, and the analysis of anecdotal evidence. Finally, the results of each cycle were used to refine teaching practices and formulate recommendations for integrating GenAI into programming education.", "llm": "ChatGPT and GitHub Copilot", "metrics": "Classroom Observations", "metricsUsage": "The basis for the analysis was the \"\"first observations on the use of GenAI tools in the classroom.\"\" The instructors observed how students, especially those with difficulties, interacted with the tools. During classes, the instructors used Copilot to solve problems, which led to a mix of plenary debates and small-group discussions to raise student awareness about quality issues in the generated code.", "resources": "The main tools used were ChatGPT, initially used spontaneously by students and later in a guided manner, and GitHub Copilot, which was employed both in live demonstrations by instructors and by the students themselves. The activities included standard programming exercises, complex integration tasks, code debugging, and Parsons Problems, all of which were conducted within IDEs and controlled assessment environments.", "benefitsAndChallenges": "The study identified that generative AI tools offer benefits such as increased productivity, serving as a personal tutor, and reducing students' inhibition to ask questions. However, they also present challenges, especially for beginners or struggling students, including shallow learning and confusion.", "swebokAreas": "Computing Foundations", "swebokSubAreas": "User and Developer  Human Factors, Artificial Intelligence  and Machine  Learning", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11016505&casa_token=f3tR3Ua3qfMAAAAA:9wnnCAtAkEluSuy7srvurav1fyCR3yy30Zt62Upmj_APPo3hwnHVXFSVf7HBcH6YqGoccpElmXE", "interaction": "Free Prompts", "analysis": "Qualitative", "usedByStudents": "Yes" },
    "Enhancing Student Learning in Scrum Projects with Generative AI Assistance": { "id": "ID47", "level": "B", "year": "2023", "source": "21st International Conference on Information Technology Based Higher Education and Training (ITHET)", "authors": "Meryem Boubakri e Khalid Nafil", "educationalObjectives": "The primary motivation of the study was to integrate and evaluate the use of Generative AI tools, specifically ChatGPT and Bard, in teaching the Scrum framework to enhance student learning and collaboration. The study sought to better understand the potential of AI-assisted teaching in project management education. The pedagogical scenario aimed to deepen students' practical understanding of Scrum through simulated activities, including creating a product vision, identifying and prioritizing epics, decomposing them into user stories, and developing agile artifacts (product backlog and sprint backlog). The results showed high effectiveness: 97% of students reported that AI contributed significantly to their understanding of the method. The main objective was to provide students with a practical and in-depth experience in applying the Scrum methodology. Using AI as an assistant, students were taught to create and manage the main Scrum artifacts and concepts in a scenario that simulated the real world.", "aiEmployment": "The teaching strategy was a practical and collaborative workshop in which fourth-year Software Engineering students, divided into groups, used either ChatGPT or Bard to support their learning of Scrum. The process involved an introduction to the tools, execution of tasks guided by prompts (such as creating a product vision, epics, user stories, a product backlog, and a sprint backlog), and validation of the outputs by the instructor. Afterward, students discussed the results within their groups and in a plenary session. The key pedagogical elements were active and hands-on learning, the acceleration of learning through AI, the simulation of real-world scenarios, and the encouragement of collaboration. AI was employed as an assistive tool within a structured pedagogical scenario, specifically a hands-on workshop for fourth-year Software Engineering students. The study clarifies that the students did not create their own prompts. The study utilized a mixed-methods analysis, combining qualitative and quantitative approaches to evaluate the integration of generative AI in Scrum education. The qualitative analysis consisted of comparing the responses from ChatGPT and Bard to the same prompts, assessing criteria such as detail, structure, and accuracy. It was found that ChatGPT produced more comprehensive answers, while Bard was more concise, with no instances of errors or \"\"hallucinations.\"\" The quantitative analysis was based on a questionnaire administered to 32 students, revealing that 97% considered AI effective for understanding Scrum, 94% rated the responses as useful and accurate, and 81% deemed the instructor's presence indispensable.", "llm": "ChatGPT and Bard", "metrics": "Questionnaires, User Satisfaction", "metricsUsage": "A survey was administered to the 32 participating students at the end of the workshop to evaluate the experience, the effectiveness of the AI, the role of the instructor, and to gather recommendations. Student satisfaction with the workshop's approach was explicitly measured, with 72% reporting they were 'Satisfied' and 25% 'Very Satisfied'. The overall experience was also rated positively by the vast majority of participants.", "resources": "The key resources were the generative AI tools (ChatGPT and Bard), which were used directly by students in groups to perform workshop tasks; the structured, hands-on workshop itself, which focused on collaboration; and a series of prompts developed by the instructor, which served as exercises guiding the practical application of Scrum.", "benefitsAndChallenges": "Benefits: AI accelerated the creation of agile artifacts, optimizing classroom time and allowing focus on analysis and discussion, which significantly improved student understanding (97% reported a positive impact). ChatGPT stood out for providing detailed, structured, and clear responses; the content was considered accurate and useful (94% of students agreed), validated without errors by the instructor, and the hands-on experience was seen as positive, engaging, and effective, with unanimous interest in future AI use. Challenges: The irreplaceable role of the instructor was highlighted (half of the students considered it the most valuable aspect, and 81% deemed their presence indispensable). Technical and capability limitations were reported (38% indicated the AI needed better contextual understanding and adaptability). Bard performed worse than ChatGPT, giving concise and less structured responses. Students suggested clearer prompts and more training to optimize tool usage.", "swebokAreas": "Models and Methods", "swebokSubAreas": "Scrum, Software Engineering Methods", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10837605&casa_token=-GOfjj4NHmIAAAAA:qqWdSQFc3RzU6_lQYmsgnBe9kq4OPlBX_JuKAwGszgoC8TxEthQyAgL-ip-M1qtYec8L78EkAfg", "interaction": "Fixed", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Enhancing User Story Generation in Agile Software Development Through Open AI and Prompt Engineering": { "id": "ID48", "level": "A", "year": "2024", "source": " Frontiers in Education Conference (FIE)", "authors": "Vijayalakshmi Ramasamy, Suganya Ramamoorthy, Gursimran Singh Walia, Eli Kulpinski e Aaron Antreassian", "educationalObjectives": "The primary motivation of the study was to investigate how AI technologies, specifically Large Language Models (LLMs) like ChatGPT, could be used to improve the process of generating user stories in Software Engineering educational settings. The objectives include: teaching requirements elicitation and documentation, particularly in the form of user stories; developing practical skills through team-based projects, such as capstone projects; enhancing problem-solving abilities; and aligning the academic curriculum with the actual demands of the job market. The main content addressed by the study is the generation and refinement of user stories. By using generative AI (ChatGPT), the study aims to teach students how to overcome the limitations of traditional brainstorming methods and create more complete, innovative, and effective user stories.", "aiEmployment": "First, students created user stories using conventional methods (as a baseline). Next, they refined these stories with the assistance of ChatGPT, exploring new requirements and functionalities. Afterward, they answered a survey to reflect on the experience and the impact of AI on their skills. Finally, industry experts evaluated the generated user stories to validate their quality. The AI, specifically OpenAI's ChatGPT utilizing models such as GPT-4 Turbo and GPT-3.5 Turbo, was employed as a tool to enhance and expand the generation of user stories in a software engineering educational environment. The study adopted a mixed-methods approach, combining quantitative and qualitative analyses to assess the impact of using ChatGPT on the generation of user stories. In the quantitative analysis, a post-test survey with a Likert scale was administered to 18 student teams. The results indicated perceived improvements in problem-solving, preparation for real-world development environments, quality of ChatGPT's responses, team collaboration, confidence in prompt creation, and the overall effectiveness of the strategies employed. The qualitative analysis was conducted by industry experts, who evaluated the user stories generated by students with and without AI support. They identified that ChatGPT expanded upon initial ideas, suggested additional functionalities, and introduced new requirements and stakeholders, thereby contributing to higher quality and innovation in the work.", "llm": "ChatGPT-4, gpt-3.5 and Whispe", "metrics": "Questionnaires, Industry Expert Evaluation", "metricsUsage": "The study utilized a post-test survey with students to collect feedback following the user story generation activity with AI. In parallel, software industry experts analyzed and compared the user stories generated by the traditional method with those enhanced by AI.", "resources": "The study did not make use of traditional teaching materials, such as slides, videos, or pre-defined handouts", "benefitsAndChallenges": "The study showed that the use of LLMs, such as ChatGPT, brought clear benefits but also relevant challenges. Among the positive aspects, highlights include improved problem-solving, the generation of more complete and innovative user stories, strengthened team collaboration, better preparation for the real-world work environment, and increased confidence in prompt creation. On the other hand, challenges emerged such as dependence on prompt quality, usability difficulties for some students, uncertainty about the effectiveness of the strategies, and the inherent risks of LLMs, including biases, inaccuracies, and privacy concerns.", "swebokAreas": "Software Requirements", "swebokSubAreas": "User Stories, Requirement Elicitation, Requirements Specification", "doiOrLink": "https://ieeexplore.ieee.org/document/10893343", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Exploration of Course Practice on the Integration of AI Language Model and Ideological and Political Education : The Course of \"Software Testing\" as an Example": { "id": "ID49", "level": "B", "year": "2024", "source": "International Conference on Information Science and Education (ICISE-IE)", "authors": "Chang Liu, Dongxia Zheng, Yan Jiang", "educationalObjectives": "The course addresses diverse areas and specific skills within the discipline of Software Testing, utilizing AI as a tool to modernize the learning process. The fundamental objective is to equip students with the practical skills necessary to work in the software testing field, aligning the curriculum with the current demands of the AI and big data industries. This article focuses on teaching Software Testing with the support of generative AI, highlighting five key fronts: Test Case Design: Students use AI to structure testing scenarios. Automated Script Generation: Aligned with industry demands for automation. Critical Thinking Development: Students analyze and validate AI-generated responses. Optimization and Refinement: Adapting artifacts to meet specific project needs. Quality Assessment of AI Outputs: Encouraging innovation in the use of the technology.", "aiEmployment": "The study adopted an integrated approach to teaching Software Testing, combining traditional methods with AI-driven practices under the concept of \"\"AI-Enhanced Enablement.\"\" The core strategy merged theory and practice through progressive projects, real-world industry cases, and the involvement of instructors from the sector. Generative AI was actively used, allowing students to ask questions, critically analyze the responses, and optimize test cases in interactive projects. Real-world scenario simulations and defect repair reinforced applied practice, complemented by a mix of methods and lectures, self-directed learning, discussions, reports, tutorials, and labs. Generative AI was integrated into the Software Testing teaching process, functioning as a tool for both learning and knowledge application. In the study, two types of prompts were used: open-ended prompts, created by the students themselves in an Intelligent Q&A System, and fixed prompts, generated by the AI in an Interactive Case Study, where students answered questions previously structured by the tool. The study's analysis was predominantly qualitative, focusing on the observation and assessment of student performance throughout the reformed course. The evaluation considered innovation reports, final presentations and defenses, observation of practical skills and competencies, as well as instructor reflection and feedback. No numerical data or statistical metrics were presented; the assessment was based on the qualitative interpretation of the students' progress, creativity, and application of AI.", "llm": "Not mentioned", "metrics": "Innovation Report Analysis", "metricsUsage": "The primary metric was the analysis of the students' \"\"innovation reports.\"\" In these documents, they described in detail how they applied AI technology, which demonstrated their \"\"deep understanding and innovative thinking\"\" in the field.", "resources": "The teaching process focused on interactive and practical materials rather than traditional resources like slides or videos. Generative AI models were utilized, including a Question and Answering (Q&A) system with open-ended prompts and an Interactive Case Study with structured prompts. The curriculum also explored real-world case studies and industry projects, online tutorials, and laboratory environments for applied practice. Learning was reinforced through structured exercises and reports, enabling students to apply AI, document their solutions, and develop both practical and analytical skills.", "benefitsAndChallenges": "The study indicates that using LLMs in Software Testing education yielded significant benefits, including the enhancement of technical skills, increased test efficiency and coverage, mastery of practical skills, and the development of critical thinking and innovation capabilities. Furthermore, it fostered student engagement and professional competencies such as teamwork and self-learning. The main challenge identified was pedagogical: teaching students how to interact critically with the AI by evaluating the rationality and completeness of its responses, and developing the ability to create effective prompts. Thus, the focus is not on the tool's limitations but on the need to cultivate reflective users who can leverage the technology consciously and effectively.", "swebokAreas": "Software Testing", "swebokSubAreas": "Testing Techniques", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11025195&casa_token=NFFzAyilcLcAAAAA:zeL7xWmkMzM962wSa6X2uKZh7R-8l69ykx9uWIOlvxNqV3EeccTDbxaftJ9OFoAe6ydVORxE1aw", "interaction": "Hybrid", "analysis": "Qualitative", "usedByStudents": "Yes" },
    "Prompting Large Language Models to Power Educational Chatbots": { "id": "ID50", "level": "B", "year": "2023", "source": " Proceedings of the 22nd International Conference on Web-Based Learning (ICWL 2023)", "authors": "Juan Carlos Farah, Sandy Ingram, Basile Spaenlehauer, Fanny Kim-Lan Lasne e Denis Gillet", "educationalObjectives": "Students interacted with the chatbot to discuss issues in code snippets, understand the corrections, and deepen their knowledge of the good programming practices taught in the course. The primary motivation of the study was to develop and test an architecture to facilitate the integration of educational chatbots powered by Large Language Models (LLMs) into digital education platforms. The main objective of the study was to teach good software engineering practices, with a focus on JavaScript. The activity covered code style standards, the concept of linting, the ESLint tool, and the use of style guides, using the Airbnb JavaScript Style Guide as a practical example. The core competency the study sought to develop in students was the ability to analyze and debug code.", "aiEmployment": "The study utilized the \"\"Fixer-Upper\"\" pedagogical pattern, in which students analyzed code snippets that were functionally correct but contained intentional style flaws in order to learn good programming practices. The activity took place in a 45-minute online lesson structured into 10 phases, which included pre- and post-tests, concept presentations (linting, ESLint, style guides), code review exercises, reflection, and feedback. In the experimental group, generative AI (the Graasp Bot) was integrated, offering hints in three stages (explaining the problem, suggesting a correction, and posing a question to engage the student), functioning as interactive support in the learning process. The AI was employed in the form of a custom educational chatbot. The prompts were entirely fixed and created by the researchers/educators. Students had no role in creating or modifying the prompts. The chatbot played a supportive role in the lesson, offering explanations and engaging students in dialogues about the learning topics. The study adopted a mixed-methods analysis, combining quantitative and qualitative data. For the quantitative part, t-tests were used to compare the groups in terms of learning gains (pre- and post-test), engagement (time spent on phases), usability (UEQ), and sentiment analysis (VADER). For the qualitative part, open-ended responses were coded, revealing positive perceptions but also criticisms that the chatbot was repetitive and insistent. The analysis of the 150 conversations showed that they were relevant to the topic and most were short.", "llm": "Graasp Bot, this chatbot was specifically developed for the study and powered by OpenAI's GPT-3 language model.", "metrics": "Questionnaires, Time Comparison, User Satisfaction", "metricsUsage": "The User Experience Questionnaire (UEQ), a standard instrument for measuring the usability of the lesson across six dimensions: Attractiveness, Perspicuity (clarity), Efficiency, Dependability, Stimulation, and Novelty. Student engagement was measured by calculating the total time they spent on each of the 10 phases of the lesson. The researchers then compared the time spent between the group with the chatbot and the group without the chatbot. This was complemented by a qualitative analysis of student feedback from open-ended questions, where they described what they liked and what could be improved in the lesson and the chatbot.", "resources": "The study used the Graasp platform with the integrated Code Capsule application and Graasp Bot (GPT-3). The materials included flawed code snippets (Fixer-Upper), explanations, and hints from the chatbot. The assessment was conducted using pre/post-tests, self-reflection, and the UEQ.", "benefitsAndChallenges": "Among the benefits, it was highlighted that students found the chatbot useful, interactive, and innovative. They also noted that it helped them focus on the activity, provided quick answers, and stimulated deeper reflection when the questions were well-formulated. On the other hand, the main challenges were its repetitiveness and the lack of naturalness in the interactions, which some perceived as irritating, leading them to question its actual necessity. The study also pointed out the sensitivity of prompt engineering and broader challenges such as bias, privacy, and the tool's lack of adaptability.", "swebokAreas": "Computing Foundations", "swebokSubAreas": "Data Structures and Algorithms, Programming Fundamentals and Languages", "doiOrLink": "https://juancarlosfarah.com/assets/pdfs/farah2023prompting_am.pdf", "interaction": "Fixed", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Innovative Approach to Agile Education: Generative AI-Supported Planning Poker Simulation": { "id": "ID51", "level": "B", "year": "2024", "source": "International Conference on Information Technology Based Higher Education and Training (ITHET)", "authors": "Khalid Nafil e Youssef Lefdaoui", "educationalObjectives": "The primary problem that the study aimed to address is the challenge of teaching Planning Poker in a university setting. The effectiveness of this ceremony relies on expert judgment, where experienced team members estimate task effort. However, in an academic context, students are not yet experienced experts, which makes the effective execution of the ceremony a significant challenge. The educational objectives in Software Engineering, as addressed in the study, are quite specific and focused on teaching agile methodologies. The main content taught is the Planning Poker ceremony from the Scrum framework.", "aiEmployment": "In the activity, students acted as facilitators, while AI agents assumed the roles of Scrum Master and developers, complete with realistic competencies and cognitive biases, to conduct user story estimation. The process involved theoretical preparation, simulation setup, iterative execution of estimates by the AI agents, and analysis of interaction logs. In the experience described in the article, AI was employed as an active part of the Planning Poker simulation process in a Scrum course. The prompts were fixed and created by the instructors; the AI played the roles of Scrum team members (tutor, co-author, expert). The study conducted a quantitative analysis of the interactions between students and AI agents (Scrum Master and developers) during the Planning Poker simulation. Chat logs were exported to CSV format, recording the sender, receiver, interaction type, and assigned estimates. The analysis followed five axes: agent interactions, student participation, collaboration patterns, communication, and decision-making. Techniques such as interaction counting, frequency analysis, and the construction of communication graphs were used to visualize message flows.", "llm": "Character.ai", "metrics": "Quantitative Analysis of Chat Messages", "metricsUsage": "Data were collected from a chat log and recorded in a CSV file for analysis.", "resources": "In the study, the teaching materials and resources were directed towards the practical simulation of Planning Poker. Students used the Character.ai platform to create and interact with AI agents (a Scrum Master and developers), which were configured using custom prompts that defined their competencies, biases, communication style, and role in Scrum. The activity included a user story with detailed acceptance criteria and an explanation of the Planning Poker rules (objective, mechanics, the Fibonacci sequence, and reaching consensus). The virtual chat environment was used to conduct the real-time simulation and to record data for analysis.", "benefitsAndChallenges": "The study indicates that LLMs and generative AI offer significant benefits in Software Engineering education, including the creation of innovative pedagogical scenarios, the simulation of realistic professional environments, increased student engagement and collaboration, and support for inexperienced practitioners, in addition to the automation of specific tasks. However, the challenges include ensuring information accuracy, the need for human oversight, the inability to completely replace human judgment, and the presence of cognitive biases in the AI agents.", "swebokAreas": "Software Testing", "swebokSubAreas": "Planning Poker, Testing Techniques", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10837671&casa_token=PYgR7GiWUL4AAAAA:O-XNTEPxV6YPhUwRz9pdNmm5Quf4aIW4z8Pktcl55O2j3vZalbkkSLsTNpn_ur-dFW2A2rRQ71k", "interaction": "Fixed", "analysis": "Quantitative", "usedByStudents": "Yes" },
    "Leveraging Lecture Content for Improved Feedback: Explorations with GPT-4 and Retrieval Augmented Generation": { "id": "ID52", "level": "C", "year": "2024", "source": "International Conference on Software Engineering Education and Training (CSEE&T)", "authors": "Sven Jacobs e Steffen Jaschke.", "educationalObjectives": "The primary objective is to assist students in learning and applying programming concepts that have been previously taught in an introductory course. The study indicates that the Tutor Kai tool aims to support introductory programming education in Software Engineering by fostering independent problem-solving, skill development, and bridging the gap between theory and practice, in addition to providing individualized support at scale. The research shows that generative AI was used in an introductory programming course to reinforce concepts, develop autonomous problem-solving, and connect theory with practice. The study's focus, however, was on student perceptions of the tool, without formally assessing the effective acquisition of knowledge or skills.", "aiEmployment": "The study employed a guided autonomous learning strategy, in which the AI tutor provides contextualized feedback without revealing the solution, using Retrieval-Augmented Generation (RAG) to integrate excerpts from video lectures. Students could choose between quick or detailed feedback, promoting independent problem-solving and connecting theory to programming practice. The AI was utilized as the core of a customized feedback system named \"\"Tutor Kai,\"\" designed for an introductory programming course. The prompts were entirely fixed and created by the researchers as an integral part of the Tutor Kai system's design. Students did not create, edit, or interact directly with the prompts. The study used a mixed-methods analysis to evaluate Tutor Kai. The quantitative analysis showed that students preferred quick feedback (478 out of 574 interactions) and rated the usability at 74.8. The qualitative analysis revealed that they adopted a situational strategy, using quick feedback for easy tasks and detailed feedback (with links to videos) for difficult tasks, thereby connecting theory with practice.", "llm": "GPT-4", "metrics": "Questionnaires, Time Comparison, User Satisfaction", "metricsUsage": "The primary assessment was conducted through a questionnaire administered to 15 students following a workshop. The questionnaire contained questions about the quality of the feedback, system usability, and general concerns. Satisfaction was measured through the responses to this questionnaire. The results showed that students were, in general, satisfied with the simplicity, length, and clarity of the feedback. The study also explicitly measured and compared the generation time for the two types of feedback.", "resources": "The study's primary materials included the Tutor Kai exercise environment, video lecture recordings, and programming assignments. To generate feedback, the AI analyzed the student's code, the task description, and the compiler output. Supporting technical resources included lecture transcripts and a vector database for implementing the Retrieval-Augmented Generation (RAG) technique.", "benefitsAndChallenges": "The study identified benefits of using LLMs in programming education, such as better concept identification, connecting theory and practice, promoting autonomy, providing understandable and reliable feedback, and reducing incorrect information via RAG. The main challenges included the slowness of the detailed feedback, students' over-reliance on the AI, and the technical complexity of implementing the system.", "swebokAreas": "Computing Foundations", "swebokSubAreas": "Programming Fundamentals and Languages", "doiOrLink": "https://ieeexplore.ieee.org/document/10837671", "interaction": "Fixed", "analysis": "Mixed", "usedByStudents": "No" },
    "Pair Programming in Programming Courses in the Era of Generative AI: Students' Perspective": { "id": "ID53", "level": "A", "year": "2024", "source": "Asia-Pacific Software Engineering Conference (APSEC) ", "authors": "Mario Simaremare, Chandro Pardede, Irma Tampubolon, Daniel Simangunsong, Putri Manurung.", "educationalObjectives": "The AI acted as an on-demand support tool, a programming assistant, and a private tutor, whose instructional focus was dictated by the immediate needs and difficulties of each student during the proposed activities. The objectives range from learning fundamental and advanced programming techniques to applying theory in practice, generating and debugging code, and refactoring to improve quality. In summary, GenAI helps to consolidate concepts, support coding, and strengthen essential competencies in the field. The study integrates AI as a support tool to assist students in learning the existing content of programming courses and, in this process, seeks to observe and develop a set of practical and metacognitive skills essential for Software Engineering.", "aiEmployment": "Student-centered learning, flipped classroom, and problem-based learning methodologies were applied. The core of the study was pair programming in two phases: in the first, student+AI (with the AI acting as a partner); in the second, student+student with AI support (the model preferred by the students). GenAI was used to clarify concepts, generate examples and code, and debug and refactor solutions. The AI was employed in an integrated and structured manner in the programming courses through an experimental pair programming strategy, divided into two phases over a semester. The AI played multiple roles, depending on the student's needs and the task at hand. The prompts were created entirely by the students. The study conducted a qualitative analysis based on semi-structured interviews with 12 CS1 and CS2 students, selected to represent different performance levels. The data was transcribed and analyzed using Thematic Analysis, resulting in three main categories: GenAI use cases, challenges in using GenAI, and perceptions of pair programming with GenAI.", "llm": "ChatGPT and Copilot", "metrics": "User Satisfaction, Thematic Analysis", "metricsUsage": "The results show that students felt a lack of engagement when pairing with the AI and preferred traditional pairing, but they considered that the AI worked better as an additional reinforcement for the student pair. The qualitative data collected from the interviews were analyzed using this technique to identify patterns and recurring themes, such as use cases, challenges, and perceptions.", "resources": "The study used flipped-learning materials (videos, slides, and notes), Generative AI tools (ChatGPT, GitHub Copilot, Gemini, and Perplexity AI), and practical exercises in problem-based laboratory sessions, complemented by an introductory workshop to train students in the ethical and effective use of AI tools.", "benefitsAndChallenges": "The study identified that students perceived benefits of Generative AI in five areas: clarifying difficult concepts, generating practical examples, developing code, debugging, and refactoring, thereby strengthening their learning and programming practice. On the other hand, they recognized six challenges: excessive dependency, solution complexity, response inaccuracy, language barrier, narrow learning horizons, and difficulty in creating contextual prompts.", "swebokAreas": "Software Construction, Computing Foundations", "swebokSubAreas": "Programming Fundamentals and Languages, Practical Considerations", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10967301&casa_token=7bOgz4JwaIkAAAAA:zfTNe-z4nk7i-EXMnnbgle1u9oYyeGtx1LgaALimIfwyK3JkEmEFylxqgf68xgSC2PR3amTwsBc", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Stimulating Critical Thinking in a Web Programming Module with Generative AI Tools": { "id": "ID54", "level": "A", "year": "2024", "source": "EEE Global Engineering Education Conference, EDUCON.", "authors": "Usman Naeem, Arne Styve, Outi T. Virkki.", "educationalObjectives": "The study focused on teaching the fundamental languages of web programming, which are HTML5, CSS, JavaScript, and PHP. The study defines four objectives: developing solid technical knowledge, fostering critical thinking, using AI responsibly, and preparing students for the job market. The content taught was web programming, and the primary skill developed through generative AI was critical thinking, preparing students to be competent and responsible users of the technology.", "aiEmployment": "The study applied an active and project-based learning approach, integrating Generative AI to stimulate critical thinking. Students developed a portfolio website in phases, using GitHub Copilot to generate code and ChatGPT for critical analysis, refactoring, and improving the code based on these evaluations. The AI was used as a support tool, not to provide ready-made answers, and self-assessment questionnaires measured changes in critical thinking. The AI was used in two complementary roles: GitHub Copilot as an initial code generator and ChatGPT as a peer reviewer, creating a cycle of generation, analysis, critique, and refactoring with the student always at the center of the decisions. The approach evolved from guided prompts to student-created prompts, allowing for the development of prompt engineering and critical analysis skills regarding the AI-generated code. The study primarily conducted a quantitative analysis through pre- and post-intervention questionnaires to measure changes in students' critical thinking. This was complemented by a qualitative assessment of project reports, which focused on prompt formulation, explanation of algorithms, and the refactoring of AI-generated code.", "llm": "GitHub CoPilot and ChatGPT", "metrics": "Questionnaires, Likert Scale, Project and Report Evaluation", "metricsUsage": "The study used a self-assessment questionnaire on critical thinking to measure students‚Äô opinions and practices. This questionnaire was administered before and after the AI-based pedagogical intervention. It employed a five-point Likert scale, with response options indicating the frequency of critical thinking practices, ranging from \"\"never\"\" to \"\"always.\"\" Finally, the instructors evaluated the quality and complexity of the prompts created by the students, as well as the depth of their explanation of the algorithm‚Äôs functioning in the submission report.", "resources": "The instruction utilized development environments (VS Code) and AI tools (GitHub Copilot and ChatGPT), weekly exercises, and a 12-week portfolio project. This was complemented by reports, critical thinking questionnaires, diagrams, and screenshots, all organized within a hands-on, problem-based, and project-based approach that included theoretical lectures and lab sessions.", "benefitsAndChallenges": "The study shows that LLMs, such as ChatGPT and GitHub Copilot, offer benefits when integrated in a structured manner: they stimulate critical thinking, provide a starting point for complex tasks, promote deep learning, and develop industry-relevant skills. On the other hand, they present challenges, including the risk of superficial learning and dependency, over-reliance on AI, the need for new pedagogical approaches, time constraints for critical reflection, and the insufficiency of a single intervention to form lasting habits, requiring continuous integration throughout the course.", "swebokAreas": "Software Construction", "swebokSubAreas": "Practical Considerations", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11016298&casa_token=ER0OXvYxHKgAAAAA:BzXh2QxzQ-uxwL4dy1Qooy0jtZtcKpJhc4Z970OogcHDeV0Nyl5GkbJtxTTIAYG2DiJOV3F01Cc", "interaction": "Free Prompts", "analysis": "Quantitative", "usedByStudents": "Yes" },
    "Supporting Brainstorming Activities with Bots in Software Engineering Education": { "id": "ID55", "level": "B", "year": "2025", "source": "International Workshop on Bots in Software Engineering (BotSE)", "authors": "Juan Carlos Farah, J√©r√©my La Scala, Sandy Ingram, Denis Gillet.", "educationalObjectives": "The primary motivation was to study the results of a tool integrated into a brainstorming environment through two \"\"bots\"\" (autonomous agents) based on OpenAI's GPT-4o model. These bots participated in the brainstorming sessions as team members, automatically proposing ideas. The educational objectives of the frontend development course include learning the software lifecycle, acquiring knowledge in UX, practicing component-based programming, developing creativity and critical thinking, and exercising ideation on high-level aspects of the project. The main focus is on measuring the impact of collaborating with AI bots on the brainstorming process within an existing Software Engineering course.", "aiEmployment": "The teaching strategy was a semester-long group project based on project-based learning, in which structured brainstorming served as the ideation method to define high-level aspects of a frontend web application. Generative AI was used solely as an experimental variable to observe its impact on the process. Generative AI, specifically OpenAI's GPT-4o model, was employed indirectly through \"\"bots\"\" integrated into an online collaborative brainstorming application. The prompts were fixed and predefined by the researchers, not by the students. The study's analysis was quantitative, based on idea count data and Likert scale questionnaires. The data was processed using descriptive statistics (means, medians, standard deviations, minimums, and maximums) and non-parametric tests, including the Wilcoxon signed-rank test to compare conditions with and without bots, rank biserial correlation to measure effect size, and the Kruskal-Wallis test to assess differences across teams and tasks.", "llm": "ChatGPT 4o", "metrics": "Questionnaires, Likert Scale", "metricsUsage": "Questionnaires were administered after each of the four brainstorming tasks. Specifically, a seven-point Likert scale (1 = Strongly Disagree to 7 = Strongly Agree) was used to measure students' perceptions of accountability, authorship, originality, and utility.", "resources": "The resources used included an introductory presentation, a one-hour brainstorming exercise integrated into a semester-long group project, and the use of an online collaborative brainstorming application within the Graasp platform to support the development of a responsive web application focused on the software lifecycle, UX, and component-based programming.", "benefitsAndChallenges": "In the study, the perception of using LLMs in collaborative brainstorming was predominantly negative. The findings showed that the presence of AI reduced idea generation, the students' sense of authorship, and their accountability, without bringing clear benefits to the utility or originality of the ideas.", "swebokAreas": "Professional Practice", "swebokSubAreas": "UX, Group Dynamics and Psychology", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11050849&casa_token=VgCvcPVz21YAAAAA:xcKYwgsr_EIdcHE30rCt-7I_adg9aeZHmVC9_AUVLXb7vSM0Np-0rvF3CpwTKxcjpZtwhT94OAs", "interaction": "Fixed", "analysis": "Quantitative", "usedByStudents": "Yes" },
    "Using an LLM to Help with Code Understanding": { "id": "ID56", "level": "B", "year": "2024", "source": " International Conference on Software Engineering (ICSE ‚Äô24)", "authors": "Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers", "educationalObjectives": "Participants, including a group of 16 students, were assigned software engineering tasks that involved understanding and modifying an unfamiliar Python codebase related to data visualization and 3D rendering. The study focuses on the development of code comprehension as a central educational objective, which includes learning to use unfamiliar APIs, assimilating domain-specific knowledge, enhancing information-seeking skills, and modifying or extending existing code. The paper aimed to investigate how generative AI can support the comprehension of unfamiliar code, assisting developers in learning and using new APIs, assimilating domain concepts, seeking information more efficiently, and modifying or extending existing code.", "aiEmployment": "The study adopted an investigative, AI-mediated learning approach, placing participants in realistic unfamiliar code modification tasks. They were required to seek information independently using either the web (control group) or the GILT tool in VS Code (treatment group). The tool offered on-demand explanations via predefined buttons (e.g., overview, API, concept, examples) and supported open-ended, natural language queries with the selected code automatically included as context. The primary goal of employing AI was not to generate new code, but to assist participants in comprehending an unfamiliar source code. The study combined fixed and open-ended prompts, enabling participants to obtain quick explanations or ask free-form questions, with the selected code always being automatically added as context. The study employed a mixed-methods analysis: quantitative, using regressions, statistical tests, and performance metrics (progress, time on task, comprehension, TAM, TLX, and usage logs); and qualitative, through the analysis of interactions, open-ended responses, emerging themes, and prompt n-grams, to assess the impact of, interaction with, and perception of the GILT tool.", "llm": "ChatGPT-3.5", "metrics": "Questionnaires, Time Comparison, User Satisfaction, Task Completion Rate", "metricsUsage": "The study utilized multiple questionnaires: a pre-study survey to collect demographic and experience data, a post-task questionnaire to measure code comprehension, and a post-study survey to assess perceptions of the tools. The time participants took to complete each subtask was measured and statistically analyzed using regression models to compare the GILT tool condition against the web search condition. The TLX questionnaire measured frustration and the perceived success in performance. Furthermore, the qualitative analysis of open-ended responses captured user feedback on their overall experience. Finally, the researchers evaluated the correctness of each participant's solution to the subtasks and measured how many were implemented correctly.", "resources": "The study provided starter code, tutorials, demonstrations, and pre- and post-task questionnaires, and permitted the use of search engines as a supplementary resource. The experimental design included a control group and a GILT (treatment) group, and the collected data has been made available in an online replication package.", "benefitsAndChallenges": "Participants perceived the GILT tool as useful, efficient, and well-integrated with the code context. This enhanced their sense of success, and they viewed it as complementary to other AI tools. However, they also faced challenges, including difficulty in crafting effective prompts, the risk of outsourcing their own comprehension, inaccurate responses, and varied interface preferences.", "swebokAreas": " Engineering Foundations", "swebokSubAreas": "Empirical Methods and Experimental Techniques", "doiOrLink": "https://dl.acm.org/doi/pdf/10.1145/3597503.3639187", "interaction": "Hybrid", "analysis": "Mixed", "usedByStudents": "Yes" },
    "Leveraging Large Language Models for Enhanced VR Development: Insights and Challenges": { "id": "ID57", "level": "A", "year": "2024", "source": " Gaming, Entertainment, and Media Conference (GEM)", "authors": "Amany Alkhayat, Brett Ciranni, Rupa Samyukta Tumuluri, Rohit Srinivas Tulasi.", "educationalObjectives": "The motivation was driven by the difficulty novices face in VR development and the lack of documented research on how new AI tools could address this problem. The study's research question directly reflects this motivation, questioning the effectiveness of LLMs in guiding interaction design, generating code, and solving common challenges in VR development with Unity. To develop C# programming skills, enhance problem-solving and debugging, learn to implement VR features and interactions, and understand code quality metrics. Generative AI assisted novices in VR development with Unity by supporting them in C# programming, error debugging, VR interaction design and implementation, the use of the Unity tool, and the application of code quality best practices.", "aiEmployment": "The identified learning strategy was self-directed and AI-assisted, where novice developers reactively turned to ChatGPT to solve specific problems. They requested C# code generation accompanied by detailed explanations and instructions for integration into Unity. The process was iterative, involving successive refinements of prompts and alternative attempts, which enhanced their understanding of errors and strengthened their problem-solving skills. Additionally, external verification was conducted using sources such as tutorials and forums, along with collaboration with a senior researcher, ensuring greater accuracy and complementing the solutions provided by the AI. The usage did not follow a fixed script; instead, the AI was triggered whenever a challenge or need arose during project development. The primary uses of the AI included C# code generation and correction, error resolution and debugging in Unity, implementation of VR functionalities with step-by-step instructions, and support for brainstorming and feature ideation. Quantitative Analysis: This evaluated the quality of the C# code generated by ChatGPT using Visual Studio 2019 metrics (Maintainability Index, Cyclomatic Complexity, Depth of Inheritance, Class Coupling, and Lines of Code), comparing script versions with and without corrections to measure improvements. Qualitative Analysis: This explored the researchers' experiences and perceptions through the documentation of prompts and interactions, followed by thematic analysis (identification of patterns, successes, and difficulties) and reflective analysis, which contrasted the perspectives of novices with that of an experienced Unity instructor.", "llm": "ChatGPT ( v3.5  v4) and Gemini", "metrics": "Generated Code Accuracy", "metricsUsage": "The main accuracy metric was code functionality. The study repeatedly compared code that \"\"worked\"\" versus code that \"\"did not work\"\" in the Unity environment to assess the effectiveness of the AI responses.", "resources": "Development tools (Unity and Visual Studio 2019), external verification resources (YouTube, Reddit, Stack Overflow, Unity forums), and collaboration with experienced instructors.", "benefitsAndChallenges": "The study identified several benefits, including accelerated development speed, step-by-step implementation support, learning opportunities from error resolution, ideation support, and the ability to interpret instructions despite inaccuracies. Conversely, the challenges included outdated knowledge and incorrect information, the AI's tendency to agree with the user ('sycophancy'), a lack of visual context and the assumption of prior knowledge, dependency on prompt quality, and inconsistent performance across different LLMs.", "swebokAreas": "Software Maintenance", "swebokSubAreas": "VR Development, Software Maintenance Techniques", "doiOrLink": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10585495&casa_token=kVLyhue33mAAAAAA:12f8jWnYxD7gxT-BpZtIq72fMGz_lS8xeXEp1B77XP4UCGffR7LL_w7Haa0gYGnAi__shrsATWs", "interaction": "Free Prompts", "analysis": "Mixed", "usedByStudents": "Yes" },
};

// --- DADOS INICIAIS PARA A APLICA√á√ÉO (FORMATO Article[]) ---
// ‚úÖ L√ìGICA CORRIGIDA PARA SER √Ä PROVA DE FALHAS (ANTI-TELA PRETA)
const INITIAL_ARTICLES: Article[] = Object.entries(FULL_ARTICLE_DETAILS).map(([title, details]) => {
    const swebokAreas = (details.swebokAreas || "").split(',').map(s => s.trim()).filter(Boolean);
    const swebokSubAreas = (details.swebokSubAreas || "").split(',').map(s => s.trim()).filter(Boolean);
    const combinedSwebok = [...new Set([...swebokAreas, ...swebokSubAreas])];
    
    const eduObjectives = details.educationalObjectives || "";
    const benefits = details.benefitsAndChallenges || "";
    const metricsStr = details.metrics || "";

    return {
        id: details.id || crypto.randomUUID(),
        title: title,
        authors: details.authors || "",
        year: parseInt(details.year, 10) || 2024,
        source: details.source || "",
        studyType: "Case Study", 
        doiOrLink: details.doiOrLink || "",
        motivation: eduObjectives.substring(0, 200) + "...",
        educationalObjectives: eduObjectives,
        genAITool: details.llm || "",
        usedByStudents: (details.usedByStudents as UseByStudents) || "Not reported",
        usedHow: details.aiEmployment || "",
        autonomy: (details.level as Autonomy) || "A",
        contentSkill: "Software Engineering Concepts",
        teachingStrategy: "AI-assisted Learning",
        audience: "Undergraduate",
        analysis: (details.analysis as AnalysisType) || "Mixed",
        resources: details.resources || "",
        metrics: metricsStr.split(',').map(m => m.trim()).filter(Boolean),
        metricsUsage: details.metricsUsage || "",
        results: benefits.split(/Conversely,|However,|On the other hand,/)[0],
        benefitsChallenges: benefits,
        swebokAreas: combinedSwebok,
        interactionType: (details.interaction as InteractionType) || "Hybrid",
    };
});


// --- COMPONENTE REACT ---
function classNames(...classes: (string | false | null | undefined)[]) {
  return classes.filter(Boolean).join(" ");
}

export default function ResearchDashboard() {
  const [tab, setTab] = useState<"plot" | "ratings" | "spectrum" | "recommender">("plot");
  const [editingArticleId, setEditingArticleId] = useState<string | null>(null);
  const [articles, setArticles] = useState<Article[]>([]);
  const [visibleRatingsCount, setVisibleRatingsCount] = useState(10);
  const [visibleArticlesCount, setVisibleArticlesCount] = useState(10);

  const [selectedArticleForDetails, setSelectedArticleForDetails] = useState<Article | null>(null);
  
  // NOVO: Estado para controlar a visibilidade do modal de formul√°rio
  const [isFormModalOpen, setIsFormModalOpen] = useState(false);

  const [selectedArea, setSelectedArea] = useState<string | null>(null);
  const [selectedSubareas, setSelectedSubareas] = useState<string[]>([]);
  const [tempSelection, setTempSelection] = useState<Set<string>>(new Set());

  const [spectrumFilter, setSpectrumFilter] = useState<{ area: string; autonomy: Autonomy } | null>(null);
  const [plotFilter, setPlotFilter] = useState<{ year: number; autonomy: Autonomy } | null>(null);

  function toggleTempSelection(subarea: string) {
    setTempSelection(prev => {
      const next = new Set(prev);
      if (next.has(subarea)) {
        next.delete(subarea);
      } else {
        next.add(subarea);
      }
      return next;
    });
  }

  function handleConfirmSelection() {
    if (selectedArea) {
      setSelectedSubareas(Array.from(tempSelection));
    }
  }

  function resetRecommender() {
    setSelectedArea(null);
    setSelectedSubareas([]);
    setTempSelection(new Set());
  }

  useEffect(() => {
    const saved = localStorage.getItem("articles");
    if (saved) {
      try {
        const parsedArticles = JSON.parse(saved);
        if (parsedArticles && parsedArticles.length > 0) {
          setArticles(parsedArticles);
        } else {
           setArticles(INITIAL_ARTICLES);
        }
      } catch {
        setArticles(INITIAL_ARTICLES);
      }
    } else {
      setArticles(INITIAL_ARTICLES);
    }
  }, []);

  useEffect(() => {
    if (articles.length > 0) {
      localStorage.setItem("articles", JSON.stringify(articles));
    }
  }, [articles]);

  const [filterStudyType, setFilterStudyType] = useState<string>("");
  const [filterTool, setFilterTool] = useState<string>("");
  const [filterAudience, setFilterAudience] = useState<string>("");

  const filtered = useMemo(() => {
    return articles.filter((a) =>
      (!filterStudyType || a.studyType === filterStudyType) &&
      (!filterTool || (a.genAITool && a.genAITool.toLowerCase().includes(filterTool.toLowerCase()))) &&
      (!filterAudience || a.audience === filterAudience)
    );
  }, [articles, filterStudyType, filterTool, filterAudience]);

  const recommendedArticles = useMemo(() => {
    if (!selectedArea || selectedSubareas.length === 0) {
      return [];
    }
    return articles.filter(article => {
      const articleAreas = article.swebokAreas || [];
      const hasMainArea = articleAreas.includes(selectedArea);
      const hasSubArea = selectedSubareas.some(subarea => articleAreas.includes(subarea));
      return hasMainArea && hasSubArea;
    });
  }, [selectedArea, selectedSubareas, articles]);

  const spectrumFilteredArticles = useMemo(() => {
    if (!spectrumFilter) return [];
    return articles.filter(article => 
      article.autonomy === spectrumFilter.autonomy &&
      (article.swebokAreas || []).includes(spectrumFilter.area)
    );
  }, [spectrumFilter, articles]);

  const plotFilteredArticles = useMemo(() => {
    if (!plotFilter) return [];
    return filtered.filter(article =>
      article.year === plotFilter.year &&
      article.autonomy === plotFilter.autonomy
    );
  }, [plotFilter, filtered]);

  const mainAreaCounts = useMemo(() => {
    const counts: Record<string, number> = {};
    for (const area of Object.keys(SUBAREAS)) {
      counts[area] = 0;
    }
    for (const article of articles) {
      if (!article.swebokAreas) continue;
      for (const area of article.swebokAreas) {
        if (counts[area] !== undefined) {
          counts[area]++;
        }
      }
    }
    return counts;
  }, [articles]);

  const yearBuckets = useMemo(() => {
    const map: Record<number, { year: number; A: number; B: number; C: number }> = {};
    for (const a of filtered) {
      if (!map[a.year]) map[a.year] = { year: a.year, A: 0, B: 0, C: 0 };
      map[a.year][a.autonomy] += 1;
    }
    return Object.values(map).sort((x, y) => x.year - y.year);
  }, [filtered]);

  const heatCounts = useMemo(() => {
    const base = SWEBOK_AREAS.map((area) => ({ area, A: 0, B: 0, C: 0 }));
    const index = Object.fromEntries(base.map((r, i) => [r.area, i]));
    for (const a of filtered) {
      if (a.swebokAreas) {
        for (const area of a.swebokAreas) {
          const i = index[area];
          if (i !== undefined) {
            base[i][a.autonomy] += 1;
          }
        }
      }
    }
    return base;
  }, [filtered]);

  // Estado de Rascunho para o novo formul√°rio
  const [draft, setDraft] = useState<Partial<Article>>({});

  // Fun√ß√£o para resetar o rascunho para um estado inicial limpo
  function resetDraft() {
    setDraft({
      id: crypto.randomUUID(), // Gera um ID novo
      title: "",
      authors: "",
      year: new Date().getFullYear(),
      source: "",
      studyType: "Case Study",
      doiOrLink: "",
      motivation: "",
      educationalObjectives: "",
      genAITool: "",
      usedByStudents: "Not reported",
      usedHow: "",
      autonomy: "B",
      contentSkill: "",
      teachingStrategy: "",
      audience: "Undergraduate",
      analysis: "Mixed",
      resources: "",
      metrics: [],
      results: "",
      benefitsChallenges: "",
      swebokAreas: [],
      interactionType: "Hybrid",
    });
  }

  // Reseta o rascunho na primeira vez que o componente carrega
  useEffect(resetDraft, []);

  // Fun√ß√£o gen√©rica para atualizar o rascunho
  function update<K extends keyof Article>(key: K, val: Article[K]) {
    setDraft((d) => ({ ...d, [key]: val }));
  }

  function deleteArticle(id: string) {
    if (confirm("Deseja realmente deletar este artigo?")) {
      setArticles((a) => a.filter((art) => art.id !== id));
      if (editingArticleId === id) {
        cancelEditing();
      }
    }
  }

  function startEditing(article: Article) {
    setDraft(JSON.parse(JSON.stringify(article))); // Carrega o artigo no rascunho
    setEditingArticleId(article.id);
    setIsFormModalOpen(true); // Abre o modal de formul√°rio
  }

  function cancelEditing() {
    resetDraft(); // Limpa o rascunho
    setEditingArticleId(null);
    setIsFormModalOpen(false); // Fecha o modal de formul√°rio
  }

  function saveArticle() {
    const required: (keyof Article)[] = ["title", "authors", "year", "autonomy"];
    for (const k of required) {
      if (!draft[k]) {
        alert(`Campo obrigat√≥rio faltando: ${k}`);
        return;
      }
    }

    const articleToSave = { ...draft } as Article;

    if (editingArticleId) {
      // Editando um artigo existente
      setArticles(articles.map(article => article.id === editingArticleId ? articleToSave : article));
    } else {
      // Adicionando um novo artigo
      setArticles((a) => [articleToSave, ...a]);
    }
    
    cancelEditing(); // Reseta o rascunho e fecha o modal
  }

  function toggleArray(field: keyof Article, value: string) {
    const currentValues = (draft as any)[field] || [];
    const arr = new Set<string>(currentValues);
    if (arr.has(value)) {
      arr.delete(value);
    } else {
      arr.add(value);
    }
    setDraft((d) => ({ ...d, [field]: Array.from(arr) as any }));
  }

  // Fun√ß√£o para converter o campo de m√©tricas (que pode ser string ou array) para array
  function getMetricsAsArray(metrics: string | string[] | undefined): string[] {
    if (Array.isArray(metrics)) {
      return metrics;
    }
    if (typeof metrics === 'string') {
      return metrics.split(',').map(m => m.trim()).filter(Boolean);
    }
    return [];
  }

  return (
    <>
      {/* --- MODAL DE DETALHES DO ARTIGO (CORRIGIDO PARA EXIBIR SUB√ÅREAS) --- */}
      {selectedArticleForDetails && (() => {
        // L√≥gica para separar √°reas principais de sub√°reas
        const allMainAreas = Object.keys(SUBAREAS);
        const articleSwebokAreas = selectedArticleForDetails.swebokAreas || [];
        const articleMainAreas = articleSwebokAreas.filter(area => allMainAreas.includes(area));
        const articleSubAreas = articleSwebokAreas.filter(area => !allMainAreas.includes(area));

        return (
          <div 
            className="fixed inset-0 bg-black/70 flex items-center justify-center z-[60] p-4"
            onClick={() => setSelectedArticleForDetails(null)}
          >
            <div 
              className="bg-slate-800 rounded-2xl p-8 max-w-4xl w-full max-h-[90vh] overflow-y-auto"
              onClick={(e) => e.stopPropagation()}
            >
              <div className="flex justify-between items-start mb-4">
                <div>
                  <h2 className="text-2xl font-bold text-slate-100">{selectedArticleForDetails.title}</h2>
                  <p className="text-sm text-slate-400 mt-1">{selectedArticleForDetails.authors} ({selectedArticleForDetails.year})</p>
                  <p className="text-xs text-slate-500">{selectedArticleForDetails.source}</p>
                </div>
                <button onClick={() => setSelectedArticleForDetails(null)} className="text-2xl text-slate-400 hover:text-white">&times;</button>
              </div>
              
              {selectedArticleForDetails.doiOrLink && (
                <div className="mt-4 mb-6">
                  <a
                    href={selectedArticleForDetails.doiOrLink}
                    target="_blank"
                    rel="noopener noreferrer"
                    className="inline-block bg-emerald-600 hover:bg-emerald-500 text-white font-medium py-2 px-4 rounded-lg transition-colors duration-200 text-sm"
                  >
                    Link
                  </a>
                </div>
              )}

              <div className="space-y-5">
                <h3 className="font-semibold text-emerald-400">Educational Objectives in Software Engineering</h3>
                <p className="text-slate-300 text-sm whitespace-pre-wrap">{selectedArticleForDetails.educationalObjectives || "N√£o informado."}</p>
                <h3 className="font-semibold text-emerald-400">How AI was Employed</h3>
                <p className="text-slate-300 text-sm whitespace-pre-wrap">{selectedArticleForDetails.usedHow || "N√£o informado."}</p>
                <h3 className="font-semibold text-emerald-400">LLM(s) Used</h3>
                <p className="text-slate-300 text-sm whitespace-pre-wrap">{selectedArticleForDetails.genAITool || "N√£o informado."}</p>
                <h3 className="font-semibold text-emerald-400">Metrics</h3>
                <p className="text-slate-300 text-sm whitespace-pre-wrap">{getMetricsAsArray(selectedArticleForDetails.metrics).join(', ') || "N√£o informado."}</p>
                <h3 className="font-semibold text-emerald-400">How the Metrics Were Used</h3>
                <p className="text-slate-300 text-sm whitespace-pre-wrap">{selectedArticleForDetails.metricsUsage || "N√£o informado."}</p>
                <h3 className="font-semibold text-emerald-400">Extra materials or resources</h3>
                <p className="text-slate-300 text-sm whitespace-pre-wrap">{selectedArticleForDetails.resources || "N√£o informado."}</p>
                <h3 className="font-semibold text-emerald-400">Benefits and Challenges</h3>
                <p className="text-slate-300 text-sm whitespace-pre-wrap">{selectedArticleForDetails.benefitsChallenges || "N√£o informado."}</p>
                
                
                {/* --- IN√çCIO DA CORRE√á√ÉO --- */}
                <h3 className="font-semibold text-emerald-400">SWEBOK Main Areas</h3>
                <p className="text-slate-300 text-sm whitespace-pre-wrap">{articleMainAreas.join(', ') || "Nenhuma √°rea principal informada."}</p>
                
                <h3 className="font-semibold text-emerald-400">SWEBOK Subareas</h3>
                <p className="text-slate-300 text-sm whitespace-pre-wrap">{articleSubAreas.join(', ') || "Nenhuma sub√°rea informada."}</p>
                {/* --- FIM DA CORRE√á√ÉO --- */}
              </div>
            </div>
          </div>
        )
      })()}
      
      {/* --- MODAL DE FILTRO DO SPECTRUM --- */}
      {spectrumFilter && (
        <div 
          className="fixed inset-0 bg-black/70 flex items-center justify-center z-50 p-4"
          onClick={() => setSpectrumFilter(null)}
        >
          <div 
            className="bg-slate-800 rounded-2xl p-8 max-w-2xl w-full max-h-[90vh] flex flex-col"
            onClick={(e) => e.stopPropagation()}
          >
            <div className="flex justify-between items-center mb-4 flex-shrink-0">
              <h2 className="text-xl font-bold text-slate-100">
                Papers of "{spectrumFilter.area}" ({AUTONOMY_LABELS[spectrumFilter.autonomy]})
              </h2>
              <button onClick={() => setSpectrumFilter(null)} className="text-2xl text-slate-400 hover:text-white">&times;</button>
            </div>
            
            <div className="overflow-y-auto space-y-3">
              {spectrumFilteredArticles.length > 0 ? (
                spectrumFilteredArticles.map(article => (
                  <div 
                    key={article.id} 
                    className="bg-slate-800/50 p-4 rounded-lg hover:bg-slate-700/50 transition cursor-pointer"
                    onClick={() => {
                      setSpectrumFilter(null); 
                      setSelectedArticleForDetails(article); 
                    }}
                  >
                    <h4 className="font-semibold text-slate-100">{article.title}</h4>
                    <p className="text-xs text-slate-400 mt-1">{article.authors} ({article.year})</p>
                  </div>
                ))
              ) : (
                <p className="text-slate-400">Nenhum artigo encontrado.</p>
              )}
            </div>
          </div>
        </div>
      )}

      {/* --- MODAL DE FILTRO DO PLOT --- */}
      {plotFilter && (
        <div 
          className="fixed inset-0 bg-black/70 flex items-center justify-center z-50 p-4"
          onClick={() => setPlotFilter(null)}
        >
          <div 
            className="bg-slate-800 rounded-2xl p-8 max-w-2xl w-full max-h-[90vh] flex flex-col"
            onClick={(e) => e.stopPropagation()}
          >
            <div className="flex justify-between items-center mb-4 flex-shrink-0">
              <h2 className="text-xl font-bold text-slate-100">
                Papers of {plotFilter.year} ({AUTONOMY_LABELS[plotFilter.autonomy]})
              </h2>
              <button onClick={() => setPlotFilter(null)} className="text-2xl text-slate-400 hover:text-white">&times;</button>
            </div>
            
            <div className="overflow-y-auto space-y-3">
              {plotFilteredArticles.length > 0 ? (
                plotFilteredArticles.map(article => (
                  <div 
                    key={article.id} 
                    className="bg-slate-800/50 p-4 rounded-lg hover:bg-slate-700/50 transition cursor-pointer"
                    onClick={() => {
                      setPlotFilter(null);
                      setSelectedArticleForDetails(article);
                    }}
                  >
                    <h4 className="font-semibold text-slate-100">{article.title}</h4>
                    <p className="text-xs text-slate-400 mt-1">{article.authors} ({article.year})</p>
                  </div>
                ))
              ) : (
                <p className="text-slate-400">Nenhum artigo encontrado.</p>
              )}
            </div>
          </div>
        </div>
      )}

      {/* --- NOVO MODAL PARA ADICIONAR/EDITAR ARTIGOS --- */}
      {isFormModalOpen && (
        <div className="fixed inset-0 bg-black/70 flex items-center justify-center z-50 p-4">
          <div 
            className="bg-slate-900 rounded-2xl p-8 max-w-4xl w-full max-h-[95vh] flex flex-col"
            onClick={(e) => e.stopPropagation()}
          >
            <div className="flex justify-between items-center mb-6 flex-shrink-0">
              <h2 className="text-2xl font-bold text-slate-100">
                {editingArticleId ? 'Editar Artigo' : 'Add New Article'}
              </h2>
              <button onClick={cancelEditing} className="text-2xl text-slate-400 hover:text-white">&times;</button>
            </div>
            
            {/* O 'key' no form for√ßa o reset do estado dos campos quando o 'draft.id' muda */}
            <form key={draft.id} className="overflow-y-auto pr-4 -mr-4 space-y-4 text-sm" onSubmit={(e) => e.preventDefault()}>
              {/* Campos Principais */}
              <label className="block">
                <span className="text-slate-400">Title*</span>
                <input className="mt-1 block w-full bg-slate-800 rounded px-3 py-2" value={draft.title || ""} onChange={(e) => update("title", e.target.value)} />
              </label>
              <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
                <label className="block">
                  <span className="text-slate-400">Authors*</span>
                  <input className="mt-1 block w-full bg-slate-800 rounded px-3 py-2" value={draft.authors || ""} onChange={(e) => update("authors", e.target.value)} />
                </label>
                <label className="block">
                  <span className="text-slate-400">Year*</span>
                  <input type="number" className="mt-1 block w-full bg-slate-800 rounded px-3 py-2" value={draft.year || ""} onChange={(e) => update("year", parseInt(e.target.value, 10))} />
                </label>
              </div>
              <label className="block">
                <span className="text-slate-400">Source (Ex: SIGCSE 2024)</span>
                <input className="mt-1 block w-full bg-slate-800 rounded px-3 py-2" value={draft.source || ""} onChange={(e) => update("source", e.target.value)} />
              </label>
              <label className="block">
                <span className="text-slate-400">DOI or Link</span>
                <input className="mt-1 block w-full bg-slate-800 rounded px-3 py-2" value={draft.doiOrLink || ""} onChange={(e) => update("doiOrLink", e.target.value)} />
              </label>
              
              {/* Campos de Texto Longos */}
              <label className="block">
                <span className="text-slate-400">Educational objectives in software engineering</span>
                <textarea className="mt-1 block w-full bg-slate-800 rounded px-3 py-2 h-24" value={draft.educationalObjectives || ""} onChange={(e) => update("educationalObjectives", e.target.value)} />
              </label>
              <label className="block">
                <span className="text-slate-400">How AI was employed</span>
                <textarea className="mt-1 block w-full bg-slate-800 rounded px-3 py-2 h-24" value={draft.usedHow || ""} onChange={(e) => update("usedHow", e.target.value)} />
              </label>
              <label className="block">
                <span className="text-slate-400">Perception of benefits and challenges</span>
                <textarea className="mt-1 block w-full bg-slate-800 rounded px-3 py-2 h-24" value={draft.benefitsChallenges || ""} onChange={(e) => update("benefitsChallenges", e.target.value)} />
              </label>
              <label className="block">
                <span className="text-slate-400">Results</span>
                <textarea className="mt-1 block w-full bg-slate-800 rounded px-3 py-2 h-24" value={draft.results || ""} onChange={(e) => update("results", e.target.value)} />
              </label>
              <label className="block">
                <span className="text-slate-400">Metrics</span>
                <input className="mt-1 block w-full bg-slate-800 rounded px-3 py-2" value={getMetricsAsArray(draft.metrics).join(', ')} onChange={(e) => update("metrics", e.target.value.split(',').map(m => m.trim()))} />
              </label>
              
      
              <label className="block">
                <span className="text-slate-400">How the Metrics Were Used</span>
                <textarea className="mt-1 block w-full bg-slate-800 rounded px-3 py-2 h-24" value={draft.metricsUsage || ""} onChange={(e) => update("metricsUsage", e.target.value)} />
              </label>
              <label className="block">
                <span className="text-slate-400">Extra materials or resources</span>
                <textarea className="mt-1 block w-full bg-slate-800 rounded px-3 py-2 h-24" value={draft.resources || ""} onChange={(e) => update("resources", e.target.value)} />
              </label>
              

              {/* Campos de Sele√ß√£o */}
              <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4">
                 <label className="block">
                  <span className="text-slate-400">Classifica√ß√£o*</span>
                  <select className="mt-1 block w-full bg-slate-800 rounded px-3 py-2" value={draft.autonomy || "B"} onChange={(e) => update("autonomy", e.target.value as Autonomy)}>
                    <option value="A">{AUTONOMY_LABELS.A}</option>
                    <option value="B">{AUTONOMY_LABELS.B}</option>
                    <option value="C">{AUTONOMY_LABELS.C}</option>
                  </select>
                </label>
                <label className="block">
                  <span className="text-slate-400">Type of Interaction</span>
                  <select className="mt-1 block w-full bg-slate-800 rounded px-3 py-2" value={draft.interactionType || "Hybrid"} onChange={(e) => update("interactionType", e.target.value as InteractionType)}>
                    <option value="Free Prompts">Free Prompts</option>
                    <option value="Hybrid">Hybrid</option>
                    <option value="Fixed Prompts">Fixed Prompts</option>
                  </select>
                </label>
                <label className="block">
                  <span className="text-slate-400">Generative AI Used</span>
                  <input className="mt-1 block w-full bg-slate-800 rounded px-3 py-2" value={draft.genAITool || ""} onChange={(e) => update("genAITool", e.target.value)} />
                </label>
                <label className="block">
                  <span className="text-slate-400">Type of Study</span>
                  <select className="mt-1 block w-full bg-slate-800 rounded px-3 py-2" value={draft.studyType || ""} onChange={(e) => update("studyType", e.target.value as StudyType)}>
                    
                    <option value="Systematic Review/Mapping">Systematic Review/Mapping</option>
                    <option value="Experimental">Experimental</option>
                    <option value="Case Study">Case Study</option>
                    <option value="Other">Other</option>
                  </select>
                </label>
                <label className="block">
                  <span className="text-slate-400">Audience</span>
                  <select className="mt-1 block w-full bg-slate-800 rounded px-3 py-2" value={draft.audience || ""} onChange={(e) => update("audience", e.target.value)}>
                    
                    <option value="Undergraduate">Undergraduate</option>
                    <option value="Graduate">Graduate</option>
                    <option value="Professional">Professional</option>
                  </select>
                </label>
                <label className="block">
                  <span className="text-slate-400">Type of Analysis</span>
                  <select className="mt-1 block w-full bg-slate-800 rounded px-3 py-2" value={draft.analysis || "Mixed"} onChange={(e) => update("analysis", e.target.value as AnalysisType)}>
                    <option value="Quantitative">Quantitative</option>
                    <option value="Mixed">Mixed</option>
                    <option value="Qualitative">Qualitative</option>
                  </select>
                </label>
                <label className="block">
                  <span className="text-slate-400">Used by Students?</span>
                  <select className="mt-1 block w-full bg-slate-800 rounded px-3 py-2" value={draft.usedByStudents || "Not reported"} onChange={(e) => update("usedByStudents", e.target.value as UseByStudents)}>
                    <option value="Yes">Yes</option>
                    <option value="Partially">Partially</option>
                    <option value="No">No</option>
                    <option value="Not reported">Not reported</option>
                  </select>
                </label>
              </div>

              {/* SWEBOK Areas */}
              <div>
                <span className="text-slate-400">SWEBOK Areas</span>
                <div className="mt-2 grid grid-cols-1 md:grid-cols-2 gap-1 max-h-48 overflow-auto p-2 bg-slate-800 rounded">
                  {SWEBOK_AREAS.map((area) => (
                    <div key={area}>
                      <label className="flex items-center gap-2 text-sm font-medium">
                        <input type="checkbox" className="w-4 h-4 rounded text-emerald-500 bg-slate-700 border-slate-600 focus:ring-emerald-600" checked={(draft.swebokAreas || []).includes(area)} onChange={() => toggleArray("swebokAreas", area)} />
                        <span>{area}</span>
                      </label>
                      {(draft.swebokAreas || []).includes(area) && SUBAREAS[area] && (
                        <div className="pl-6 pt-1 pb-2 space-y-1">
                          {SUBAREAS[area].map(subarea => (
                            <label key={subarea} className="flex items-center gap-2 text-xs text-slate-300">
                              <input type="checkbox" className="w-4 h-4 rounded text-emerald-500 bg-slate-700 border-slate-600 focus:ring-emerald-600" checked={(draft.swebokAreas || []).includes(subarea)} onChange={() => toggleArray("swebokAreas", subarea)} />
                              <span>{subarea}</span>
                            </label>
                          ))}
                        </div>
                      )}
                    </div>
                  ))}
                </div>
              </div>
            </form>

            <div className="mt-6 flex justify-end gap-4 flex-shrink-0">
              <button onClick={cancelEditing} className="bg-slate-700 hover:bg-slate-600 transition rounded px-4 py-2 font-medium">
                Cancel
              </button>
              <button onClick={saveArticle} className="bg-emerald-600 hover:bg-emerald-500 transition rounded px-4 py-2 font-medium">
                {editingArticleId ? 'Salvar Altera√ß√µes' : 'Add Article'}
              </button>
            </div>
          </div>
        </div>
      )}

      {/* --- LAYOUT PRINCIPAL DA P√ÅGINA --- */}
      <div className="min-h-screen flex bg-slate-950 text-slate-100">
        <aside className="w-72 border-r border-slate-800 p-6 space-y-6 fixed left-0 top-0 h-full overflow-y-auto">
          <h1 className="text-2xl font-semibold tracking-tight">LearnSE AI</h1>
          
          <div className="space-y-2">
            <div className="text-xs uppercase text-slate-400">Navigation</div>
            <button onClick={() => setTab("recommender")} className={classNames("w-full text-left px-3 py-2 rounded-lg", tab === "recommender" ? "bg-slate-800" : "hover:bg-slate-900")}>Paper repository</button>
            <button onClick={() => setTab("plot")} className={classNames("w-full text-left px-3 py-2 rounded-lg", tab === "plot" ? "bg-slate-800" : "hover:bg-slate-900")}>Plot</button>
            <button onClick={() => setTab("ratings")} className={classNames("w-full text-left px-3 py-2 rounded-lg", tab === "ratings" ? "bg-slate-800" : "hover:bg-slate-900")}>Ratings</button>
            <button onClick={() => setTab("spectrum")} className={classNames("w-full text-left px-3 py-2 rounded-lg", tab === "spectrum" ? "bg-slate-800" : "hover:bg-slate-900")}>Spectrum</button>
          </div>

          <div className="space-y-3">
            <div className="text-xs uppercase text-slate-400">Filters</div>
            <select className="w-full bg-slate-900 rounded px-3 py-2" value={filterStudyType} onChange={(e) => setFilterStudyType(e.target.value)}>
              <option value="">Study Type</option>
              <option>Systematic Review/Mapping</option>
              <option>Experimental</option>
              <option>Case Study</option>
              <option>Other</option>
            </select>
            <input className="w-full bg-slate-900 rounded px-3 py-2" placeholder="GenAI tool (e.g., ChatGPT)" value={filterTool} onChange={(e) => setFilterTool(e.target.value)} />
            <select className="w-full bg-slate-700 rounded px-3 py-2" value={filterAudience} onChange={(e) => setFilterAudience(e.target.value)}>
              <option value="">Audience</option>
              <option>Undergraduate</option>
              <option>Graduate</option>
              <option>Professional</option>
            </select>
             <div className="text-sm text-slate-400">Records: <span className="text-slate-200 font-medium">{filtered.length}</span> / {articles.length}</div>
          </div>

          {/* BOT√ÉO ATUALIZADO PARA ABRIR O MODAL DE FORMUL√ÅRIO */}
          <div className="space-y-2">
             <div className="text-xs uppercase text-slate-400">Manage Articles</div>
             <button 
                onClick={() => {
                  resetDraft(); // Garante que o formul√°rio estar√° limpo
                  setEditingArticleId(null); // Garante que √© "novo" e n√£o "edi√ß√£o"
                  setIsFormModalOpen(true);
                }} 
                className="w-full bg-emerald-600 hover:bg-emerald-500 transition rounded px-3 py-2 font-medium"
              >
                Add New Article Manually
              </button>
          </div>
        </aside>

        {/* ================================================================== */}
        {/* CONTE√öDO PRINCIPAL (RESTAURADO) */}
        {/* ================================================================== */}
        <main className="flex-1 p-8 ml-72">
          <div className="flex items-center justify-between mb-6">
            <h2 className="text-xl font-semibold">
              {tab === "plot" ? "Year ‚Äì Plot" : tab === "ratings" ? "Ratings" : tab === "spectrum" ? "Spectrum (SWEBOK)" : "Papers repository"}
            </h2>
            <div className="flex items-center gap-4">
              {(Object.keys(AUTONOMY_LABELS) as Autonomy[]).map(key => (
                <span key={key} className="inline-flex items-center gap-2 text-sm">
                  <span className="w-3 h-3 rounded-full" style={{ background: AUTONOMY_COLORS[key] }} />
                  {AUTONOMY_LABELS[key]}
                </span>
              ))}
            </div>
          </div>

          {tab === "plot" && (
            <section className="bg-slate-900 rounded-2xl p-6 shadow-lg">
              <div className="w-full h-[420px]">
                <ResponsiveContainer width="100%" height="100%">
                  <BarChart data={yearBuckets}>
                    <CartesianGrid stroke="#1f2937" />
                    <XAxis dataKey="year" stroke="#94a3b8" />
                    <YAxis stroke="#94a3b8" />
                    <Tooltip contentStyle={{ background: "#0f172a", border: "1px solid #334155" }} />
                    <Legend formatter={(value) => AUTONOMY_LABELS[value as Autonomy]} />
                    <Bar dataKey="A" stackId="a" fill={AUTONOMY_COLORS.A} style={{ cursor: 'pointer' }} onClick={(data) => data.A > 0 && setPlotFilter({ year: data.year, autonomy: 'A' })} />
                    <Bar dataKey="B" stackId="a" fill={AUTONOMY_COLORS.B} style={{ cursor: 'pointer' }} onClick={(data) => data.B > 0 && setPlotFilter({ year: data.year, autonomy: 'B' })} />
                    <Bar dataKey="C" stackId="a" fill={AUTONOMY_COLORS.C} style={{ cursor: 'pointer' }} onClick={(data) => data.C > 0 && setPlotFilter({ year: data.year, autonomy: 'C' })} />
                  </BarChart>
                </ResponsiveContainer>
              </div>
            </section>
          )}

          {tab === "ratings" && (
            <section className="bg-slate-900 rounded-2xl p-6 shadow-lg">
              <h3 className="text-lg font-semibold mb-4">Article Ratings Overview</h3>
              <div className="overflow-y-auto max-h-[60vh]">
                <table className="w-full text-sm">
                  <thead>
                    <tr>
                      <th className="p-2 text-left sticky top-0 bg-slate-900 z-10">Article</th>
                      <th className="p-2 text-center sticky top-0 bg-slate-900 w-1/5">Autonomy</th>
                      <th className="p-2 text-center sticky top-0 bg-slate-900">Interaction</th>
                      <th className="p-2 text-center sticky top-0 bg-slate-900">Analysis</th>
                      <th className="p-2 text-center sticky top-0 bg-slate-900">Used by Students</th>
                    </tr>
                  </thead>
                  <tbody>
                    {filtered.slice(0, visibleRatingsCount).map((article) => (
                      <tr key={article.id} className="border-t border-slate-700 hover:bg-slate-800/50 transition cursor-pointer" onClick={() => setSelectedArticleForDetails(article)}>
                        <td className="p-2 text-left">
                          <div className="font-medium">{article.title}</div>
                          <div className="text-xs text-slate-400">{article.authors} ({article.year})</div>
                        </td>
                        <td className="p-2 text-center">
                          <span className="inline-flex items-center gap-1">
                            <span className="w-3 h-3 rounded-full" style={{ background: AUTONOMY_COLORS[article.autonomy] }} />
                            {AUTONOMY_LABELS[article.autonomy]}
                          </span>
                        </td>
                        <td className="p-2 text-center">{article.interactionType}</td>
                        <td className="p-2 text-center">{article.analysis}</td>
                        <td className="p-2 text-center">{article.usedByStudents}</td>
                      </tr>
                    ))}
                  </tbody>
                </table>
              </div>
               {filtered.length > 10 && (
                <div className="mt-4 flex justify-center">
                  <button
                    onClick={() => 
                      visibleRatingsCount >= filtered.length 
                        ? setVisibleRatingsCount(10) 
                        : setVisibleRatingsCount(filtered.length)
                    }
                    className="bg-slate-700 hover:bg-slate-600 px-4 py-2 rounded text-sm"
                  >
                    {visibleRatingsCount >= filtered.length ? 'Show Less' : 'Show All'}
                  </button>
                </div>
              )}
            </section>
          )}

          {tab === "spectrum" && (
            <section className="bg-slate-900 rounded-2xl p-6 shadow-lg">
               <div className="overflow-auto">
                {/* 1. Voltamos para a classe original da tabela para que ela se adapte */}
                <table className="min-w-full text-sm">
                  <thead>
                    <tr>
                      <th className="text-left p-2 sticky left-0 bg-slate-900">SWEBOK Area</th>
                      {(Object.keys(AUTONOMY_LABELS) as Autonomy[]).map(key => (
                        // 2. Adicionamos uma largura proporcional para cada coluna
                        <th key={key} className="p-2 w-1/6">{AUTONOMY_LABELS[key]}</th>
                      ))}
                    </tr>
                  </thead>
                  <tbody>
                    {heatCounts.map((row) => {
                      const max = Math.max(row.A, row.B, row.C, 1);
                      return (
                        <tr key={row.area} className="border-t border-slate-800">
                          <td className="p-2 sticky left-0 bg-slate-900 whitespace-nowrap pr-6">{row.area}</td>
                          {(["A", "B", "C"] as const).map((k) => {
                            const v = row[k] as number;
                            const intensity = v / max;
                            const color = AUTONOMY_COLORS[k];
                            return (
                              <td key={k} className="p-2">
                                <div
                                  onClick={() => v > 0 && setSpectrumFilter({ area: row.area, autonomy: k as Autonomy })}
                                  className={classNames(
                                    "h-10 rounded-md grid place-items-center text-slate-900 font-semibold relative",
                                    v > 0 && "cursor-pointer hover:ring-2 hover:ring-white/50 transition"
                                  )}
                                  style={{ background: `${color}33` }}
                                  title={v > 0 ? `Clique para ver os ${v} artigos` : 'Nenhum artigo'}
                                >
                                  <div className="w-full h-full rounded" style={{ background: color, opacity: 0.25 + intensity * 0.75 }} />
                                  <span className="absolute text-slate-200">{v}</span>
                                </div>
                              </td>
                            );
                          })}
                        </tr>
                      );
                    })}
                  </tbody>
                </table>
              </div>
            </section>
          )}
          
          {tab === "recommender" && (
            <section className="bg-slate-900 rounded-2xl p-6 shadow-lg">
              {selectedArea && selectedSubareas.length > 0 ? (
                <div>
                  <div className="flex items-center justify-between mb-4">
                    <h3 className="text-lg font-semibold">
                      Artigos Recomendados para "{selectedArea}"
                    </h3>
                    <button onClick={resetRecommender} className="bg-slate-700 hover:bg-slate-600 px-4 py-2 rounded text-sm transition">
                      ‚Üê Nova Busca
                    </button>
                  </div>

                  {recommendedArticles.length > 0 ? (
                    <div className="space-y-4">
                      {recommendedArticles.map(art => (
                        <div key={art.id} className="bg-slate-800/50 p-4 rounded-lg hover:bg-slate-700/50 transition cursor-pointer" onClick={() => setSelectedArticleForDetails(art)}>
                          <div className="flex justify-between items-start">
                            <div>
                              <h4 className="font-semibold text-base text-slate-100">{art.title}</h4>
                              <p className="text-xs text-slate-400 mt-1">{art.authors} ({art.year})</p>
                            </div>
                            <span className="flex-shrink-0 ml-4 inline-flex items-center gap-2 text-sm font-bold px-2 py-1 rounded" style={{ background: `${AUTONOMY_COLORS[art.autonomy]}33`, color: AUTONOMY_COLORS[art.autonomy]}}>
                              <span className="w-2 h-2 rounded-full" style={{ background: AUTONOMY_COLORS[art.autonomy] }} />
                              {AUTONOMY_LABELS[art.autonomy]}
                            </span>
                          </div>
                          <p className="text-sm text-slate-300 mt-2">
                            {art.educationalObjectives || art.results}
                          </p>
                        </div>
                      ))}
                    </div>
                  ) : (
                    <p className="text-slate-400 text-center py-8">Nenhum artigo encontrado para a √°rea "{selectedArea}" com as sub√°reas selecionadas.</p>
                  )}
                </div>
              ) : 
              selectedArea ? (
                <div>
                  <div className="flex items-center gap-4 mb-4">
                    <button onClick={() => setSelectedArea(null)} className="bg-slate-700 hover:bg-slate-600 px-3 py-1 rounded text-sm transition">
                      ‚Üê Go back
                    </button>
                    <h3 className="text-lg font-semibold">Select the subareas of "{selectedArea}"</h3>
                  </div>
                  <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-2 mb-4">
                    {(SUBAREAS[selectedArea] || []).map(subarea => (
                      <label key={subarea} className="flex items-center gap-2 p-3 bg-slate-800 rounded-md hover:bg-slate-700 cursor-pointer transition">
                        <input type="checkbox" checked={tempSelection.has(subarea)} onChange={() => toggleTempSelection(subarea)} className="w-4 h-4 rounded text-emerald-500 bg-slate-700 border-slate-600 focus:ring-emerald-600" />
                        <span className="text-sm">{subarea}</span>
                      </label>
                    ))}
                  </div>
                  <div className="mt-6 flex justify-center">
                    <button
                      onClick={handleConfirmSelection}
                      disabled={tempSelection.size === 0}
                      className="bg-emerald-600 hover:bg-emerald-500 disabled:bg-slate-600 disabled:cursor-not-allowed transition rounded px-6 py-2 font-medium"
                    >
                      Confirm Selection and View Articles
                    </button>
                  </div>
                </div>
              ) : (
                <div>
                  <h3 className="text-lg font-semibold mb-4">Select a main area of ‚Äã‚ÄãSWEBOK</h3>
                  <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4">
                    {Object.keys(SUBAREAS).map(area => (
                      <button
                        key={area}
                        onClick={() => setSelectedArea(area)}
                        className="p-4 bg-slate-800 rounded-lg text-left hover:bg-slate-700 hover:ring-2 hover:ring-emerald-500 transition focus:outline-none focus:ring-2 focus:ring-emerald-500 flex justify-between items-center"
                      >
                        <span className="text-sm font-medium">{area}</span>
                        <span className="bg-slate-700 text-slate-200 text-xs font-bold px-2.5 py-1 rounded-full">
                          {mainAreaCounts[area] || 0}
                        </span>
                      </button>
                    ))}
                  </div>
                </div>
              )}
            </section>
          )}

          <section className="mt-8 bg-slate-900 rounded-2xl p-6 shadow-lg">
            <h3 className="text-lg font-semibold mb-4">Articles ({filtered.length})</h3>
            <div className="overflow-y-auto max-h-96">
              <table className="w-full text-sm">
                <thead>
                  <tr className="text-slate-300">
                    <th className="p-2 text-left w-2/5">Title</th>
                    <th className="p-2 text-center">Year</th>
                    <th className="p-2 text-center">Tool</th>
                    <th className="p-2 text-center">Autonomy</th>
                    <th className="p-2 text-left">SWEBOK Areas</th>
                    <th className="p-2 text-center">Actions</th>
                  </tr>
                </thead>
                <tbody>
                  {filtered.slice(0, visibleArticlesCount).map((a) => (
                    <tr key={a.id} className="border-t border-slate-800 hover:bg-slate-800/50 transition">
                      <td className="p-2 text-left cursor-pointer hover:text-emerald-400" onClick={() => setSelectedArticleForDetails(a)}>{a.title}</td>
                      <td className="p-2 text-center">{a.year}</td>
                      <td className="p-2 text-center">{a.genAITool}</td>
                      <td className="p-2 text-center">
                        <span className="inline-flex items-center gap-2"><span className="w-2 h-3 rounded-full" style={{ background: AUTONOMY_COLORS[a.autonomy] }} />{AUTONOMY_LABELS[a.autonomy]}</span>
                      </td>
                      <td className="p-2 text-left text-xs">{(a.swebokAreas || []).join(", ")}</td>
                      <td className="p-2 text-center">
                        <div className="flex gap-2 justify-center">
                          <button onClick={() => startEditing(a)} className="bg-blue-600 hover:bg-blue-500 px-2 py-1 rounded text-xs">
                            Edit
                          </button>
                          <button onClick={() => deleteArticle(a.id)} className="bg-red-600 hover:bg-red-500 px-2 py-1 rounded text-xs">
                            Delete
                          </button>
                        </div>
                      </td>
                    </tr>
                  ))}
                </tbody>
              </table>
            </div>
             {filtered.length > 10 && (
              <div className="mt-4 flex justify-center">
                <button
                  onClick={() => 
                    visibleArticlesCount >= filtered.length 
                      ? setVisibleArticlesCount(10) 
                      : setVisibleArticlesCount(filtered.length)
                  }
                  className="bg-slate-700 hover:bg-slate-600 px-4 py-2 rounded text-sm"
                >
                  {visibleArticlesCount >= filtered.length ? 'Show Less' : 'Show All'}
                </button>
              </div>
            )}
          </section>
        </main>
      </div>

      <a
        href="https://docs.google.com/forms/d/e/1FAIpQLSdWYbKPjIAMTkEaZsPFpSLqjHCO7vWFXvEvyxGNkZzdE2WNlQ/viewform?usp=publish-editor"
        target="_blank"
        rel="noopener noreferrer"
        className="fixed bottom-6 right-6 bg-emerald-600 hover:bg-emerald-500 text-white font-medium text-sm py-2 px-4 rounded-lg shadow-lg transition-colors duration-200 z-50"
        title="Acredita que falta algum artigo? Nos informe preenchendo esse formul√°rio"
      >
        Think an article is missing? Let us know!
      </a>
      
    </>
  );
}

